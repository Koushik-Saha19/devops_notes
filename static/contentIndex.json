{"Computer-Networking/L-01---Introduction":{"slug":"Computer-Networking/L-01---Introduction","filePath":"Computer Networking/L 01 - Introduction.md","title":"L 01 - Introduction","links":[],"tags":[],"content":"\nNodes are any device which can send or receive data or both, that is called node.\n\nCommunication link can be wired or wireless link. The link carries the information.\n\nComputer networking is mainly used for resource sharing.\n\nIPv4 address - 32 bit\nIPv6 address - 128 bit\nMAC address - 48 bit"},"Computer-Networking/L-02---Computer-Networks---Basic-Characteristics":{"slug":"Computer-Networking/L-02---Computer-Networks---Basic-Characteristics","filePath":"Computer Networking/L 02 - Computer Networks - Basic Characteristics.md","title":"L 02 - Computer Networks - Basic Characteristics","links":[],"tags":[],"content":"\n\n\n\n\n\nFor example, if a network has 10 computer, now if we connect 10 more computer the network should work fine without any issue. In that case that network is called &quot;**scalable**&quot; network.\nExample - internet, it is a scalable network. Internet welcomes any new device or computer.\n\n\nVoIP phone and traditional phone is different. VoIP sends data over internet (e.g. WhatsApp call). Traditional voice call has dedicated wire connection.\n\nAs explained in the above screenshot, VoIP gets more priority when different types of network packets are available to Routers at the same time. This is called QoS.\n\n"},"Computer-Networking/L-03---Network-Protocols-and-Communications-(Part-1)":{"slug":"Computer-Networking/L-03---Network-Protocols-and-Communications-(Part-1)","filePath":"Computer Networking/L 03 - Network Protocols and Communications (Part 1).md","title":"L 03 - Network Protocols and Communications (Part 1)","links":[],"tags":[],"content":"Data Communication\n\nData Flow\n\nIt is 3 types\n\nSimplex\nHalf duplex\nFull duplex\n\n\n\nHere, Both Sending &amp; Receiving can&#039;t happen at same time. \n\n\n\n\n\n\n\n\n\n\nMessage Timing is crucial. Protocol is the one where the rate at which sender can send and the rate at which receiver can receive network packets. \nAfter sending network packets sender waits for acknowledgement signal from the receiver. The time for which the sender should wait is set by the protocol.\n\nUnicast -  one sender and one receiver.\n\nMulticast - one sender and multiple receiver, but not all the nodes will receive.\n\nBroadcast - one sender and it sends the message to all the participants in the network.\n\nPeer to Peer Network\n\nEvery node is called a peer\nAll the peers are in equal level, it means there is nobody superior and nobody inferior. All the node or peers have same rights.\nThere is no centralized administrator like who will managing the peers.\nThe problem with this network is “peer to peer” network is not scalable. It means new devices can’t be added.\n\n\n\nClient Server Network\n\nhere we have centralized administration, the administrator is called “master” and remaining nodes are called slave.\nThe administrator is called “Server” and other nodes are called “client”.\n\n"},"Computer-Networking/L-04---Network-Protocols-and-Communications-(Part-2)":{"slug":"Computer-Networking/L-04---Network-Protocols-and-Communications-(Part-2)","filePath":"Computer Networking/L 04 - Network Protocols and Communications (Part 2).md","title":"L 04 - Network Protocols and Communications (Part 2)","links":[],"tags":[],"content":""},"Computer-Networking/L-05---Components-of-a-Computer-Network":{"slug":"Computer-Networking/L-05---Components-of-a-Computer-Network","filePath":"Computer Networking/L 05 - Components of a Computer Network.md","title":"L 05 - Components of a Computer Network","links":[],"tags":[],"content":"Nodes can be few types -\n\nEnd nodes or end devices\nintermediary nodes or intermediary devices\n\nEnd nodes are the starting point of the network as well as the end point of the network. If two devices want to communicate, then the device which starts the connection is an end device and the device which end the connection, that is also an end device.\nThis two end communicate with each other through some intermediary devices.\nExample of end nodes -\n\nExample of Intermediary nodes -\n\n\n\nMedia\n\nit is also called link\nAlso called -\n\nWired Medium (or Guided Medium)\nWireless Medium (or Unguided Medium)\n\n\n\n\nEthernet cable -\n\nEthernet straight-through cable - it is used to connect two device of different types.\nEthernet crossover cable - it is used to connect two device of same types. Example - two computer, two switches, etc.\nThrough ethernet cable data passes through electrical signal.\n\nFiber optic cable\n\nIt carries data through light, and as light is the fastest, so optical cable gives the fastest wired connection.\n\nCoaxial cable\n\nused for audio or video connection, it also carries data in electrical signal form.\n\nUSB cable\n\nUniversal Serial Bus\n\n\n\nServices\n"},"Computer-Networking/L-06---Classification-of-Computer-Networks":{"slug":"Computer-Networking/L-06---Classification-of-Computer-Networks","filePath":"Computer Networking/L 06 - Classification of Computer Networks.md","title":"L 06 - Classification of Computer Networks","links":[],"tags":[],"content":"\nLAN\n\n\nMAN\n\n\nMAN is a bigger network\nIt must have at least two LAN\nAnd those LAN must be connected via switches, bridges\nCommunication over MAN is possible in a city or in a metropolitan city. In other words in a region whose size is like a city, there only communication over MAN work.\n\n\n\nWAN\n\n\nCommunication work in a large region\n\n\n\nInternet is example wide WAN. This is called wide WAN, because internet has so many LAN, MAN, WAN.\n\nNew trends in Computer Network\n\n\nBring your own device (BYOD)\nOnline collaboration\nCloud Computing\n\n"},"Computer-Networking/L-07---Network-Topology":{"slug":"Computer-Networking/L-07---Network-Topology","filePath":"Computer Networking/L 07 - Network Topology.md","title":"L 07 - Network Topology","links":[],"tags":[],"content":"\nTopology\n\nPhysical topology\nLogical topology\n\n\nTypes of Topology (according to logical data flow):\n\nBus Topology\n\n\n\nRing Topology\n\n\n\nif node-1 wants to send data or token to node-4, it has to come via other nodes.\n\nStar Topology or Hub Topology\n\n\n\nSingle point of failure is - if anything happens to the &quot;hub&quot; or the &quot;central node&quot;, then the network will be down.\nTwo or more start topology can be connected like below -\n\nMesh Topology\n\n\nSince each node is connected to each other node, so it gives fault tolerance. If any of the link gets affected, then the network will continue to work as all the nodes are connected to each other. \nDue to this this network is also reliable.\n\n\nAs the nodes are connected din “mesh”, so there is an issue in broadcasting. Suppose node-1 broadcast a message and it will be received by all of them. Now, if node-2 broadcast the same message, again it will be received by all of them.\nMesh Topology works fine for smaller network as it requires lot of links (from cost point of view).\nFor a larger network like 1000 or 2000 or more nodes are connected, in that case it will be impractical to have mesh topology.\n\nHybrid Topology\n\n"},"Computer-Networking/L-08---Network-Topology-(Solved-Questions)":{"slug":"Computer-Networking/L-08---Network-Topology-(Solved-Questions)","filePath":"Computer Networking/L 08 - Network Topology (Solved Questions).md","title":"L 08 - Network Topology (Solved Questions)","links":[],"tags":[],"content":"\n&lt;img src = /assets/Pasted-image-20250619-23.png width=500&gt;\nAns: Number of cable required - 6, Number of ports required - 12\n&lt;img src = /assets/Pasted-image-20250621.png width=500&gt;\n\n\nProblem with star topology is “it sends everything to central place” - so it is not a good option for traffic problem minimization.\n\n\nBus topology - all the nodes depends on the common line and that too a bi-directional, so it is also not good.\n\n\nRing topology has unidirectional traffic flow - so it is also not good.\n\n\nAns: Mesh topology is best. Every pair of node has a dedicated line due to which traffic can effectively managed.\n&lt;img src = /assets/Pasted-image-20250621-1.png width=500&gt;\nAns:\nTotal Cable = 5 (N), where N is number of nodes.\nTotal port = 10 (2N)\n&lt;img src = /assets/Pasted-image-20250621-2.png width=500&gt;"},"Computer-Networking/L-09---Basics-of-IP-Addressing":{"slug":"Computer-Networking/L-09---Basics-of-IP-Addressing","filePath":"Computer Networking/L 09 - Basics of IP Addressing.md","title":"L 09 - Basics of IP Addressing","links":[],"tags":[],"content":"&lt;img src = /assets/Pasted-image-20250621-3.png width=500&gt;\nIP address of any device can be changed based on the logic of network or based on the location of the device, so IP address is called “Logical address”.\n&lt;img src = /assets/Pasted-image-20250621-4.png width=500&gt;"},"Computer-Networking/L-10---Basics-of-MAC-Addressing":{"slug":"Computer-Networking/L-10---Basics-of-MAC-Addressing","filePath":"Computer Networking/L 10 - Basics of MAC Addressing.md","title":"L 10 - Basics of MAC Addressing","links":[],"tags":[],"content":"&lt;img src = /assets/Pasted-image-20250621-5.png width=500&gt;\nAnalogy:\nIP address is like location of person, wherever he goes, accordingly his location changes.\nMAC address is like name of person, wherever he goes, his location might change but his name won’t change.\n&lt;img src = /assets/Pasted-image-20250622.png width=500&gt;\n&lt;img src = /assets/Pasted-image-20250621-7.png&gt;\nNote:\n\nRouter\n\nRouters need IP addresses to forward the traffic.\nIn other words, IP addresses are router friendly addresses.\nRouter takes a decision based on IP address.\n\n\nSwitch\n\nSwitches need MAC address to forward the traffic.\nIn other words, MAC addresses are switch friendly.\nSwitch takes a decision based on MAC address.\nSwitch maintains a table of MAC address, based on the MAC address (of the next hop) it forwards the traffic. In LAN (Local Area Network) we need MAC address of all the device to send the traffic to correct destination.\n\n\n\nIP address vs MAC address:\n&lt;img src = /assets/Pasted-image-20250622-1.png width=600&gt;\nMAC addressed are very important in a LAN.\nAny Data goes from a computer it includes both IP address and MAC address as source address.\n\nThe source IP address identifies the original sender and remains with the data packet throughout its journey from end to end.\nThe source MAC address identifies the physical network interface from which the data is currently leaving, but this MAC address changes at each router or Layer 3 device as the data hops across different network segments towards its final destination.\n"},"Computer-Networking/L-11---Basics-of-Port-Addressing":{"slug":"Computer-Networking/L-11---Basics-of-Port-Addressing","filePath":"Computer Networking/L 11 - Basics of Port Addressing.md","title":"L 11 - Basics of Port Addressing","links":[],"tags":[],"content":"Analogy:\nSomeone in Canada wants to send a parcel to you at Mumbai.\nNow, to send the parcel he/she require complete address.\nNow address can be divided in two section.\nOne part tells you which country, which city it should go\nAnother part tells in that city in which specific apartment and room number that parcel should go.\n&lt;img src = /assets/Pasted-image-20250622-2.png width=500&gt;\nIn real example, traffic can reach the destination by IP address and MAC address. But in that computer so many process would be running. To which process the data should reach, that is decided by “port address”.\n&lt;img src = /assets/Pasted-image-20250622-3.png width=600&gt;\n&lt;img src = /assets/Pasted-image-20250622-4.png&gt;\n&lt;img src = /assets/Pasted-image-20250622-6.png&gt;"},"Computer-Networking/L-12---Switching-Techniques-in-Computer-Networks":{"slug":"Computer-Networking/L-12---Switching-Techniques-in-Computer-Networks","filePath":"Computer Networking/L 12 - Switching Techniques in Computer Networks.md","title":"L 12 - Switching Techniques in Computer Networks","links":[],"tags":[],"content":"&lt;img src = /assets/Pasted-image-20250622-7.png&gt;\n&lt;img src = /assets/Pasted-image-20250622-8.png&gt;\n&lt;img src = /assets/Pasted-image-20250622-9.png&gt;\n\n&lt;img src = /assets/Pasted-image-20250622-11.png&gt;\nIn message switching, a big message or data is broken down into smaller unit and then sent. Later they gets stored into intermediary nodes and once all the units are received and stored, then the full message/date is forwarded.\n&lt;img src = /assets/Pasted-image-20250622-12.png&gt;\n&lt;img src = /assets/Pasted-image-20250622-13.png&gt;\n\n&lt;img src = /assets/Pasted-image-20250622-14.png&gt;\nwhen any specific packet is received by the receiver it sends a acknowledgment to the sender. For every packet it does this.\n&lt;img src = /assets/Pasted-image-20250622-15.png width=400&gt;\n&lt;img src = /assets/Pasted-image-20250622-16.png&gt;\nIn datagram packet switching, the path is not fixed. Various packets or datagram can take various path (not a fixed path), the path is decided by the intermediary devices.\n&lt;img src = /assets/Pasted-image-20250622-17.png&gt;\n&lt;img src = /assets/Pasted-image-20250622-18.png&gt;\n&lt;img src = /assets/Pasted-image-20250622-19.png&gt;\nin the above example, Datagram 4 arrived earlier compare to Datagram 3, though it started late compare to 3.\n&lt;img src = /assets/Pasted-image-20250622-20.png&gt;\nSince each datagram have sequence number, so receiver reorders them and final data become as same as what was sent.\n&lt;img src = /assets/Pasted-image-20250622-21.png&gt;\n\nPacket switching - Virtual Circuit Switching\n&lt;img src = /assets/Pasted-image-20250622-22.png&gt;\n&lt;img src = /assets/Pasted-image-20250622-23.png&gt;"},"Computer-Networking/L-13---Layering-in-Computer-Networks":{"slug":"Computer-Networking/L-13---Layering-in-Computer-Networks","filePath":"Computer Networking/L 13 - Layering in Computer Networks.md","title":"L 13 - Layering in Computer Networks","links":[],"tags":[],"content":"&lt;img src = /assets/Pasted-image-20250622-24.png&gt;\n&lt;img src = /assets/Pasted-image-20250622-25.png width=500&gt;\n&lt;img src = /assets/Pasted-image-20250622-26.png width=400&gt;\n&lt;img src = /assets/Pasted-image-20250622-27.png&gt;\n&lt;img src = /assets/Pasted-image-20250622-28.png&gt;\nExample - It does not matter what is the OS, traffic sent from Linux should be accepted by Windows/android/Linux/macOS, traffic sent by Android should be accepted be windows/Linux/android, etc.\n\n&lt;img src = /assets/Pasted-image-20250622-29.png&gt;\nWe know for data communication to happen in computer network we need to attach IP address, MAC address, Port address.\nEach of the layer handles each of the address."},"Computer-Networking/L-14---The-OSI-Reference-Model-(Part-1)":{"slug":"Computer-Networking/L-14---The-OSI-Reference-Model-(Part-1)","filePath":"Computer Networking/L 14 - The OSI Reference Model (Part 1).md","title":"L 14 - The OSI Reference Model (Part 1)","links":[],"tags":[],"content":"&lt;img src = /assets/Pasted-image-20250622-30.png&gt;\n&lt;img src = /assets/Pasted-image-20250622-31.png&gt;\n&lt;img src = /assets/Pasted-image-20250622-32.png width=500&gt;\nHere, the orders of layer are very important.\nTrick to remember the layers - Please Do Not Throw Sausage Pizza Away.\nPlease - P - Physical Layer\nDo - D - Data Link Layer\nNot - N - Network Layer\nThrow - T - Transport Layer\nSausage - S - Session Layer\nPizza - P - Presentation Layer\nAway - A - Application Layer\n&lt;img src = /assets/Pasted-image-20250622-33.png width=500&gt;\nHow these layers interact with each other?\n&lt;img src = /assets/Pasted-image-20250622-34.png&gt;\nApplication layer produces the data, then date is handed over data to Presentation layer.\nNext, Presentation layer hands over it to Session layer …this way it goes on and finally data is handed over to Physical layer.\nWhen data is in physical layer, then only data is handed over to the physical cables for transmission.\nThe “intermediary nodes” which are shown in the above screenshot, they transmits or process or handles or get access to the Network layer, Data link layer and Physical layer. Because we don’t want other layers to be available to the physical medium. Like we don’t want application layer specific data to be out in the network. Only bad hackers tries to access application layer specific data.\nAt the receiver end, when data through physical layer is received, next it is handed over to “Data link” layer, then it is handed over to “Network layer”, in this way it goes and finally it reaches “Application layer”.\n\nSyllabus:\n&lt;img src = /assets/Pasted-image-20250622-35.png&gt;\nChapter-5 consist of Application layer, Presentation layer and Session layer."},"Computer-Networking/L-15---The-OSI-Reference-Model-(Part-2)":{"slug":"Computer-Networking/L-15---The-OSI-Reference-Model-(Part-2)","filePath":"Computer Networking/L 15 - The OSI Reference Model (Part 2).md","title":"L 15 - The OSI Reference Model (Part 2)","links":[],"tags":[],"content":"Flow of data in layers of OSI model:\n&lt;img src = /assets/Pasted-image-20250622-36.png&gt;\nApplication Layer\n\n&lt;img src = /assets/Pasted-image-20250622-37.png&gt;\nPresentation Layer\n\n&lt;img src = /assets/Pasted-image-20250622-38.png&gt;\nHere, “Translation” means Presentation layer translate the data into a format which can be understandable/acceptable by both sender and receiver.\nEncryption - it encrypts the data to prevent any unauthorized access.\nCompression - Multi media messages like audio, video, etc gets compressed.\nSession Layer\n\n&lt;img src = /assets/Pasted-image-20250622-39.png&gt;\nDialog control - it means when two end computer wants to send/receive some data, during the process of data transmission they need to talk to each other and that happens through a session (by session layer). The session layer manages the talk or dialog. It can do the talk via various manner like - half-duplex (one way) or full-duplex (bi-directional).\nSynchronization - it is another task by Session layer. Example - when a large file is sent, they say every 100 pages sender sends a check point or synchronization message to check all the pages has been received. If any page gets corrupted or not sent properly, then by the synchronization, sender get to know about this and retransmit that page."},"Computer-Networking/L-16---The-OSI-Reference-Model-(Part-3)":{"slug":"Computer-Networking/L-16---The-OSI-Reference-Model-(Part-3)","filePath":"Computer Networking/L 16 - The OSI Reference Model (Part 3).md","title":"L 16 - The OSI Reference Model (Part 3)","links":[],"tags":[],"content":"Transport Layer\n\n&lt;img src = /assets/Pasted-image-20250622-40.png&gt;\nPort addressing - sender sends the data over some port and at the receiver end also the data is delivered to some port in which the concerned process/service must be listening.\nExample - Network Load Balancer.\nThough NLB uses the IP address (L3 info), its core job is managing connections using port numbers, and ensuring those connections are balanced and persistent to the right server and ensuring connections are healthy beforehand by sending SYNC, ACK, etc signal which is an L4 (Transport Layer) responsibility.\nSegmentation and reassembly - When any big message is being sent, it is fragmented into smaller parts and then it is sent. Later all the fragments are reassembled again.\nConnection control - when two device communicate, there could be two options. One is “connection oriented” and another is “connection less”.\nConnection oriented means before sending the data, connection will be established. And connection less means there won’t be any fixed connection set previously.\nEnd-to-End flow control - when two end devices communicate, the data sending rate of sender and data receiving rate of receiver must be in sync, there should be some mechanism to sync/manage/control the data flow.\nError control - When any data is being sent by the transport layer, it should be checked to see if there is any error or not. When the sender is sending any data, it could be correct, but in between it is getting transmitted there could be some error getting added, so we don’t want that erroneous data to get received by the receiver.  Error control is really required to check whether any error is there or not. This error is specifically not from the sender side, but it could be from the transmission line.\nNetwork Layer\n\n&lt;img src = /assets/Pasted-image-20250622-41.png&gt;\nLogical addressing - it means IP addressing. This layer is dealing with IP addresses. It helps a router to take decision, when a packet is received by the router it will have src IP and dest IP.\nRouting - Network layer is concerned about routing, it tries to find the best route to send the traffic.\nData Link Layer\n\n&lt;img src = /assets/Pasted-image-20250622-43.png&gt;\nFraming - When node A wants to send data to node B, in that case, the framing means in node A, it groups several bits (0 &amp; 1), which is called a frame.\nPhysical Addressing -\n We know every computer gets identified by an IP address and a MAC address, and any process gets identified by a port address.\nPort Address is handled by the transport layer, IP address is handled by the network layer, and the MAC address is handled by the data link layer.\nFlow control -  Flow control is a service of the data link layer that is also included in the Transport layer.\nError Control - If a frame gets corrupted or damaged, it can be easily recovered by the error control technique.\nAccess Control - When two or more devices are connected to the same link, the data link layer decides which device will have control over that link at a given point in time.\nPhysical Layer\n\n&lt;img src = /assets/Pasted-image-20250622-44.png&gt;\nIt is the responsibility of the physical layer when any frame is provided (by the data link layer) to place it in the medium. This medium can be of 2 types: one is a weird medium, another is a wireless medium.\nData link layer converts the stream of bit (any frame) -\n\nFor ethernet cable - it converts the bit stream to electrical signal\nFor wireless - it converts the bit stream to wave\nFor Optical fiber - it converts the bit stream to light signal.\n\nTo see description of each services - youtu.be/oQzueBVyAM4\n\nPhysical characteristics of the media - it is responsible for seeing the physical characteristics of the media. We know very well that there are two kinds of media. Wired media and wireless media. So, it knows that what kind of media is connected. So it defines the type of the media, whether it is wired or wireless.\nRepresentation of bits - It  means encoding. Encoding means this physical layer defines the type of encoding how those zeros and ones are converted into signals. So that is what we call as the representation of bits.\ndata rate - Data rate is also called as transmission rate. It means the number of bits sent each second. So that is what we call as the data rate. It is also called as transmission rate.\nsynchronization of bits - it means the clock between the sender and the receiver must also be synchronized.\nline configuration - Line configuration means whether it is a point-to-point communication or it is a point-to-multipoint communication. Point to point means between two nodes we will have only one channel or a medium that medium is exclusively dedicated for these two nodes. If it is a point to multipoint configuration, where the common channel or a medium is accessed or shared by many nodes. So this line configuration is also decided by the physical layer. And\nphysical topology - there are various topologies in the computer network. Like star topology, ring topology, mesh topology. So this physical topology defines how devices are connected to make a network. So this topology is also addressed by this physical layer.\ntransmission mode - We know there are three kinds of transmission mode- simplex, half duplex and full duplex. Simplex means data will flow only in one direction. Half duplex means data will flow in the both directions but not at the same time. And finally, full duplex means two devices can send and receive at the same time.\n"},"Computer-Networking/L-17---The-OSI-Reference-Model-(Part-4)":{"slug":"Computer-Networking/L-17---The-OSI-Reference-Model-(Part-4)","filePath":"Computer Networking/L 17 - The OSI Reference Model (Part 4).md","title":"L 17 - The OSI Reference Model (Part 4)","links":[],"tags":[],"content":"&lt;img src = /assets/Pasted-image-20250623-1.png&gt;\nFTAM - File Transfer &amp; Access Management.\n&lt;img src = /assets/Pasted-image-20250623-2.png&gt;\nAt Sender side\n\nIf you say this is the sender computer and this is the receiver computer,\nso this sender computer and receiver computer are connected through a network. So obviously there is a transmission medium. So the data generated by the application layer is named as D7 (left side computer is sender). That is the data generated by layer seven. We know very well that layer seven means it is application layer. Then some activity is carried out in the application layer, that is the header H7 is added to the application layer, it is then given to the presentation layer. D6 means it is presentation layer. So in this presentation layer, the data is encrypted or translated or compressed and this header is added in the presentation layer. After that it is given to the session layer. After session layer, it is given to the important layer which is transport layer where source port number and destination port number related information is added. So that is in the H4 that is header four. After adding the source port number and destination port number, this data or information is given to the network layer. In network layer, source IP address and destination IP address are added. After adding the source IP address and the destination IP address, it is then handed over to the data link layer. In the data link layer, the source MAC address and the destination MAC address are added in the header part, and error control related things are added in the trailer part. So, data link layer encapsulates or puts header as well as trailer to the data what it has received from the network layer. And finally, all the data is converted into zeros and ones in the physical layer and it is the responsibility of the physical layer to take the frames which are generated by the data link layer, and place the frames on the transmission medium.\nAt Receiver side\n\nAnd upon the reception, all the data are received in the form of bits only in the receiver side. So the physical layer of the receiver, receives the data and gives the data to the data link layer and then gives the data to the network layer, transport layer, session layer, presentation layer and application layer. Now you could see that this application layer information has finally reached the receiver’s application layer. Whatever data that is generated by the application layer of the sender, it is now handed over to the application layer of the receiver.\n\n&lt;img src = /assets/Pasted-image-20250623-3.png&gt;\nLet A is the IP of the sender computer and P is the IP of the receiver computer.\nWe know very well that in every computer there will be many processes running. Let a, b &amp; c are port numbers where 3 processes are running.\nSimilarly, in the receiver side also, we will have so many processes running. And let this j and K be the port number of these processes.\nNow this process in sender, that is with the port number a, is going to send some data to this process with the port number j. So in this case, the source port number is going to be a, and the destination port number is going to be j.\nSo once the data is generated by the application layer (we are skipping the presentation and session layer here), So, that data is then given to the transport layer.\nIn the transport layer, we know very well that the source port number and destination port numbers are going to be added.\nAnd the source port number is going be a and the destination port number is j. So this data is added with A and J which are the source and destination port numbers respectively.\nAfter the transport layer data is added, it is then given to the network layer. And in this network layer, whatever it has received from the transport layer, it is going to add with the source IP address and the destination IP address. So A is the source IP address. (Making the things simple, we have used it as A and P. So actually IP addresses are different. It has four octets, but for simplicity we are taking it as A and P here).\nSo A and P are the source and destination IP addresses respectively. After adding the source and destination IP address, now the data is given to the data link layer.\nThe data link layer, whatever it has received from the network layer, it is going to be added with header and the trailer. Header contains source MAC address and the destination MAC address and trailer deals with the error control related part.\nFor time being, you just understand data link layer adds a header and a trailer where header has source MAC address and destination MAC address related information and the trailer has error control related information.\nHere, the destination MAC address means it is not the MAC address of this receiver. Because, it is very very difficult to find the destination MAC address. So what this computer does, it puts the MAC address of the immediate router or the first router it is hitting. That router’s MAC\naddress will be used as the destination MAC address here."},"Computer-Networking/L-18-(Best)---Addressing-in-Networking":{"slug":"Computer-Networking/L-18-(Best)---Addressing-in-Networking","filePath":"Computer Networking/L 18 (Best) - Addressing in Networking.md","title":"L 18 (Best) - Addressing in Networking","links":[],"tags":[],"content":"Watch the below lecture from 4:30 minutes.\nyoutu.be/yDTC6sbYFFE"},"Computer-Networking/L-19---The-TCP-IP-Protocol-Suite":{"slug":"Computer-Networking/L-19---The-TCP-IP-Protocol-Suite","filePath":"Computer Networking/L 19 - The TCP-IP Protocol Suite.md","title":"L 19 - The TCP-IP Protocol Suite","links":[],"tags":[],"content":"&lt;img src = /assets/Pasted-image-20250623-4.png&gt;\nThe TCP/IP protocol suite was actually developed prior to the OSI model. And therefore, the layers in the TCP/IP protocol suite do not exactly match with the OSI model.\nThe functionalities of the physical layer and the data link layer are combined into a single layer called Network Access layers.\nThe network layer’s equivalent in the TCP/IP model is internet layer. The transport layer is the same. And the functionalities of the session layer,  presentation layer and  application layer are merged into a single layer called application layer.\nActually TCP/IP model was developed prior to the OSI model. So this is the actual implemented model and OSI is just a reference model— just the guidelines. In some books, TCP/IP model is shown with five layers. So no problem whether it is a five layered architecture or four layered architecture, the functionalities still remains the same.\nIf you want TCP/IP model with five layers, just break this network access layer in to physical layer separately and data link layer separately. There are some protocols that are there in every layer of the TCP/IP model.\nThere are some protocols\nthat are there in every layer of the TCP/IP model because the protocol only does\nthe functionality of that layer. If you take network access layer, point-to-point protocol\nsimply PPP, frame layer is very popular\nin network access layer, Ethernet is a very popular protocol\nin the network access layer.\nComing to the internet layer,\nwe have IP protocol. IP means it’s simply IPv4 or IPv6. ICMPv4 and ICMPv6.\nIn the transport layer,\nwe have TCP and UDP.\nIn the application layer we have\nHTTP, DNS, DHCP and FTP.\nIt’s quite difficult to understand\nall the protocols now, even the expansions.\nBut let it be as such, anyway we are going to\ntalk about these protocols elaborately in the entire\ncourse, so no worries. For time being, we will just have\nthe expansion of this protocol and the basic working of\nthese protocols.\n&lt;img src = /assets/Pasted-image-20250623-5.png&gt;\nNow, above screenshot shows the\nactual TCP/IP model. So the TCP/IP model\nhas four layers network access layer,\nso it controls the hardware because physical layer is a part\nof network access layer. So it controls the hardware devices and media that make up the network.\nAnd coming to the internet layer, this is the network layer actually, so it determines the best path\nthrough the network.\nAnd coming to the transport layer, it supports communication\nbetween diverse devices across diverse networks. Simply transport layer deals with process to process communication.\nAnd coming to the application layer, this layer is actually dealing with the users. So representing the data to the user, plus encoding and dialog control. Simply the functionalities\nof application layer, presentation layer and session layer are combined into a single layer.\nNow we will see the TCP/IP protocol suite.\nSo network access layer\nhas protocols like point-to-point protocol (PPP), Ethernet protocol, it is\na very popular protocol for LAN technologies, to be precise, wired LAN technologies and there are many interface drivers that work on this network access layer.\nAnd coming to the internet layer, the routing protocols will\ncome in the internet layer. We know very well, router is a device that works with the network layer. Routing protocols like RIP, OSPF, EIGRP BGP. We will talk about these protocols elaborately in the network layer part.\nAnd we have IP support protocol like ICMP and NAT, Network Address Translation. And coming to the transport layer, the widely used transport layer protocols are TCP and UDP. TCP means Transmission Control Protocol and UDP means User Datagram Protocol. So any application will use\neither TCP or UDP. We will talk about these protocols, transport layer protocols in the upcoming lecturers\nin the network layer part.\nAnd coming to the application layer protocol, we have domain name servers\nor domain name system, that is the name system. There are host configuration protocols like BOOTP or Bootstrap Protocol, Dynamic Host Configuration Protocol\nor simply DHCP.\nAnd we have email related protocols like Simple Mail Transfer Protocols (SMTP), Post Office Protocol or simply POP or Internet Message Access Protocol\nor simply IMAP.\nAnd we have File Transfer protocols like FTP- File Transfer Protocol and Trivial File Transfer Protocol.\nAnd to access the web or\nweb pages, we have HTTP. So these are the popular\nprotocols in the application layer. This is the TCP/IP protocol suite.\n&lt;img src = /assets/Pasted-image-20250623-6.png&gt;\nNow we will see what is PDU that is Protocol Data Unit.\nProtocol data units are named according to the protocols of the TCP/IP suite. It means the PDUs can be data, segment, packet, frames, and bits. Whatever the user generates that is from the application layer, that application layer information or that application layer PDU is called as simply data. So, application layer PDU are simply called as data. With application layer data, we will be obviously adding the transport layer header or transport layer information. So, once the transport layer information is added with the application layer data, we will call that as segment.\nSo, the transport layer PDU is called as a segment. After adding the transport layer information, we will be adding the network layer information. Once the network layer header is added, we will call that as packet. So, the network layer PDU is called as a packet. Now with this network layer, we will be adding the data link layer part, that is the header and the trailer. So, with the packet we will be adding the header and the trailer in the data link layer. After adding the header and the trailer to the packet, the data link layer PDU is called as a frame.\nNow, these frames are converted into zeros and ones in the physical layer. So, the physical layer PDU are called as bits. The application layer PDU is called as data, the transport layer PDU is called as segment, the network layer PDU is called as packet, the data link layer PDU is called as frame, and the physical layer PDU is called as bits.\n&lt;img src = /assets/Pasted-image-20250623-7.png&gt;\nWatch the lecture from 7:27 minutes.\nyoutu.be/wvPe4Zb0tUA\nAbove screenshot shows an example.\nSo, in this example, there is a user who is accessing a mail server\nsomewhere in the Internet. So there is a mail server, let’s assume this is gmail.com. He has opened a gmail.com in his browser and he is generating the application layer data. So this email data whatever he is generating, is simply called as data\nin the application layer. So we can say, the application layer PDU\nis called as data, If it is a big data, obviously it is\nbroken into smaller pieces. Now each of these smaller pieces are added with the transport\nheader in the transport layer. Transport layer PDU is\nnow called as a segment. Now this segment,\nthis is actually the segment. So, with this content, we are going to add\nthe network layer header. After adding the network layer header, we will call this network\nlayer PDU as a packet.\nNow, with this information, we are going to add a header and a trailer in the data link layer. This data link layer PDU is called as a frame.\nNow, this frame is a medium dependent frame, why because, if this is an ethernet cable, this frame is going to be different. If it is a Wi-Fi frame, it is different. So, Ethernet frames are different and Wi-Fi frames are different. And that’s why the frames are medium dependent. And finally, the data link layer information are converted into zeros and ones. So, the physical layer PDU are called as bits. So, application layer PDU or simply data, transport layer PDU- segment, network layer PDU- packet, data link layer PDU- frames, and physical layer PDU- bits. We will understand the significance of these terms like data, segment, packet, frames, and bits in the upcoming lectures."},"Computer-Networking/L-20---Basic-Networking-Commands-(Part-1)":{"slug":"Computer-Networking/L-20---Basic-Networking-Commands-(Part-1)","filePath":"Computer Networking/L 20 - Basic Networking Commands (Part 1).md","title":"L 20 - Basic Networking Commands (Part 1)","links":[],"tags":[],"content":"Watch the video to know about the commands.\nVideo URL - youtu.be/rurs7cdT5cc"},"Computer-Networking/L-21---Basics-of-Cisco-Packet-Tracer-(Part-1)":{"slug":"Computer-Networking/L-21---Basics-of-Cisco-Packet-Tracer-(Part-1)","filePath":"Computer Networking/L 21 - Basics of Cisco Packet Tracer (Part 1).md","title":"L 21 - Basics of Cisco Packet Tracer (Part 1)","links":[],"tags":[],"content":"Watch the Tutorials\nL 21 - youtu.be/frUQMHXhnvs"},"Computer-Networking/L-22---Basics-of-Cisco-Packet-Tracer-(Part-2)---Hub":{"slug":"Computer-Networking/L-22---Basics-of-Cisco-Packet-Tracer-(Part-2)---Hub","filePath":"Computer Networking/L 22 - Basics of Cisco Packet Tracer (Part 2) - Hub.md","title":"L 22 - Basics of Cisco Packet Tracer (Part 2) - Hub","links":[],"tags":[],"content":"Tutorial Link - youtu.be/FZ8hRDakHvI\n&lt;img src = /assets/Pasted-image-20250623-8.png&gt;\nHub comes under star topology.\n&lt;img src = /assets/Pasted-image-20250623-9.png&gt;\nWhen a packet arrives at one port of Hub, it is copied to the other ports so that all segments of the LAN can see all packets - this is a drawback of Hub.\n&lt;img src = /assets/Pasted-image-20250623-10.png width=400&gt;\nBecause of the above disadvantages “Hub” is replaced by “Switch” in most places.\nHub broadcast or send any received packet (over any port) to all the ports - this is a disadvantage. Where as Switch don’t broadcast packets to all ports, it only send the packet to intended port (this is unicasting).\nBut it does not mean Switch can’t do broadcasting, switch can do broadcasting also.\nSwitch &amp; Hub - both are used to setup local area network (LAN)\n\nHub is a Layer-1 device (that is physical layer)\nSwitch is a Layer-2 device (that is data link layer)\n"},"Computer-Networking/L-23---Basics-of-Cisco-Packet-Tracer-(Part-3)---Switch":{"slug":"Computer-Networking/L-23---Basics-of-Cisco-Packet-Tracer-(Part-3)---Switch","filePath":"Computer Networking/L 23 - Basics of Cisco Packet Tracer (Part 3) - Switch.md","title":"L 23 - Basics of Cisco Packet Tracer (Part 3) - Switch","links":[],"tags":[],"content":"Tutorial Link - youtu.be/eFY6mi3lmRQ\n&lt;img src = /assets/Pasted-image-20250623-11.png&gt;\n&lt;img src = /assets/Pasted-image-20250623-12.png&gt;\n&lt;img src = /assets/Pasted-image-20250623-13.png&gt;"},"Computer-Networking/L-24---Basics-of-Router":{"slug":"Computer-Networking/L-24---Basics-of-Router","filePath":"Computer Networking/L 24 - Basics of Router.md","title":"L 24 - Basics of Router","links":[],"tags":[],"content":""},"DevOps/CD-Pipeline-in-High-Level":{"slug":"DevOps/CD-Pipeline-in-High-Level","filePath":"DevOps/CD Pipeline in High-Level.md","title":"CD Pipeline in High-Level","links":["DevOps/Docker/Docker-and-Dockerfile","DevOps/K8/Secret-from-AWS-SM","DevOps/K8/Types-of-Kubernetes-Deployment"],"tags":[],"content":"Stages\n\n\nGit Checkout\n\n\nDocker image build - See Docker and Dockerfile\n\n\nDocker Image push\n\n\nSecret Deployment - See the Secret from AWS SM\n\n\nDeployment - See Types of Kubernetes Deployment\n\n\nService Deployment\n\n\nSending Notification\n\n"},"DevOps/CI-Pipeline-in-High-Level":{"slug":"DevOps/CI-Pipeline-in-High-Level","filePath":"DevOps/CI Pipeline in High Level.md","title":"CI Pipeline in High Level","links":[],"tags":[],"content":""},"DevOps/Cloud/AWS-Services":{"slug":"DevOps/Cloud/AWS-Services","filePath":"DevOps/Cloud/AWS Services.md","title":"AWS Services","links":[],"tags":[],"content":"Popular AWS Services\n\nCompute Services\n\n\nAmazon EC2 (Elastic Compute Cloud):\n\nDescription: This is like renting virtual computers (servers) in the cloud. You can choose their size, operating system, and how much power they have to run your applications.\nAnalogy: Your personal computer, but hosted and managed by AWS in a data center.\n\n\n\nAWS Lambda:\n\nDescription: A “serverless” compute service that lets you run your code without having to provision or manage servers. You just upload your code, and Lambda automatically runs it when needed, scaling up and down automatically. You only pay when your code runs.\nAnalogy: A “function-on-demand” service. You don’t manage the kitchen, just provide the recipe.\n\n\n\nAmazon ECS (Elastic Container Service) / Amazon EKS (Elastic Kubernetes Service):\n\nDescription: Services for running and orchestrating containers (like Docker containers). ECS is Amazon’s own container orchestration service, while EKS allows you to run the popular Kubernetes container orchestration software on AWS.\nAnalogy: A large automated factory floor designed specifically for running many small, self-contained mini-apps (containers).\n\n\n\n\nStorage Services\n\n\nAmazon S3 (Simple Storage Service):\n\nDescription: Object storage for virtually unlimited amounts of data (files, images, videos, backups). It’s highly durable, scalable, and cost-effective. You access data via web links.\nAnalogy: An infinitely large, secure digital locker or hard drive accessible over the internet.\n\n\n\nAmazon EBS (Elastic Block Store):\n\nDescription: Block storage volumes specifically designed to attach to and be used by Amazon EC2 instances. It’s like a virtual hard drive for your virtual servers.\nAnalogy: The C: drive (or D: drive, etc.) attached directly to your rented virtual computer.\n\n\n\n\nDatabase Services\n\n\nAmazon RDS (Relational Database Service):\n\nDescription: A managed service that makes it easy to set up, operate, and scale a relational database (like MySQL, PostgreSQL, Oracle, SQL Server). AWS handles the patching, backups, and scaling.\nAnalogy: A fully managed, self-driving database rather than one you have to install and maintain yourself.\n\n\n\nAmazon DynamoDB:\n\nDescription: A fast, flexible, fully managed NoSQL database service for applications that need single-digit millisecond performance at any scale. It’s great for mobile, web, gaming, and IoT applications.\nAnalogy: A super-fast, highly scalable, flexible database designed for massive amounts of quick reads and writes, without the rigid structure of a traditional spreadsheet-like database.\n\n\n\nAmazon Aurora:\n\nDescription: AWS’s own high-performance, fully managed relational database engine compatible with MySQL and PostgreSQL. It offers the speed and availability of commercial databases at a fraction of the cost.\nAnalogy: An upgraded, optimized version of RDS for MySQL/PostgreSQL, built for even higher performance and reliability.\n\n\n\n\nNetworking &amp; Content Delivery\n\n\nAmazon VPC (Virtual Private Cloud):\n\nDescription: Lets you provision a logically isolated section of the AWS Cloud where you can launch AWS resources in a virtual network that you define. It’s your own private corner of the AWS cloud.\nAnalogy: Your own private, secure office building within the larger AWS city, where you control all the walls, doors, and internet connections.\n\n\n\nAmazon Route 53:\n\nDescription: A highly available and scalable cloud Domain Name System (DNS) web service. It translates human-readable domain names (like example.com) into computer-readable IP addresses.\nAnalogy: The internet’s phone book that helps your browser find the right server when you type a website address.\n\n\n\nAmazon CloudFront:\n\nDescription: A fast Content Delivery Network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency and high transfer speeds. It caches content closer to your users.\nAnalogy: A global network of express couriers that deliver your website content to users from the closest possible location, making your website load faster.\n\n\n\n\nSecurity, Identity, &amp; Management\n\n\nAWS IAM (Identity and Access Management):\n\nDescription: Manages who can access your AWS resources (users, groups, roles) and what actions they can perform. It’s fundamental for security.\nAnalogy: The security guard and permissions system for your AWS account, controlling who has keys to which parts of your building.\n\n\n\nAmazon CloudWatch:\n\nDescription: A monitoring and observability service that provides data and actionable insights to monitor your applications, respond to system-wide performance changes, and optimize resource utilization.\nAnalogy: The surveillance cameras, sensors, and alarm system for your entire AWS infrastructure.\n\n\n\nAWS CloudFormation:\n\nDescription: Lets you model and provision all your AWS infrastructure resources (like EC2 instances, S3 buckets, databases) in a code file (e.g., JSON or YAML). This is known as “Infrastructure as Code.”\nAnalogy: A blueprint or recipe that automatically builds your entire cloud setup exactly as specified, every time.\n\n\n\nAWS RAM (Resource Access Manager):\n\nDescription: A service that enables you to securely and easily share your AWS resources (like VPC subnets, Transit Gateways, etc.) with other AWS accounts or within your AWS Organization. This helps avoid duplication and simplifies management.\nAnalogy: A digital “resource sharing key” system for your cloud resources. You grant specific accounts or departments keys to use your resources without giving them ownership or forcing them to build their own duplicates.\n\n\n"},"DevOps/Cloud/Azure-Services":{"slug":"DevOps/Cloud/Azure-Services","filePath":"DevOps/Cloud/Azure Services.md","title":"Azure Services","links":[],"tags":[],"content":"Popular Azure Services\nCompute Services\n\n\nAzure Virtual Machines (VMs):\n\nDescription: Allows you to create and run virtual computers (servers) in the cloud. You choose the operating system, size, and resources, just like owning a server without the physical hardware.\nAnalogy: Renting a customizable computer from a massive data center.\n\n\n\nAzure Functions:\n\nDescription: A “serverless” compute service that lets you run small pieces of code (functions) in response to events, without managing any servers. You only pay for the time your code is actually running.\nAnalogy: A “recipe execution service” where you just provide the recipe, and Azure handles all the kitchen setup, cooking, and cleanup automatically whenever someone needs that dish.\n\n\n\nAzure Kubernetes Service (AKS):\n\nDescription: A managed service for deploying, managing, and scaling containerized applications using Kubernetes. Azure handles the complexity of managing the Kubernetes control plane.\nAnalogy: An automated factory floor manager for your software components (containers), optimizing where and how they run to meet demand.\n\n\n\nAzure App Service:\n\nDescription: A fully managed platform for building, deploying, and scaling web apps, mobile backends, and RESTful APIs quickly, without worrying about infrastructure. Supports various languages and frameworks.\nAnalogy: A specialized web server rental service where you just upload your website files, and Azure takes care of the hosting, scaling, and maintenance.\n\n\n\n\nStorage Services\n\n\nAzure Blob Storage:\n\nDescription: Object storage for massive amounts of unstructured data (like images, videos, documents, backups). It’s highly scalable, durable, and accessible via HTTP/HTTPS.\nAnalogy: An infinitely vast and organized digital warehouse for all your files, accessible from anywhere.\n\n\n\nAzure Disk Storage:\n\nDescription: Block-level storage volumes designed to be attached to Azure Virtual Machines. It provides persistent data storage for your virtual servers.\nAnalogy: The virtual hard drive that you plug directly into your rented virtual computer.\n\n\n\nAzure Files:\n\nDescription: Managed file shares that you can access via the industry-standard Server Message Block (SMB) protocol or NFS protocol, from both on-premises and cloud deployments.\nAnalogy: A shared network drive (like your company’s Z: drive) that you can access securely from anywhere, on-premises or in the cloud.\n\n\n\n\nDatabase Services\n\n\nAzure SQL Database:\n\nDescription: A fully managed relational database service that provides the core capabilities of SQL Server as a service, without managing the underlying infrastructure.\nAnalogy: A fully managed, self-driving version of a familiar SQL database, where Azure handles all the maintenance and tuning.\n\n\n\nAzure Cosmos DB:\n\nDescription: A globally distributed, multi-model NoSQL database service that provides guaranteed low latency, high availability, and scalability for mission-critical applications.\nAnalogy: A super-fast, infinitely scalable database that can store many types of data and is designed to respond almost instantly, no matter where your users are in the world.\n\n\n\nAzure Database for MySQL/PostgreSQL/MariaDB:\n\nDescription: Fully managed database services for popular open-source relational database engines. Azure handles patching, backups, and scaling for you.\nAnalogy: A professional service that takes care of all the headaches of running and maintaining open-source relational databases for you.\n\n\n\n\nNetworking &amp; Content Delivery\n\n\nAzure Virtual Network (VNet):\n\nDescription: Enables you to provision your own private, isolated network in Azure where you can deploy your cloud resources. You have full control over your network environment.\nAnalogy: Your own private, secure office campus within the vast Azure cloud city, where you control all the roads, buildings, and security.\n\n\n\nAzure DNS:\n\nDescription: A hosting service for DNS domains that provides name resolution using Microsoft Azure infrastructure. It’s how domain names are translated into IP addresses.\nAnalogy: The Azure-managed “internet phone book” that directs web traffic to the correct Azure services when someone types your website address.\n\n\n\nAzure CDN (Content Delivery Network):\n\nDescription: A global CDN solution for delivering high-bandwidth content (like videos, images, and web pages) to users with high performance and reliability, by caching content at edge locations worldwide.\nAnalogy: A worldwide network of express delivery stations that store copies of your website’s content and deliver them to users from the closest possible location, speeding up access.\n\n\n\n\nIdentity &amp; Security\n\n\nMicrosoft Entra ID (formerly Azure Active Directory):\n\nDescription: A cloud-based identity and access management service that helps employees sign in and access resources (internal apps, cloud apps like Microsoft 365, Azure portal).\nAnalogy: Your company’s central digital ID card and access control system for all its applications and cloud resources.\n\n\n\nAzure Key Vault:\n\nDescription: A service for securely storing and managing sensitive information like cryptographic keys, passwords, certificates, and other secrets used by cloud applications and services.\nAnalogy: A highly secure, tamper-proof digital safe deposit box for your most sensitive credentials.\n\n\n\n\nManagement &amp; Governance\n\n\nAzure Monitor:\n\nDescription: A comprehensive solution for collecting, analyzing, and acting on telemetry from your cloud and on-premises environments. It provides full-stack observability.\nAnalogy: The control room with all the monitors, sensors, and alert systems that watch over your entire Azure setup, telling you exactly what’s happening.\n\n\n\nAzure Resource Manager (ARM Templates):\n\nDescription: The deployment and management service for Azure. It enables you to create, update, and delete resources in your Azure subscription. ARM templates allow you to deploy infrastructure as code.\nAnalogy: A powerful construction manager and blueprint system that lets you define and automatically build your entire Azure infrastructure setup using code, ensuring consistency.\n\n\n"},"DevOps/Cloud/SaaS-vs-PaaS-vs-IaaS---Cloud-Service-Models":{"slug":"DevOps/Cloud/SaaS-vs-PaaS-vs-IaaS---Cloud-Service-Models","filePath":"DevOps/Cloud/SaaS vs PaaS vs IaaS - Cloud Service Models.md","title":"SaaS vs PaaS vs IaaS - Cloud Service Models","links":[],"tags":[],"content":""},"DevOps/DevOps-Learning-Perspective":{"slug":"DevOps/DevOps-Learning-Perspective","filePath":"DevOps/DevOps Learning Perspective.md","title":"DevOps Learning Perspective","links":[],"tags":[],"content":"“In every task what you do, it’s not about only implementation, but most importantly its about learning on high-level, in a bigger system what exactly your part is, try to understand the concept and develop a bird eye understanding of your task. Implementation can be done/learn any time”\nIn other words -\n\nElevate your perspective: Don’t just see the task; see how it fits into the larger system.\nGrasp the “why”: Understand the concept behind what you’re doing and the overall purpose it serves.\nDevelop a “bird’s-eye view”: This helps you see your piece of the puzzle within the entire landscape.\n\nEssentially, the technical “how-to” (the implementation) can always be learned or figured out later. What truly makes you a valuable engineer is your ability to understand the purpose, context, and impact of your work within the broader architecture.\nTools will come and go, but you need to develop a mindset, core understanding, system-level understanding.\n“In DevOps, specific tools are temporary, but cultivating a strong mindset and a deep, system-level understanding is what truly provides lasting value. That foundational knowledge is your enduring asset.”"},"DevOps/Docker/Docker-Commands":{"slug":"DevOps/Docker/Docker-Commands","filePath":"DevOps/Docker/Docker Commands.md","title":"Docker Commands","links":[],"tags":[],"content":"Docker Run command:\ndocker run -p 8080:5000 my-image:v1\n“-p”: publish\n“5000”: actual app is running at port 5000\n“8080”: we will be access the app from machine using port 8080\n\ndocker run → start a new container.\ndocker stop, docker rm → stop and clean up.\ndocker exec → run a command inside a running container.\n"},"DevOps/Docker/Docker-Volume":{"slug":"DevOps/Docker/Docker-Volume","filePath":"DevOps/Docker/Docker Volume.md","title":"Docker Volume","links":[],"tags":[],"content":"Docker volumes are the preferred mechanism for persisting data in Docker containers. They allow containers to store data outside of the container’s writable layer—enabling data persistence, sharing, and backup.\n🔷 1.1 What is a Docker Volume?\nA Docker volume is a special directory that lives outside of the container’s filesystem but is mounted inside the container. Volumes are managed by Docker and live under /var/lib/docker/volumes/.\n🔷 1.2 Types of Docker Storage Mounts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTypeDescriptionBind MountMounts a specific path on the host into the container.Volume MountUses Docker-managed storage, abstracted from host paths.tmpfsMounts temporary filesystems in memory. (not persisted)\nIn two way we can mount a drive or location from local machine (or docker host) to the docker container.\n\n“bind mount”\n“volume mount”\n\nBind mount:\n🔹 Concept:\n\n\nYou specify an exact host path (absolute) and mount it into a container.\n\n\nGreat for development, logs, or sharing files between host and container.\n\n\nChanges made manually on the host (to the source directory of the bind mount) are immediately reflected inside the Docker container, and vice-versa.\n\n\nYou do not need to restart the container.\n\n\nThis is why bind mounts are so popular for development workflows: you can edit code on your host machine with your favorite IDE, and the changes are instantly available to the application running inside the container.\n\n\nExample of bind mount:\ndocker run -d -p 8080:5000 -v /path/on/host:/app/code my-flask-app\n\n🔹 Characteristics:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeatureDescriptionVisibilityDirect view of host filesystemPortability❌ Not portable across machinesSecurity⚠️ Risky if not controlledUse CaseDev, debugging, local mounting\nVolume Mount:\n🔹 Concept:\n\n\nYou create and use volumes managed by Docker itself.\n\n\nDocker stores it in /var/lib/docker/volumes/ (Linux).\n\n\nIdeal for production usage, where you want abstraction and isolation.\n\n\nChanges made manually to the volume’s underlying storage location on the host are generally NOT recommended or directly reflected in a way you’d typically interact with.\n\n\nDocker manages the lifecycle and location of named volumes. While the data inside the volume persists, directly modifying the files at Docker’s internal storage location for that volume is discouraged and can lead to issues.\n\n\nThe primary way to interact with data in a named volume is through the container itself.\n\n\nExample of volume:\ndocker volume create my-app-data\ndocker run -d -p 8080:5000 -v my-app-data:/app/data my-flask-app\n\n🔹 Characteristics:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeatureDescriptionPortability✅ Easy to migrate/backupStorage driversCan integrate with plugins (e.g., NFS, EBS)LifecycleIndependent from containerUse CaseDBs, log retention, stateful apps\n🔷 1.3 Volume Lifecycle Commands\ndocker volume create my-vol\ndocker volume inspect my-vol\ndocker volume rm my-vol\nVolumes vs Bind Mounts:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeatureBind MountDocker VolumeWhere defined?Host path specified manuallyManaged by DockerUse CaseDev: share code or config with containerProd: persistent data for DBs, logs, etc.Managed by Docker❌ No✅ YesBackup/Restore❌ Manual✅ Docker volume commandsPortable❌ Host-specific✅ Easily moved/shared\n\nSummary:\n\n\nBind Mounts (-v /host/path:/container/path): Changes are instantly reflected in both directions. No restart needed. Ideal for code development.\n\n\nNamed Volumes (-v my-volume-name:/container/path): Changes are not typically made manually on the host to the underlying Docker-managed storage. Interaction with the data within the volume should primarily be done via the container(s) that mount it. Manual host-side modification is not the standard or recommended practice.\n\n"},"DevOps/Docker/Docker-and-Dockerfile":{"slug":"DevOps/Docker/Docker-and-Dockerfile","filePath":"DevOps/Docker/Docker and Dockerfile.md","title":"Docker and Dockerfile","links":[],"tags":[],"content":"What is Docker?\nDocker is a platform that allows developers to build, ship, and run applications within lightweight, portable containers\nWhat is container?\nA container or a Docker container is a lightweight, isolated package that holds an application and everything it needs to run (code, libraries, dependencies, configuration) so it works consistently anywhere.\n\nDockerfile in High-Level\n\nBelow is an example Dockerfile of a Java Spring-Boot application\nFROM eclipse-temurin:8-jdk  ---- Base Image\nADD risk-listener.jar app.jar   ---- Adding the JAR file\nEXPOSE 8080 ---- exposing a port through which application will receive request\nENV SPRING_PROFILES_ACTIVE=not-local  ---- setting an environment variable\nCMD [“java”, “-jar”, “/app.jar”]  ---- starting the application\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstruction in DockerfileWhat is it?Bare minimum requirement?CommentsFROM eclipse-temurin:8-jdkBase ImageYesIt defines the base image upon which your application will run. You need a Java Development Kit (JDK) or Java Runtime Environment (JRE) to run Java applications. eclipse-temurin:8-jdk provides Java 8 JDK, which is suitable for building and running.ADD risk-listener.jar app.jarAdding the JAR fileYesYou need a way to get your compiled Java application (typically a .jar or .war file) into the Docker image. ADD (or COPY) serves this purpose. ADD has some extra features (like URL fetching and automatic tar extraction) that COPY doesn’t, but for a local JAR file, COPY is often preferred as it’s more explicit and avoids potential pitfalls of ADD’s extra features.EXPOSE 8080exposing a port through which application will receive requestNoThe EXPOSE instruction is primarily documentation. It tells anyone inspecting the image which ports the application inside the container intends to listen on. It does not actually publish or open the port on the host machine. Docker:For the application to be accessible from outside the Docker host, you need to use the -p or --publish flag with docker run (e.g., docker run -p 8080:8080 my-image).Kubernetes:We can specify port using K8 service manifest (YAML) file.ENV SPRING_PROFILES_ACTIVE=not-localsetting an environment variableNoThis line sets an environment variable inside the container. This is specific to your application’s configuration (in this case, a Spring Boot application using profiles). Not all Java applications use Spring Boot, nor do all Java applications require specific environment variables.CMD [“java”, “-jar”, “/app.jar”]starting the applicationYesThe CMD instruction provides the default command that will be executed when a container is started from this image. Without CMD (or ENTRYPOINT), Docker wouldn’t know what process to run when you start the container, and it would immediately exit (unless you specify a command at docker run time).For an executable JAR, java -jar /app.jar is the standard way to run it.\nAbsolute Bare Minimum Instruction in Dockerfile for any Java Application\n\nThe below is bare minimum Dockerfile for any Java application.\nFROM eclipse-temurin:8-jdk\nADD my-app.jar app.jar\nCMD [“java”, “-jar”, “/app.jar”]\nENTRYPOINT vs CMD ?\n✅ 1. CMD (Command)\n\nPurpose: Specifies default arguments for the container when no command is provided at runtime.\nCan be overridden by runtime arguments in docker run.\nExample:\n\nFROM ubuntu\nCMD [&quot;echo&quot;, &quot;Hello from CMD&quot;]\n\nIf you run:\ndocker run myimage\n➡️ It prints: Hello from CMD\nBut if you run:\ndocker run myimage echo &quot;Overridden&quot;\n➡️ It prints: Overridden (CMD is replaced)\n✅ 2. ENTRYPOINT\n\nPurpose: Sets the main command to run when container starts.\nNot easily overridden by runtime arguments unless explicitly done.\nOften used to wrap shell scripts or commands where you want to append runtime args.\n\nExample:\nFROM ubuntu\nENTRYPOINT [&quot;echo&quot;]\n\nNow run:\ndocker run myimage Hello\n➡️ Output: Hello (appended to ENTRYPOINT)\n🆚 ENTRYPOINT vs CMD Summary Table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeatureENTRYPOINTCMDPurposeMain container commandDefault argumentsOverridden easily?No (unless --entrypoint)Yes (with runtime command)Common Use CaseWrapper scripts, binariesConfig defaults or flagsCombined?Yes (ENTRYPOINT + CMD)CMD = default args\n✅ Example: Combining Both\nFROM ubuntu\nENTRYPOINT [&quot;curl&quot;]\nCMD [&quot;example.com&quot;]\n\n\ndocker run myimage → runs: curl example.com\ndocker run myimage openai.com → runs: curl openai.com\n\n\n"},"DevOps/Docker/Networking-in-Docker-Containers":{"slug":"DevOps/Docker/Networking-in-Docker-Containers","filePath":"DevOps/Docker/Networking in Docker Containers.md","title":"Networking in Docker Containers","links":[],"tags":[],"content":"Containers are isolated but can communicate:\n\nBridge network (default): for containers on same host.\nHost network: shares host’s network namespace.\nOverlay network: for multi-host (used in Swarm/K8s).\n"},"DevOps/Environments":{"slug":"DevOps/Environments","filePath":"DevOps/Environments.md","title":"Environments","links":[],"tags":[],"content":"Non-Prod Envs\n\nPaaS DEV:\nPlayground for Developers, DevOps, etc peoples.\nDEV:\n\nDescription: The primary working ground for individual developers or development teams to write, debug, and locally test new features, bug fixes, and code changes.\nSpecific Criteria/Purpose: It’s highly dynamic, with frequent code changes. Features are often incomplete or unstable here, as the focus is on active coding and unit testing.\n\nTST or SIT (Test or System Integration Testing):\n\nDescription: A more stable non-production environment used for formal testing by Quality Assurance (QA) teams.\nSpecific Criteria/Purpose: This is where System Integration Testing (SIT) takes place, ensuring that all individual components, modules, and integrated systems (e.g., APIs, databases, third-party services) work correctly together as a cohesive whole. The environment aims for higher stability than DEV and uses more comprehensive test data.\n\nUAT (User Acceptance Testing):\n\nDescription: An environment where actual end-users, business stakeholders, or clients test the system to verify it meets their original business requirements and is truly fit for their daily operations.\nSpecific Criteria/Purpose: This is often considered the last formal testing stage before production. The focus is on real-world use cases and “acceptance.” User sign-off is typically required to proceed to later stages. Data is usually realistic or anonymized production data.\n\nMAT (Market Acceptance Testing):\n\nDescription: A specialized non-production environment used for final validation before a product is released to a broader market or audience.\nSpecific Criteria/Purpose: It often involves a limited group of pilot users or early adopters, simulating real-world usage and gathering feedback to ensure market readiness. This is crucial for products targeting specific customer segments or undergoing a soft launch.\n\nProd Envs\n\nNFT (Non Functional Testing):\n\nDescription: This is an environment specifically configured to rigorously test non-functional requirements such as performance, scalability, reliability, load capacity, stress tolerance, and security.\nSpecific Criteria/Purpose: It is built to closely mirror the production environment’s hardware, network, and data volume as much as possible to ensure accurate performance metrics. Its goal is to identify bottlenecks and ensure the system can handle expected (and peak) user loads before going live.\n\nPre-Prod or Staging Env:\n\nDescription: A high-fidelity replica of the actual Production environment. It serves as the final checkpoint for code and configurations before deployment to live users.\nSpecific Criteria/Purpose: This is where final end-to-end testing, smoke testing, and last-minute sanity checks occur. It should ideally have production-like data (anonymized if sensitive) and configurations. Its critical role is to mitigate deployment risks by catching any issues that might arise only in a production-like setting.\n\nProd (Production):\n\nDescription: The live, operational environment that end-users interact with directly. This is where the application or system is actually running and serving its intended purpose.\nSpecific Criteria/Purpose: It handles real customer data, real transactions, and requires the highest level of stability, security, and uptime. All issues here require immediate attention and resolution. Monitoring and incident response are paramount.\n"},"DevOps/K8-Authentication/Client-Certificate-Authentication":{"slug":"DevOps/K8-Authentication/Client-Certificate-Authentication","filePath":"DevOps/K8 Authentication/Client Certificate Authentication.md","title":"Client Certificate Authentication","links":[],"tags":[],"content":"This is used by:\n\nCluster admins\nKubelets (nodes) to securely talk to kube-apiserver\n\nIt uses X.509 client certificates signed by a trusted CA.\n📘 Use Case:\n\nAn admin uses kubectl with a certificate signed by the cluster CA\nThe API server verifies the cert’s signature and extracts the username/group from the certificate’s subject\n\n🧠 Core Concepts:\n\nCertificate must be signed by a CA that API server trusts (--client-ca-file)\nIdentity is extracted from certificate fields (CN, O)\nNo password/token — pure mutual TLS\n\n"},"DevOps/K8-Authentication/How-kube-apiserver-combines-all-auth-layers":{"slug":"DevOps/K8-Authentication/How-kube-apiserver-combines-all-auth-layers","filePath":"DevOps/K8 Authentication/How kube-apiserver combines all auth layers.md","title":"How kube-apiserver combines all auth layers","links":[],"tags":[],"content":"The Kubernetes API server supports multiple authentication methods, and evaluates them in order, stopping at the first method that succeeds.\n✅ Evaluation Order (as per docs &amp; internal flow):\n\nClient Certificates (e.g., kubelet, admins)\nBearer Tokens:\n\nServiceAccount Tokens\nOIDC ID Tokens (JWT)\nStatic Tokens\n\n\nWebhook Token Authentication\nAnonymous Access (only if explicitly allowed)\n\n🔁 The API server tries each method in this order until one authenticates the request.\n🔐 What Happens Internally:\n\nA user or service sends an API request (usually HTTPS)\nThe API server:\n\nExtracts credentials (token, cert, etc.)\nEvaluates against each configured authenticator\n\n\nIf authentication succeeds, user identity is established\nThen RBAC/ABAC is evaluated for authorization\nIf everything passes → request is served\nIf not → rejected with 401 Unauthorized or 403 Forbidden\n\n✅ Summary\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStepWhat Happens1Request comes to API server2Tries to authenticate (Cert → Token → Webhook → Anonymous)3First success wins (others skipped)4RBAC/ABAC checked5Response sent (200 OK or 403/401)\n"},"DevOps/K8-Authentication/Kubernetes-Authentication-and-Authorization-Techniques":{"slug":"DevOps/K8-Authentication/Kubernetes-Authentication-and-Authorization-Techniques","filePath":"DevOps/K8 Authentication/Kubernetes Authentication and Authorization Techniques.md","title":"Kubernetes Authentication and Authorization Techniques","links":["DevOps/K8-Authentication/ServiceAccount-Token-Authentication","DevOps/K8-Authentication/Client-Certificate-Authentication","DevOps/K8-Authentication/OIDC-(OpenID-Connect)-Authentication","DevOps/K8-Authentication/Webhook-Token-Authentication"],"tags":[],"content":"Below are types of authentication in K8s:\n🔐 Types of Authentication in Kubernetes\n✅ 1. Service Account Token Auth\n\nUsed by pods inside the cluster to talk to the API server.\n\nThis is default behavior when pod wants to talk with API server.\nPod to Pod communication does not use this type of authentication.\nPod to Pod communication is not authenticated by default.\nThere are tools like Istio which can implement mTLS and other security configurations.\n\n\nJWT token auto-mounted into each pod so that Pod can authenticate itself to the Kubernetes API Server, not for it to authenticate to other application Pods.\n\nBelow document has more details.\nServiceAccount Token Authentication\n✅ 2. Client Certificate Auth\n\nUsed by admins or kubelet nodes\nAuthenticates based on X.509 client certificates.\n\nBelow document has more details.\nClient Certificate Authentication\n✅ 3. Static Token File Auth (legacy / rarely used)\n\nPredefined tokens in a file on the API server. Not recommended now.\n\n✅ 4. OIDC (OpenID Connect) / Token-based Auth\n\nBrowser-based or CLI-based SSO login (it is mostly browser based SSO login)\nIntegrates with Azure AD, Okta, Google, etc.\n\nBelow document has more details.\nOIDC (OpenID Connect) Authentication\n✅ 5. Webhook Token Authentication\n\nDelegates authentication to a custom external service\nAPI server calls a webhook to verify the token.\n\nBelow document has more details.\nWebhook Token Authentication\n\n🧩 Bonus Topics (Crucial to Real-World Setups)\n🔐 6. Authorization Layer (RBAC)\n\nDecides what actions are allowed after authentication.\n\n✅ 7. Admission Controllers\n\nFinal gatekeepers that can modify or reject requests after authentication and authorization.\n\n🔍 8. Audit Logging\n\nTracks who did what, when, and from where for security auditing.\n"},"DevOps/K8-Authentication/OIDC-(OpenID-Connect)-Authentication":{"slug":"DevOps/K8-Authentication/OIDC-(OpenID-Connect)-Authentication","filePath":"DevOps/K8 Authentication/OIDC (OpenID Connect) Authentication.md","title":"OIDC (OpenID Connect) Authentication","links":[],"tags":[],"content":"📘 Use Case:\nKubernetes can delegate authentication to an external identity provider like:\n\nGoogle, GitHub, Azure AD, Okta, Keycloak, etc.\nTypically used for SSO login from kubectl or browser\n\nThe user logs in via the OIDC provider and gets an ID Token (JWT)\nKubernetes API Server uses that JWT to authenticate the user.\n🧠 Key Concepts:\n\nOIDC provider issues a signed ID token (JWT)\nAPI Server is configured with --oidc-issuer-url, --oidc-client-id, etc.\nIdentity is extracted from token claims (sub, email, groups)\nOften paired with a login helper like kubelogin\n\n"},"DevOps/K8-Authentication/ServiceAccount-Token-Authentication":{"slug":"DevOps/K8-Authentication/ServiceAccount-Token-Authentication","filePath":"DevOps/K8 Authentication/ServiceAccount Token Authentication.md","title":"ServiceAccount Token Authentication","links":[],"tags":[],"content":"ServiceAccount Token Authentication (Pod-to-API Server):\nThis is the most common auth mechanism used within the cluster — every pod gets a ServiceAccount token, mounted automatically, and it uses that to call the Kubernetes API.\n📘 Example Use Case:\n\nA pod (like a controller or operator) needs to list pods or config maps\nIt sends an HTTP request to the kube-apiserver\nAuth happens via token that was mounted into the pod\n\n🧠 Key Concepts:\n\nKubernetes mounts the token under /var/run/secrets/kubernetes.io/serviceaccount/token\nThis is a JWT token, signed by the cluster\nkube-apiserver verifies it via its internal token authenticator\n\n"},"DevOps/K8-Authentication/Webhook-Token-Authentication":{"slug":"DevOps/K8-Authentication/Webhook-Token-Authentication","filePath":"DevOps/K8 Authentication/Webhook Token Authentication.md","title":"Webhook Token Authentication","links":[],"tags":[],"content":"This mechanism allows Kubernetes to delegate authentication to an external HTTP service, often used in enterprise SSO systems or for integrating custom token validation (e.g., LDAP, IAM systems, etc.).\n📘 Example Use Case:\n\nA user sends a request with a custom token\nKubernetes does not understand it, so it sends the token to a remote HTTP endpoint (the webhook)\nThe webhook service responds with authenticated: true/false and user identity\n\n🧠 Core Concepts:\n\nAPI server uses --authentication-webhook-config-file to configure it\nThe webhook must implement the TokenReview API\nDecouples token verification logic from the cluster\n\n"},"DevOps/K8-Networking/Kubernetes-Networking-(Planning-K8s-Networking-for-Pod-Deployment)---Part-3":{"slug":"DevOps/K8-Networking/Kubernetes-Networking-(Planning-K8s-Networking-for-Pod-Deployment)---Part-3","filePath":"DevOps/K8 Networking/Kubernetes Networking (Planning K8s Networking for Pod Deployment) - Part 3.md","title":"Kubernetes Networking (Planning K8s Networking for Pod Deployment) - Part 3","links":[],"tags":[],"content":"How Kubernetes Uses CIDR Blocks\nIn Kubernetes, you typically allocate CIDRs for:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPurposeUsed forExamplePod CIDRIP range from which pods get IPs10.244.0.0/16Service CIDRIPs assigned to ClusterIP services10.96.0.0/12Node CIDR (Optional)If needed for IPAM plugins192.168.0.0/16Subnet CIDR (Cloud)From VPC/virtual networkAWS VPC: 172.31.0.0/16\n\nThese CIDRs must not overlap with each other or with your host/VPC CIDRs.\n\n\n🔹 3. How Are CIDRs Allocated in Kubernetes?\nWhen using cloud CNI plugins like AWS VPC CNI, Azure CNI:\n\nIPs for Pods come from the underlying VPC subnet CIDR (because each Pod gets a real VPC IP).\nYou must ensure the subnet has enough IPs for nodes and pods.\n\nWhen using overlay CNIs like Calico, Flannel:\n\nIn this case IPs does not get allocated from VPC/Subnet CIDR. The overlay network plugins provide CIDR ranges that can be used. We can select any CIDR in this case based on our preference. Also, we must ensure these CIDR does not overlap with any other CIDR in VPC/Subnet.\nYou provide Pod CIDRs per node. Example:\n10.244.0.0/16 = cluster pod CIDR → each node gets a /24 block (256 IPs per node)\nPod traffic is encapsulated over the host network.\n\n\n🔹 4. How to Plan CIDRs for 50+ Applications\nHere’s a practical approach:\n✅ Step 1: Estimate Total Pods\nEstimate the number of pods in the cluster.\nExample:\n\n50 applications\nEach app might scale to 10 pods\nAdd 30% buffer\n\nTotal = 50 x 10 = 500 pods Buffer = 150 pods Total Required ≈ 650–700 pods\n\n✅ Step 2: Choose a Cluster Pod CIDR\nPick a Pod CIDR that supports 1000+ IPs:\n\n/22 → 1024 IPs (for small)\n/21 → 2048 IPs (for medium)\n/16 → 65,536 IPs (typical large cluster)\n\nExample: 10.244.0.0/16\n\nIn EKS/AKS/GKE — this is passed during cluster creation or configured via CNI.\n\n\n✅ Step 3: Plan Service CIDR\nChoose Service CIDR that does not overlap with:\n\nPod CIDR\nVPC subnets\nOn-prem IPs\n\nExample:\nService CIDR: 10.96.0.0/12\nUsually 4096 IPs (/20) is more than enough for thousands of services.\n\n✅ Step 4: Plan VPC/Subnets (for Cloud)\nIn AWS, Azure, GCP:\n\n\nYour VPC CIDR must be large enough to accommodate:\n\nNode IPs\nPod IPs (if using VPC-native CNI)\n\n\n\nUse multiple subnets (AZ-wise), e.g.:\nVPC CIDR: 172.20.0.0/16 Subnet AZ1: 172.20.1.0/24 Subnet AZ2: 172.20.2.0/24\n\n\n\nIf you’re using AWS VPC CNI, it assigns Pod IPs from subnet ranges, so make subnets large.\n\n\n🔹 5. Example (AWS EKS with VPC CNI)\nLet’s say you have:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComponentCIDRVPC172.31.0.0/16Subnet AZ1172.31.1.0/24Subnet AZ2172.31.2.0/24Pod IPsFrom above subnetsService CIDR10.96.0.0/12\n\n✅ Summary\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDesign ElementRecommendationPod CIDR/16 or /21 based on pod countService CIDR/20 or largerAvoid overlapPod/Service/VPC CIDRs should NOT overlapCloud Native CNIsPlan subnet size wiselyOverlay CNIsUse large Pod CIDR + /24 blocks per nodeDNSUse internal DNS, CoreDNS resolution\n🔷 Edge Case &amp; Pitfall Prevention\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMistakeOutcomeSolutionPod CIDR overlaps VPC CIDRRoute conflict, packets droppedPlan CIDRs carefullyToo small subnetENI IP exhaustionUse /21 or largerService CIDR overlaps on-premDNS issues, unreachable servicesReserve unique non-overlapping rangeFlat CIDR for multi-tenancyCross-app traffic, no isolationUse namespace isolation + policies\n🧩 Additional Design Concerns for a Large Number of Apps Deployment in K8 Cluster\n\nUse NetworkPolicies or Cilium for fine-grained L3/L7 security\nImplement Istio or Linkerd for egress/ingress traffic control\nUse IPAM management (AWS IPAM, Calico IPAM) to track CIDR usage\nPlan for DNS scale limits (CoreDNS tuning for thousands of services)\n"},"DevOps/K8-Networking/Kubernetes-Networking-(Plugin--and--Policy)---Part-2":{"slug":"DevOps/K8-Networking/Kubernetes-Networking-(Plugin--and--Policy)---Part-2","filePath":"DevOps/K8 Networking/Kubernetes Networking (Plugin & Policy) - Part 2.md","title":"Kubernetes Networking (Plugin & Policy) - Part 2","links":[],"tags":[],"content":"K8s networking there is 2 main component -\n\nNetwork plugin (or) Container Network Interface (CNI) plugin.\nNetwork policy\n\nCNI Plugin - Kubernetes Networking Plugin\n\nCNI plugins are the foundation of Kubernetes networking.\nCNI plugins are responsible for:\n\nAssigning IP addresses to each Pods\n\nEnsuring Pods can talk to each other across nodes\nEnabling Pods to reach Services.\n\n\nSetting up routes between Pods\nAllowing Pods to reach outside the cluster and external traffic to reach Services (via NodePort, LoadBalancer, Ingress).\nEnabling connectivity across nodes\n\nCommon CNI plugins:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPluginFeaturesCalicoPolicy enforcement, fast routing, supports eBPFProvide Overlay networkFlannelSimple overlay networking using VXLANCiliumPowered by eBPF, deep visibility, fine-grained policiesProvide Overlay networkWeaveWorks without needing IP layer config, easy to set upProvide Overlay networkCanalFlannel + Calico (overlay + policy)AWS VPC CNIDefault in EKS, takes IP from VPC subnet rangeDoes not provide Overlay networkAzure CNI, Azure CNI OverlayAzure CNI is default, takes IP from VNET subnet range.Whereas Azure CNI Overlay does not take IP from subnet CIDR.Google GKE CNI\nNetwork Policies\n\nIt is a Kubernetes API resource.\nBy default:\n➡️ All pods can talk to all other pods.\nYou can restrict communication using NetworkPolicy.\nThink of it as a Kubernetes firewall between Pods.\nNetwork Policies require a CNI plugin that supports their enforcement (e.g., Calico, Cilium, Weave Net are popular choices that do).\nPlugin Vs Policy\n\n\nPlugin (CNI) is foundational:  Without a CNI plugin, Pods literally cannot get an IP address or communicate; Kubernetes networking simply doesn’t function.\nPolicy is restrictive: It’s the “traffic rules and checkpoints” on that network.\nInterdependence: Policies depend entirely on the plugin to be enforced. A CNI plugin must be active and must have the capability to parse and apply the Network Policy rules.\nWhich is more crucial?\n\nThe CNI Plugin is more crucial for basic functionality and existence of the network. You can’t have a functional Kubernetes cluster without a CNI plugin.\nNetwork Policies are more crucial for security, multi-tenancy, and compliance. You can have a functional (but insecure) cluster without them.\nConclusion: The Plugin is fundamentally more crucial because it provides the underlying network. Policies build security on top of that existing network. You can’t secure what doesn’t exist.\n\n\n\nPolicy Types\n\nIngress: Controls incoming traffic to the selected Pods\nEgress: Controls outgoing traffic from the selected Pods\n\nBasic Example of Ingress policy (incoming traffic):\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-frontend-to-backend\n  namespace: myapp\nspec:\n  podSelector:\n    matchLabels:\n      app: backend\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          app: frontend\n    ports:\n    - protocol: TCP\n      port: 80\n  policyTypes:\n  - Ingress\n\n\nThe above ingress does below -\n\n\nDestination = Pods with app=backend\n\n\nSource = Pods with app=frontend\n\n\n🟢 This allows only frontend → backend (port 80) traffic.\n\n\nSelects Pods with app=backend (Destination)\n\n\nOnly allows traffic from Pods with app=frontend on port 80\n\n\n🧠 Think of podSelector as always defining the target of the policy — ingress to them, or egress from them.\nBasic example of Egress Policy:\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: restrict-egress\n  namespace: myapp\nspec:\n  podSelector:\n    matchLabels:\n      role: database\n  policyTypes:\n  - Egress\n  egress:\n  - to:\n    - ipBlock:\n        cidr: 10.0.0.0/24\n\n\n✅ This restricts role=database Pods to only talk to 10.0.0.0/24\n\nSource = Pods with role=database\nDestination = IPs in 10.0.0.0/24\n🔴 Any traffic going out from database pods to anything else is denied, except to that IP range.\n\n🧠 Think of podSelector as always defining the target of the policy — ingress to them, or egress from them.\n\n\n🧠 Key Notes\n\nNetwork Policies do nothing unless a CNI plugin that supports them (like Calico, Cilium) is installed.\nPolicies are additive — you need to define both ingress &amp; egress explicitly if you want both restricted.\nYou can use namespaceSelectors, podSelectors, IPBlocks, and ports to fine-tune rules.\n\nIn Kubernetes NetworkPolicies, there is no concept of priority or order like in firewalls or security groups. Instead, all applicable policies are combined (ANDed) to determine if traffic is allowed.\n🧩 How it works (Important Concept):\n\nIf any traffic is denied by one policy, it’s denied.\nTo allow traffic, you must explicitly allow it in at least one matching allow policy.\nKubernetes does not evaluate priorities — instead, it uses the union of all applicable “allow” rules.\nIf a pod is selected by multiple policies, all those policies must allow the connection for it to succeed.\n\nExample -\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPolicyWhat it doesdeny-allDenies all by default (baseline)allow-egress...Adds an exception for HTTPS\n📌 They don’t override each other. Instead, they combine, and traffic must be allowed by at least one policy and not denied by any default restrictions.\nOutbound Traffic from Kubernetes Cluster\n\n✅ Option 1: Direct Internet Access via NAT Gateway or Internet Gateway\nIf you’re not using Istio or any egress controller, then:\n\nOutbound pod traffic goes out via the node’s network interface (ENI)\nThat traffic then hits the NAT Gateway (for private subnets) or Internet Gateway (for public subnets)\nBasically in this case, traffic will follow normal VNET/VPC routing.\n\nExample:\nPod → Node ENI → NAT Gateway → Internet\n\n🔧 Use case:\n\nSimple setup (no service mesh)\nYou want all outbound traffic to be NAT’d or hidden behind node IP\n\n✅ Option 2: Through an Egress Gateway (Service Mesh Controlled)\nIf you’re using Istio (like your architecture):\n\nPods don’t send traffic directly to the internet\nThey send it to a dedicated Egress Gateway pod\nEgress Gateway handles:\n\nTLS termination/mutual TLS\nTraffic control (rate limiting, allow/block domains)\nAuditing or observability\n\n\nAfter the egress gateway, the traffic then goes to the internet via the NAT Gateway or Internet Gateway\n\nExample:\nIf we are using Istio as Service Mesh tool.\nPod → Istio sidecar proxy → Istio Egress Gateway → NAT Gateway → Internet\n\n🔁 Which one do you use?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetupUses NAT Gateway?Uses Istio Egress Gateway?BenefitsBasic VPC + Kubernetes✅ Yes❌ NoSimpler, but less secure/controlIstio with Egress Gateway✅ Yes✅ YesFine-grained control over egress"},"DevOps/K8-Networking/Kubernetes-Networking---Part-1":{"slug":"DevOps/K8-Networking/Kubernetes-Networking---Part-1","filePath":"DevOps/K8 Networking/Kubernetes Networking - Part 1.md","title":"Kubernetes Networking - Part 1","links":[],"tags":[],"content":"Kubernetes networking is the system that allows containers/pods on the same node or different nodes in a Kubernetes cluster to communicate with each other.\n“Kubernetes networking” is the system that allows all parts of a Kubernetes cluster (like applications in Pods) to communicate with each other, and enables traffic from outside to reach them. It provides IP addresses, load balancing, and rules for how everything connects.\nKubernetes networking components\nThe following are the main components of Kubernetes networking:\n\nContainer Network Interface (CNI): CNI is a plugin system that allows Kubernetes to integrate with different networking providers. Some popular CNI plugins include Calico, Flannel, and Weave Net.\nKubernetes Proxy: Kubernetes Proxy is a network proxy that runs on each node in the cluster. It is responsible for routing traffic between containers/pods on the same node and between containers/pods on different nodes.\nDNS Server: Kubernetes DNS Server is a DNS server that provides service discovery for containers/pods in the cluster. It resolves Kubernetes service names to the IP addresses of the containers/pods that are running the service.\nServices: Services are a way to expose pods to the outside world. They provide a load-balanced IP address and DNS name for a pod or group of pods.\nIngresses: Ingresses are a way to configure external load balancing for your applications. They allow you to route traffic from outside the cluster to your services.\n\nFull Guide:\ncscontents.com/kubernetes-networking-a-simple-guide/\nContainer Networking Basics (e.g., Docker, Kubernetes)\n\n\nDescription: How individual containers communicate with each other and with the outside world.\nRelevance:\n\nBridge Networks: Default way Docker containers on the same host communicate.\nOverlay Networks: How containers across different hosts (e.g., Kubernetes pods) communicate as if they were on the same network.\nService Meshes (e.g., Istio, Linkerd): Add advanced networking features (traffic management, security, observability) to microservices.\nCNI (Container Network Interface): The standard used by Kubernetes for network plugins.\n\n\n"},"DevOps/K8/K8-API-Server":{"slug":"DevOps/K8/K8-API-Server","filePath":"DevOps/K8/K8 API Server.md","title":"K8 API Server","links":[],"tags":[],"content":"🔍 What is the API Server in Kubernetes?\nThe Kubernetes API server (called kube-apiserver) is the central control plane component that:\n\nExposes the Kubernetes API\nActs as the frontend for the entire Kubernetes control plane\nIs the only component that talks to the etcd (where cluster state is stored)\n\n\n💡 Why is it called a “Server”?\nIt’s called a “server” because:\n\nIt serves HTTP(S) requests over REST API endpoints\nIt listens for incoming API calls (just like a web server listens for HTTP requests)\nIt validates, authenticates, and authorizes every request\nIt processes, stores, or forwards requests to other components\n\n📌 In short: it’s a server that manages and processes all cluster operations.\n\n✅ What It Does:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunctionExampleAccepts &amp; processes API requestskubectl get pods, kubectl apply -f deployment.yamlAuthenticates and authorizes usersChecks your identity and RBAC roleValidates object definitionsEnsures your YAML specs are validWrites to etcdStores the state of pods, deployments, nodes, etc.Triggers other componentsNotifies scheduler, controller-manager, etc.\n\n📦 Analogy:\nThink of kube-apiserver as the brain and the gatekeeper of the Kubernetes cluster.\nEvery tool, user, or component — including kubectl, CI/CD pipelines, the dashboard — must go through the API server to interact with the cluster."},"DevOps/K8/K8-Components":{"slug":"DevOps/K8/K8-Components","filePath":"DevOps/K8/K8 Components.md","title":"K8 Components","links":["DevOps/K8/K8-API-Server","DevOps/K8/K8-ConfigMap"],"tags":[],"content":"✅ Kubernetes Cluster Components\n🔹 1. Control Plane Components\nResponsible for managing the overall state and orchestration of the cluster.\n\n\nkube-apiserver\nExposes the Kubernetes API (acts as the front-end for the control plane).\n\n\netcd\nConsistent and highly-available key-value store for all cluster data.\n\n\nkube-scheduler\nAssigns Pods to Nodes based on constraints and resource availability.\n\n\nkube-controller-manager\nRuns controller loops (watch → compare → act):\n\nDeployment Controller\nReplicaSet Controller\nReplication Controller\nJob Controller\nDaemonSet Controller\nStatefulSet Controller\nNode Controller\nService Account &amp; Token Controllers\nPV controller\n\n\n\ncloud-controller-manager (optional)\nInteracts with cloud provider APIs for managing routes, load balancers, etc.\n\n\n\n🔹 2. Node Components (Data Plane)\nResponsible for running actual workloads (containers) and networking.\n\n\nkubelet\nAgent that runs on each node, communicates with API server, starts/stops containers.\n\n\nkube-proxy\nMaintains network rules on nodes. Enables communication inside/outside the cluster.\n\n\nContainer Runtime\nExecutes containers (e.g., containerd, Docker, CRI-O).\n\n\n\n🔹 3. Add-ons / Supporting Components\nOptional but commonly used for full production setups.\n\n\nCoreDNS\nCluster DNS for service discovery.\n\n\nIngress Controller\nManages external access to services (e.g., NGINX, Istio Ingress Gateway).\n\n\nMetrics Server\nCollects resource metrics (CPU, memory) for autoscaling.\n\n\nDashboard\nWeb UI to interact with the cluster.\n\n\nNetwork Plugins (CNI)\nHandles pod networking (e.g., Calico, Flannel, Azure CNI, Cilium).\n\n\n\n🧠 Bonus (Conceptual Roles)\n\n\nMaster Node\nRuns control plane components (can be separate from worker nodes).\n\n\nWorker Node (Minion)\nRuns application workloads (pods/containers), data plane components.\n\n\nKubernetes Core Components — Responsibility\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComponentTypeResponsibilityLinkkube-apiserverControl PlaneActs as the front door (HTTP REST interface) to the Kubernetes cluster. Validates, authenticates, and processes all API requests. All communication (CLI, kubectl, kubelet, controllers) goes through it.K8 API ServeretcdData StoreA distributed, strongly consistent key-value store. Stores all cluster state, including nodes, pods, secrets, configmaps, etc.kube-schedulerControl PlaneWatches for unscheduled pods and selects a suitable node for placement based on resource availability, affinity rules, taints/tolerations, etc.kube-controller-managerControl PlaneRuns multiple controllers like the Node Controller, ReplicaSet Controller, Job Controller, and ensures the desired state is maintained (e.g., scaling up/down).cloud-controller-managerControl PlaneIntegrates with cloud provider APIs (AWS, Azure, GCP). Manages load balancers, volume provisioning, node lifecycle in cloud environments.kubeletNode AgentAn agent that runs on every worker node. Communicates with API Server to get Pod specs and ensures containers are running as per spec.kube-proxyNode NetworkingMaintains network rules on nodes to enable service-level networking using iptables or IPVS. Handles traffic routing to correct pods.Container Runtime (e.g., containerd, CRI-O, Docker)Node RuntimePulls container images, starts/stops containers, manages container lifecycle. Works under kubelet.\n🔌 Optional but Important Components\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComponentTypeResponsibilityCoreDNSDNS Add-onProvides service discovery via internal DNS (e.g., my-service.default.svc.cluster.local).CNI PluginNetworking PluginHandles Pod-to-Pod networking. Examples: Calico, Flannel, Cilium, Azure CNI, Amazon VPC CNI.Ingress ControllerEdge RouterManages ingress traffic from outside the cluster. Examples: NGINX Ingress, Istio Gateway.Metrics ServerMonitoring Add-onCollects resource usage (CPU, memory) from nodes/pods. Used by kubectl top, HPA, etc.Dashboard (optional)UI Add-onProvides a web-based UI for cluster management and visualization.\n🔐 Authentication &amp; Authorization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComponentResponsibilitykube-apiserverHandles both authentication and authorization. Integrates with OIDC, service accounts, RBAC, etc.RBAC ControllerEvaluates roles and bindings to allow/deny access to resources.\n📦 Workload Components (Declarative Objects)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKubernetes Object (K8 API Object)RoleLinkPodSmallest deployable unit. Holds one or more containers.DeploymentManages stateless app replicas (scaling, rolling update).StatefulSetManages stateful apps (with stable identities, storage).DaemonSetEnsures a pod runs on every node (e.g., log collectors).Job / CronJobOne-time / recurring tasks.ServiceExposes a set of pods via a stable IP and DNS name.IngressExposes HTTP/S routes to services using rules.ConfigMap / SecretStores non-sensitive and sensitive data respectively. Injected into pods.K8 ConfigMapPersistentVolume / ClaimHandles storage lifecycle and consumption by pods.\n🎯 Visualizing a Typical Flow (Summarized):\n\nUser runs kubectl apply -f pod.yaml\nkubectl talks to kube-apiserver\nkube-apiserver validates request, stores pod spec in etcd\nkube-scheduler sees pending pod, chooses node\nkubelet on that node pulls image and starts container via container runtime\nkube-proxy sets up routing to reach that pod via service\nCoreDNS allows service name resolution\nOptional: Ingress controller exposes app to outside\n\n"},"DevOps/K8/K8-ConfigMap":{"slug":"DevOps/K8/K8-ConfigMap","filePath":"DevOps/K8/K8 ConfigMap.md","title":"K8 ConfigMap","links":[],"tags":[],"content":"ConfigMap\n\nPurpose : Used to store non-sensitive configuration data.\nData Type : Stores key-value pairs of strings. It can also store binary data.\nUsage : Ideal for storing configuration files, environment variables, and other non-sensitive data that applications need to run.\nVisibility : Data stored in ConfigMaps is not encrypted by default and is visible in plain text.\nExample Use Cases :\n\nStoring database connection strings (if not sensitive).\nHolding application configuration parameters.\nProviding configuration files to containers.\n\n\n\n🔧 What Is a ConfigMap?\nA ConfigMap is a native Kubernetes API object used to decouple configuration data from application code. It allows you to inject key-value pairs (like environment variables, config files, command-line arguments) into Pods.\nIn Kubernetes, everything is managed as an API object, defined via the Kubernetes API server.\nwhen you create a ConfigMap via kubectl or YAML, it interacts directly with the Kubernetes control plane (API server) — not through any CRDs or operators.\n🔷 What Does It Mean:\n“In Kubernetes, everything is managed as an API object”?\nAn API object is just a type of resource you can manage in Kubernetes.\nExamples of API objects:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAPI ObjectWhat It RepresentsPodA running container or appServiceA stable network address to access a PodDeploymentA way to manage multiple Pods and ensure they stay runningConfigMapExternal config for your appSecretEncrypted sensitive data like passwordsNamespaceA way to group and isolate resourcesNodeA worker machine (VM or physical) in your cluster\n🧭 So when we say:\n\n“Everything in Kubernetes is managed as an API object…”\n\nWe mean:\n\n\nYou define things (like a Pod or ConfigMap) using YAML or JSON.\n\n\nThese are sent to the Kubernetes API Server, which:\n\nValidates them\nStores them (in etcd, a key-value database)\nInstructs other components (like kubelet) to take action\n\n\n\n🗃️ Important points:\n\nKubernetes resources (Pods, Services, ConfigMaps…) are called API objects.\nThey are all sent to and managed by the Kubernetes API Server.\nThe API Server is like the central brain or gatekeeper of the cluster or gateway that handles communication, validation, and CRUD operations.\nThese API objects are stored in etcd, and other components (like kubelet) follow the instructions to make sure the cluster behaves as desired.\n\n🔁 Flow Summary (High Level):\n\nUser applies a ConfigMap → kubectl apply -f configmap.yaml\nAPI Server receives it → validates it\nAPI Server writes the ConfigMap to etcd. In other words etcd stores the actual ConfigMap.\nWhen a pod needs it, Kubelet asks API Server\nAPI Server reads from etcd, returns data to Kubelet\nKubelet mounts or injects config into the pod\n\n✅ Why Use ConfigMaps? (Senior-Level Perspective)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBenefitReasonDecouples config from imagePromotes immutable container images and 12-factor app designEnvironment-specific overridesReuse same Pod spec across dev/test/prod by changing config onlyDynamic reload (if supported by app)Enables configuration updates without restarting appCleaner GitOps workflowsStore config outside image; can be versioned &amp; diffed separatelySecurity + AuditabilityBetter visibility/control over runtime config (separate from app code)\n🧩 Key Features of ConfigMap\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeatureDescriptionKey-Value StorageCan store strings, JSON, config filesMounted as VolumeFiles inside the container, used by apps like NGINX, Spring BootInjected as Env VarsSimple configs for CLI-based appsUsed in command argsRefer ConfigMap in container startup commands\n📦 How to Create a ConfigMap\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: my-config\ndata:\n  APP_PORT: &quot;8080&quot;\n  LOG_LEVEL: &quot;debug&quot;\n  myconfig.json: |\n    {\n      &quot;featureFlag&quot;: true\n    }\n\n🚀 Ways to Use a ConfigMap\n\nAs environment variables\n\n    envFrom:\n      - configMapRef:\n          name: my-config\n\n\nAs volume mount (files)\n\nvolumes:\n  - name: config-volume\n    configMap:\n      name: my-config\n\nvolumeMounts:\n  - name: config-volume\n    mountPath: /etc/config\n\n\nAs CLI arguments or command\n\ncommand: [&quot;./myapp&quot;]\nargs: [&quot;--log-level=$(LOG_LEVEL)&quot;]\n\n💡 What Happens Under the Hood?\nWhen you create a ConfigMap like this:\nkubectl apply -f my-configmap.yaml\n\nKubernetes:\n\n\nSends it to the API server\n\n\nStores it in etcd (cluster state)\n\n\nWhen a Pod that references the ConfigMap is scheduled, the Kubelet on the node fetches it and injects into the container either as:\n\nenv var\nmounted file\ncommand-line arg\n\n\n\n🔄 Reloading ConfigMap Dynamically\n\n\nKubernetes does not auto-reload mounted ConfigMaps.\n\n\nUse tools like:\n\nReloader\nConfigMap Controller\nOr app logic with inotify (fs watcher)\n\n\n\n🔐 Security &amp; Best Practices\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipReasonDon’t store secrets in ConfigMapsUse Secrets for sensitive data (ConfigMap is base64-encoded only, not encrypted)Use RBAC to restrict accessAvoid unintended config tamperingUse resource naming &amp; labels consistentlyFor GitOps and automation pipelinesAudit ConfigMap changes in CI/CDEnsure compliance, traceability\n🧠 Senior-Level Scenarios\n\n\nMulti-Environment Deployments\nUse Helm templates or Kustomize overlays to manage ConfigMaps per environment.\n\n\nRollout on ConfigMap Change\nPair ConfigMap with checksum annotations to trigger Pod restart when config changes.\n\n\nIntegrate with External Config Management\nUse External Secrets, Vault, or config sync from SSM/Parameter Store (via controllers).\n\n\n"},"DevOps/K8/K8-Persistent-Volume":{"slug":"DevOps/K8/K8-Persistent-Volume","filePath":"DevOps/K8/K8 Persistent Volume.md","title":"K8 Persistent Volume","links":[],"tags":[],"content":"🧱 1. Pod Lifecycle and Ephemeral Storage\n\nPods are ephemeral – if they die, all data inside the container is gone.\nemptyDir volumes allow basic data persistence only while the pod is running.\n\n📝 But what if we want to persist data even after pod restart?\nThat’s where Persistent Storage comes in.\nVolume Types in Kubernetes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVolume TypeUse CaseemptyDirTemp storage (pod-lifetime)hostPathAccess host node’s filesystemnfsShared network volumepersistentVolumeClaimUse PVsconfigMap, secretInject config/secrets as filescsiModern pluggable volume mechanism\nReal-World Storage Backends\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCloudVolume TypeCSI ProvisionerAWSEBSebs.csi.aws.comAzureDisk/Filedisk.csi.azure.comGCPPersistent Diskpd.csi.storage.gke.io\n📌 Persistent Volume (PV)?\nA Persistent Volume in Kubernetes is a piece of storage provisioned by an administrator or dynamically by a StorageClass. It is a cluster-level resource that lives beyond the lifetime of individual pods and it represents an actual physical or cloud disk (e.g., EBS, Azure Disk).\nIt decouples storage from pods, enabling data persistence across pod restarts, reschedules, or failures.\n📝 Persistent Volume Claim (PVC)\n\n\nA PVC is a request for storage by a pod.\n\n\nPVC specifies:\n\nSize (e.g., 10Gi)\nAccess mode (ReadWriteOnce, ReadOnlyMany, etc.)\nOptionally a storageClassName\n\n\n\nThe life cycle of PV and PVC is divided into 5 stages.\n\nProvisioning—a PV is created in advance by the administrator in static mode, or via a StorageClass provided by the administrator in dynamic mode.\nBinding—the PV is bound and assigned to PVC.\nUsing—the container consumes a PV, via the PVC.\nReleasing—the container releases the PV, removing the PVC.\nReclaiming—Kubernetes reclaims the storage resources previously used by the PV.\n\nAfter the user has finished using the volume, two strategies can be used to reclaim the storage resources used by the PV (a third strategy, “reclaim”, is now deprecated):\n\nRetain—the retention reuse strategy allows users to request a PV again in the future.\nDelete—the delete strategy causes the PV to be deleted from the Kubernetes cluster, and the space taken up on external storage devices is also deleted.\n\n🔷 2.1 Core Concepts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComponentPurposePV (PersistentVolume)The actual storage resource in the cluster (EBS, NFS, local disk, etc.)PVC (PersistentVolumeClaim)A request for storage by a user (pod/deployment)StorageClassDefines how storage is provisioned dynamically (like “gp2” on AWS)\n🔸 Example Workflow (Manual Binding or Static Building)\n\nAdmin creates a PV\nDeveloper writes a PVC\nKubernetes binds PVC to a suitable PV\nPod uses the PVC\n\n🔸 Dynamic Provisioning (Recommended)\n\nDeveloper writes only a PVC\nStorageClass creates PV automatically\nPod consumes the PVC\nWhen deleted (based on reclaimPolicy), volume is deleted or retained\n\n🧱 StorageClass (Most Crucial Concept)\n\nDefines how to dynamically provision storage.\nPoints to a provisioner (like AWS EBS, Azure Disk, GCE PersistentDisk).\nOptional parameters, reclaimPolicy, volumeBindingMode, etc.\n\nA Kubernetes StorageClass is a Kubernetes storage mechanism that lets you dynamically provision persistent volumes (PV) in a Kubernetes cluster. Kubernetes administrators define classes of storage, and then pods can dynamically request the specific type of storage they need.\nStorageClasses\nKubernetes administrators can define StorageClasses and assign PVs to them. Each StorageClass represents a type of storage—for example, fast SSD storage vs regular magnetic drives or remote cloud storage. This allows a Kubernetes cluster to provision different types of storage depending on the changing requirements of its workload.\nA StorageClass is a Kubernetes application programming interface (API) for setting storage parameters. It is a dynamic configuration method that creates new volumes on demand. The StorageClass specifies the name of the volume plugin used, an external provider if any, and a Container Storage Interface (CSI) driver, which allows containers to interact with storage devices.\nDynamic Provisioning of StorageClasses\nKubernetes supports dynamic volume provisioning, which allows for creation of storage volumes on demand. This eliminates the need for administrators to manually create new storage volumes in their cloud or storage provider, and then create PersistentVolume objects to make them available in the cluster. This whole process happens automatically when a specific storage type is requested by users.\nThe cluster administrator defines StorageClass objects as needed. Each StorageClass references a volume plugin, also known as a provisioner. The volume plugin specifies a set of parameters and passes them to a provisioner when it automatically provisions a storage volume.\nEach StorageClass defined by the administrator can represent a different type of storage or the same storage with different parameters (for example, S3 using the normal storage tier vs an archive tier). This allows users to select from several storage options, without worrying about the underlying implementation of each one.\nEvery StorageClass has the following fields:\n\nProvisioner—this is the plugin used to provision the storage volume\nParameters—indicate properties of the underlying storage system\nreclaimPolicy—indicates under what conditions the storage is released by a pod and can be reclaimed for use by other pods\n\nThese are explained in more detail below.\nProvisioner\nA StorageClass object contains a provisioner that decides which volume plugin is used to provision PersistentVolumes. Admins must specify this field.\nKubernetes provides internal provisioners, which you can see listed here. Their names have a kubernetes.io prefix and they are shipped by default as part of Kubernetes.\nUsers can specify and run external provisioners - these are independent programs that follow a Kubernetes-defined specification. The author of an external provisioner has full discretion over the code’s storage location, the provisioner’s shipping and running, and the selection of volume plugins (including Flex). To see an example, read our blog about building a CSI driver.\nAn example of an external provisioner is the Network File System (NFS), which is not available as an internal provisioner, but offers an external provisioner. In some cases, third-party storage vendors provide their own external provisioners.\nRelated content: Read our guide to Kubernetes NFS\nParameters\nParameters describe volumes belonging to the StorageClass. The parameters depend on the provisioner plugin and the properties of the underlying storage system.\nA StorageClass can have at most 512 parameters. The parameters object can have a total length of up to 256 KiB including keys and values.\nReclaim Policy\nStorageClass can dynamically create PersistentVolumes that specify either Delete or Retain in the class’s reclaimPolicy field. If there is no specified reclaimPolicy in a StorageClass object (admins must specify it when creating the object), it is set to Delete by default:\n\nSetting the reclaim policy to Delete means that the storage volume is deleted when it is no longer required by the pod.\nSetting the reclaim policy to Retain means that the storage volume is retained when no longer required by the pod and can be reused by other pods.\n\nA PersistentVolume that was created manually and managed using a StorageClass retains the reclaim policy assigned to it at creation.\n🔁 Binding PVC to PV\n\nKubernetes automatically matches a PVC to a suitable PV (based on size, access mode, StorageClass).\nOnce bound, the PVC can’t switch PV.\n🧠 Binding is 1:1 mapping.\n\n🔷 2.2 Key YAML Examples\n📄 PersistentVolume (PV)\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: my-pv\nspec:\n  capacity:\n    storage: 5Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /mnt/data\n\n📄 PersistentVolumeClaim (PVC)\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\n\n🔷 2.3 Access Modes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAccessModeDescriptionReadWriteOnceOne node can read/writeReadOnlyManyMany nodes can readReadWriteManyMany nodes can read/write\n🔷 2.4 Reclaim Policy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPolicyBehavior on PVC DeletionRetainKeep the PV (manual cleanup)DeleteAutomatically delete PVRecycleDeprecated\n🧠 DevOps/SRE Tips\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsightBenefitAlways prefer dynamic provisioningCleaner, no manual PV creationUse StorageClasses for different backendsLike EBS, Azure Disk, GCP PD, etc.Don’t use hostPath in productionIt ties to a node (no portability)Monitor disk IOPS &amp; latency with metricsPrevent app slowness\n"},"DevOps/K8/K8-Secret":{"slug":"DevOps/K8/K8-Secret","filePath":"DevOps/K8/K8 Secret.md","title":"K8 Secret","links":[],"tags":[],"content":"Secret\n\nPurpose : Used to store sensitive information such as passwords, tokens, and keys.\nData Type : Stores key-value pairs, but the data is base64 encoded. It can store strings, passwords, or tokens.\nUsage : Designed for storing sensitive data that should be kept secure.\nVisibility : Data is base64 encoded, but it is not encrypted by default. Kubernetes recommends using additional security measures like encryption at rest and in transit.\nExample Use Cases :\n\nStoring database passwords.\nHolding API keys and tokens.\nKeeping SSL/TLS certificates and keys.\n\n\n\n🔐 What Is a Kubernetes Secret?\nA Secret is a Kubernetes-native API object used to store sensitive data, such as:\n\nPasswords\nTokens\nSSH keys\nTLS certificates\nDocker registry credentials\n\nThis avoids hardcoding sensitive info in Pod specs or container images.\n🧩 Why Use Secrets?\nWithout secrets, you might:\n\nEmbed credentials in container images → insecure.\nAdd them to ConfigMaps → not encrypted.\nStore them in plain-text YAML files → risky.\n\nSecrets provide a way to manage sensitive data securely and independently from application code.\n🔑 Types of Kubernetes Secrets\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTypeDescriptionOpaqueDefault type. Stores arbitrary key-value pairs (e.g., username, password).kubernetes.io/service-account-tokenAutomatically generated for each service account to authenticate with the API server.kubernetes.io/dockerconfigjsonFor pulling images from private Docker registries.kubernetes.io/tlsStores TLS certificate and private key.\n⚙️ How Secrets Are Created\nTwo way -\n\nBy running command in CLI\n\nkubectl create secret generic my-secret \\\n  --from-literal=username=admin \\\n  --from-literal=password=secret\n\n\nUsing YAML file\n\napiVersion: v1\nkind: Secret\nmetadata:\n  name: my-secret\ntype: Opaque\ndata:\n  username: YWRtaW4=   # base64 encoded\n  password: c2VjcmV0\n\n🔐 How to Use Secrets in Pods\nTwo way -\n\nAs environment variable\n\n\tenv:\n  - name: DB_USER\n    valueFrom:\n      secretKeyRef:\n        name: my-secret\n        key: username\n\n\nAs volumes (mounted as files)\n\nvolumes:\n- name: secret-volume\n  secret:\n    secretName: my-secret\n\nvolumeMounts:\n- name: secret-volume\n  mountPath: &quot;/etc/secret&quot;\n\n🔒 How Secrets Are Secured?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSecurity LayerDescriptionBase64 encodingOnly encodes, not encrypts. Used for transport/storage in YAML.etcd encryptionSecrets stored in etcd must be encrypted at rest (must be configured explicitly).RBACRestricts who can read secrets (kubectl get secrets).Pod accessOnly pods that explicitly reference the secret can use it.\n🧠 Best Practices\n✅ Enable etcd encryption (EncryptionConfiguration file).\n✅ Use RBAC to tightly control access.\n✅ Avoid exposing secrets to containers unnecessarily.\n✅ Rotate secrets regularly.\n✅ Audit secret access using tools like OPA, Kyverno, or Gatekeeper.\n\nWhere &amp; how tokens are stored? or tokens are injected into pods?\nAns: by init containers\n\n\n🔐 1. Secrets in Managed Clusters (EKS/AKS)\nWhere are Secrets stored?\n\nIn Kubernetes itself, just like in self-managed clusters.\nBacked by etcd, the same way — but managed by AWS or Azure control plane.\nYou create secrets using kubectl or GitOps/Helm etc.\n\n\n✅ 2. Who Can Pull a Secret?\nKubernetes controls access to secrets using authentication and authorization:\n🔑 Step 1: Authentication\nBefore a pod or user (or tool like kubectl) can interact with a Secret, it must authenticate:\n\n🧑‍💻 Human (DevOps): Authenticates via kubectl using kubeconfig → which may use IAM (AWS) or AAD (Azure AD) under the hood.\n🤖 Pod (Application): Authenticates using a ServiceAccount token mounted into the Pod by Kubernetes.\n\n\n🔓 Step 2: Authorization\nOnce authenticated, Kubernetes uses RBAC policies to decide:\n\nCan this user/service account read the Secret?\nFrom which namespace?\nWhat verbs are allowed? (get, list, watch…)\n\nExample RBAC rule:\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: secret-reader\n  namespace: app\nrules:\n- apiGroups: [&quot;&quot;]\n  resources: [&quot;secrets&quot;]\n  verbs: [&quot;get&quot;]\n\n\n📦 3. When Are Secrets Injected into Pods?\nThere are two common ways:\n🔹 A. As Environment Variables\nenv:\n  - name: DB_USER\n    valueFrom:\n      secretKeyRef:\n        name: my-secret\n        key: username\n\n🔹 B. As Mounted Volumes (Files)\nvolumes:\n- name: secret-volume\n  secret:\n    secretName: my-secret\n\nvolumeMounts:\n- name: secret-volume\n  mountPath: /etc/secrets\n\n✅ Kubernetes handles this during Pod creation\nWhen a Pod is scheduled and started:\n\nThe Kubelet on the node securely pulls the Secret (if authorized)\nInjects it into the container (as env or file)\nNo internet call is made from the Pod to AWS or Azure → it’s internal.\n\n\n🔐 Where Are the Tokens Stored?\nA. Human Users (e.g., kubectl)\n\nTokens can be short-lived session tokens (e.g., from AWS IAM, AAD).\nStored in your ~/.kube/config file or temporary credentials via cloud CLI.\n\nB. Pods\n\n\nA ServiceAccount token is mounted at /var/run/secrets/kubernetes.io/serviceaccount/token.\n\n\nKubernetes uses this token to authenticate the Pod when it accesses the API server (e.g., to talk to a Secret, ConfigMap, or call another API).\n\n\n\n🔁 Putting It All Together\nExample: App Pod needs my-secret to access a database\n\nDevOps creates my-secret in the cluster via YAML or kubectl.\nThe app Deployment refers to this secret (in env or volumeMount).\nWhen the Pod is scheduled:\n\nThe node’s Kubelet requests the Secret from API server.\nServiceAccount token of the Pod is used to authenticate.\nRBAC is checked to allow access to the Secret.\n\n\nThe secret is injected into the Pod → application can now read it.\n\n\n☁️ Special Note: Using External Secret Stores (AWS/GCP/Azure Vault)\nIn modern setups, many teams use:\n\nAWS Secrets Manager\nAzure Key Vault\nHashiCorp Vault\n\nIn such cases, a controller like “External Secrets Operator” is used to:\n\nAuthenticate to the cloud provider (IAM role or workload identity)\nSync external secrets into Kubernetes Secrets\n\n\n✅ Summary\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStepWhat Happens1Secret created via YAML or CLI2Stored in etcd, encrypted (in managed control plane)3Pod scheduled with reference to Secret4Kubelet pulls Secret if ServiceAccount has access5Injected into container as env or file6App reads it securely inside Pod"},"DevOps/K8/Kubeconfig-file":{"slug":"DevOps/K8/Kubeconfig-file","filePath":"DevOps/K8/Kubeconfig file.md","title":"Kubeconfig file","links":[],"tags":[],"content":"kubeconfig is a YAML file used by Kubernetes CLI tools like kubectl to authenticate and communicate with the Kubernetes cluster (to be specific it allows us to communicate with Kubernetes API server).\nIt’s typically located at:\n~/.kube/config\n\n🧩 What Does It Contain?\nThe kubeconfig file contains three key types of data:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSectionDescriptionclustersInfo about Kubernetes clusters: name, API server URL, certificateusersHow to authenticate: token, certificate, username/password, etc.contextsA combination of a cluster and user. You “switch context” to select which cluster/user pair you’re using.These are essential for authenticating and interacting with the cluster.\n🔄 Switch Contexts\nkubectl config use-context dev-context\n\nkubectl config get-contexts\n\nIf someone gets access to your kubeconfig file, they can access your Kubernetes cluster with the same permissions as the user configured in that file.\nThis is critical from a security standpoint.\n🔐 Why Is kubeconfig So Sensitive?\nThe file often contains:\n\nAuthentication tokens, client certificates, or OIDC config\nAPI server URL\nCluster CA certificates\n\nSo, if someone copies this file and runs:\nkubectl get pods\n\n.it will work as if they are you — no password or MFA prompt.\n🛡️ How to Secure It\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipDescription🔒 File permissionsDefault location (~/.kube/config) should be readable only by the user (chmod 600)🔑 Use short-lived tokensAvoid long-lived static tokens or service accounts🧹 Rotate credentialsRevoke and regenerate credentials if exposed👤 Use RBACLimit the user’s permissions via Role/ClusterRole☁️ Use cloud-native IAME.g., AWS IAM Authenticator, Azure AD, GKE IAM binding🔐 Vault integrationUse tools like HashiCorp Vault for dynamic credential management\nExample Kubeconfig file:\napiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: LS0tLSfuihdhuhdsuchsuushfuvhiuwyuwet6wuwhoqwioalcnvjbhdbfvugdfqf6rsq2rwqeqouwprfoqkfkmvlknvheifhiuwhihdjwdkq[pgltrpgirtigoeutuwhruhqwdhbqhgduyqwgdiuqhdqfciwhuiqfhuqhafqjhfiuhwqiufhiuwhfiqwhdiqwi3ye872y8eydqwsijsjndkjchdquhdiqdoiqdnVJYjVEeUk2NjFSUStvYWdlK2FsL3ZMClJ3WERKak9PeUU4TlZaNnhMRjlsK3JieWV4S3dhdmJveDZxL3NIUGVKbTNGQkxleGFoNXdJZktrSDlKZllTM1QKR0prPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n    server: tst.eu-west-2.eks.amazonaws.com\n  name: arn:aws:eks:eu-west-2:12345678:cluster/EKSC01-IMR-APPSREM-DEVAR-IMR-CPS-LM\ncontexts:\n- context:\n    cluster: arn:aws:eks:eu-west-2:12345678:cluster/EKSC01-IMR-APPSREM-DEVAR-IMR-CPS-LM\n    user: arn:aws:eks:eu-west-2:12345678:cluster/EKSC01-IMR-APPSREM-DEVAR-IMR-CPS-LM\n  name: arn:aws:eks:eu-west-2:12345678:cluster/EKSC01-IMR-APPSREM-DEVAR-IMR-CPS-LM\ncurrent-context: arn:aws:eks:eu-west-2:12345678:cluster/EKSC01-IMR-APPSREM-DEVAR-IMR-CPS-LM\nkind: Config\npreferences: {}\nusers:\n- name: arn:aws:eks:eu-west-2:12345678:cluster/EKSC01-IMR-APPSREM-DEVAR-IMR-CPS-LM\n  user:\n    exec:\n      apiVersion: client.authentication.k8s.io/v1beta1\n      args:\n      - --region\n      - eu-west-2\n      - eks\n      - get-token\n      - --cluster-name\n      - EKSC01-IMR-APPSREM-DEVAR-IMR-CPS-LM\n      - --output\n      - json\n      command: aws\n"},"DevOps/K8/PV-vs-PVC-vs-Storage-Class":{"slug":"DevOps/K8/PV-vs-PVC-vs-Storage-Class","filePath":"DevOps/K8/PV vs PVC vs Storage Class.md","title":"PV vs PVC vs Storage Class","links":[],"tags":[],"content":"Core concept:\nIn K8,\nVolume which Persist or Persistent Volume can be create in two ways -\n\n\nStatic way - here K8 admin provision the PV manually and keep them available in the cluster. Pods can claim these PVs through PVCs. This approach is useful when you have specific storage resources that you want to manage directly.\n\n\nDynamic way - here pod raise a request by PVC to create PV and PVC must have “storage class” specified in it and based on this storage class PV is provisioned dynamically. The storageClass defines the type of storage and the provisioner that will create the PV. When a PVC is created with a storageClass, Kubernetes dynamically provisions a PV that meets the PVC’s requirements.\n\n\nBasic Process:\nwhen developer write deployment manifest file, there they need to specify the PVC name (if any PV is required by the pod) and also define a PVC manifest file.\nSo, the steps are -\nIf a pod requires PV -\n\n\nDefine a PVC Manifest :\n\n\nBefore deploying an application, you need to create a PVC manifest file.\n\n\nThis file specifies the storage requirements, such as size, access modes, and optionally a storageClass for dynamic provisioning.\n\n\n\n\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n  storageClass: standard  # Optional, used for dynamic provisioning\n\n\n\nReference the PVC in the Deployment Manifest :\n\n\nIn the deployment manifest, you reference the PVC in the pod’s volume configuration.\n\n\nThe pod uses this PVC to mount the persistent storage.\n\n\n\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: my-app\n  template:\n    metadata:\n      labels:\n        app: my-app\n    spec:\n      containers:\n        - name: my-container\n          image: my-image\n          volumeMounts:\n            - mountPath: /data\n              name: my-volume\n      volumes:\n        - name: my-volume\n          persistentVolumeClaim:\n            claimName: my-pvc\n\nDetailed Explanation:\n\n\nPersistent Volume (PV) :\n\nA PV is a piece of storage in the cluster that is provisioned by an administrator or dynamically through a Storage Class.\nIt is a cluster-scoped resource, meaning it exists independently of any pods or nodes.\nPVs can be created manually or dynamically. When created manually, they are pre-provisioned and available for use. When created dynamically, they are provisioned based on a PVC’s request.\n\n\n\nPersistent Volume Claim (PVC) :\n\nA PVC is a request for storage by a user (typically a pod).\nIt specifies the size, access modes (e.g., ReadWriteOnce, ReadManyMany), and other attributes required for storage.\nPVCs are namespace-scoped resources, meaning they are specific to a namespace.\nWhen a PVC is created, Kubernetes attempts to find a suitable PV that matches the PVC’s requirements. If a match is found, the PVC is bound to the PV.\n\n\n\nStorage Class :\n\nA Storage Class provides a way to define different classes of storage.\nIt allows for dynamic provisioning of PVs when a PVC requests storage and specifying a Storage Class.\nStorage Classes are cluster-scoped resources.\nThey define the type of storage (e.g., SSD, HDD) and the provisioner that will create the PV.\n\n\n\nDependencies:\n\nStorage Class → Persistent Volume (PV) : A Storage Class can dynamically provision PVs when a PVC requests storage.\nPersistent Volume Claim (PVC) → Persistent Volume (PV) : A PVC is bound to a PV that satisfies its criteria.\nStorage Class → Persistent Volume Claim (PVC) : A PVC can specify a Storage Class to dynamically provision a PV.\n\n\nGood reading aticle:\nwww.netapp.com/blog/cvo-blg-kubernetes-storageclass-concepts-and-common-operations/"},"DevOps/K8/Secret-from-AWS-SM":{"slug":"DevOps/K8/Secret-from-AWS-SM","filePath":"DevOps/K8/Secret from AWS SM.md","title":"Secret from AWS SM","links":[],"tags":[],"content":"miro.com/app/board/uXjVKd5XC10=/  - Flow diagram in Miro\nAWS SM → External Secrets in EKS → Secret"},"DevOps/K8/Types-of-Kubernetes-Deployment":{"slug":"DevOps/K8/Types-of-Kubernetes-Deployment","filePath":"DevOps/K8/Types of Kubernetes Deployment.md","title":"Types of Kubernetes Deployment","links":[],"tags":[],"content":"Rolling Update:\n\nkubectl rollout restart deploy (Rolling Update):\nHow it works:\nWhen you run this command (or when you update an image tag in a Deployment and apply it), Kubernetes starts creating new Pods (with the updated version) one by one, while simultaneously terminating old Pods one by one.\nTraffic Flow:\nTraffic gradually shifts from old Pods to new Pods as they become ready. There’s an overlap period where both old and new versions of Pods are running and serving traffic.\nGoal:\nTo achieve zero-downtime updates by ensuring that a minimum number of Pods are always available to serve traffic.\nRollback:\nIf issues arise, Kubernetes can roll back to a previous revision, but this also happens as a rolling update, not an instantaneous switch.\nBlue-Green deployment:\n\nBlue - old deployment or pods\nGreen - New deployment or pods.\nWhen we want to deploy a new image tag by updating the deployment -\nHere rule is, the old pods will continue to server the traffics until the new pod is fully up &amp; running.\nBlue-Green Deployment:\n\nHow it works: As discussed, you deploy the new version to a completely separate set of Pods (the “Green” environment). The old version (“Blue”) continues to serve all traffic. Only after the Green environment is fully ready and tested is all traffic instantly switched from Blue to Green.\nTraffic Flow: An “all or nothing” switch from one fully stable environment to another fully stable environment. There’s no gradual shifting of traffic or mixing of old/new versions at the production traffic level.\nGoal: Zero-downtime deployment with a fast, reliable rollback mechanism (just switch back to Blue).\n\nCanary Deployment\n\nIn Kubernetes, Canary deployment is a strategy where you deploy a new version of your application (the “canary”) to a small subset of Pods alongside your current production version.\nTraffic is then gradually diverted to these canary Pods (e.g., 5%, then 25%, then 50%) using Service Mesh features (like Istio’s traffic splitting) or advanced Ingress controller rules. You monitor the canary’s performance and stability closely. If healthy, you incrementally shift more traffic; if issues arise, you quickly revert the traffic split to the old version, limiting impact to a small user group.\nRestarting Deployment\n\n\nkubectl scale deploy test-deploy -n my-ns —replicas=0\nkubectl scale deploy test-deploy -n my-ns —replicas=1\n\nIt’s often used as a quick way to force a full “restart” of an application if the Deployment’s rolling update strategy isn’t behaving as expected (in non-production environments where downtime is acceptable. It is not a method for zero-downtime deployments like Blue-Green or Rolling Updates."},"DevOps/Questions":{"slug":"DevOps/Questions","filePath":"DevOps/Questions.md","title":"Questions","links":[],"tags":[],"content":"What are some application of AI in DevOps?\n\nAI in DevOps, often termed AIOps, applies machine learning and artificial intelligence to enhance and automate IT operations and the entire software delivery lifecycle.\nHere are some key applications:\n\n\nIntelligent Monitoring &amp; Alerting:\n\nAI analyzes vast amounts of operational data (logs, metrics, traces) to detect anomalies, predict outages, and reduce alert fatigue by correlating related events and prioritizing critical issues. It can identify patterns that human operators might miss.\n\n\n\nAutomated Testing &amp; Quality Assurance:\n\nAI can generate test cases, optimize test suites, predict defect probabilities, and automate visual regression testing. This speeds up the testing phase, improves test coverage, and catches bugs earlier in the CI/CD pipeline.\n\n\n\nPredictive Analytics for Operations:\n\nAI models can forecast future resource needs (CPU, memory, network), predict system failures before they occur, and anticipate performance bottlenecks. This enables proactive scaling and maintenance, preventing downtime.\n\n\n\nSmart Automation of CI/CD Pipelines:\n\nAI can optimize build times, suggest optimal merge strategies, and automate release orchestration by learning from past pipeline executions. It can also route code changes to appropriate reviewers or testing environments.\n\n\n\nSecurity Operations (SecDevOps/DevSecOps):\n\nAI enhances security by rapidly detecting unusual activity, identifying vulnerabilities in code or infrastructure, and automating responses to security threats. It can analyze large datasets of security logs to pinpoint subtle attack patterns. AI is increasingly integrated into DevOps to automate, optimize, and enhance various stages of the software delivery lifecycle. Here are some key applications:\n\n\n\nIntelligent Automation &amp; Code Generation: AI assists developers with real-time code suggestions, auto-completion, and even generating boilerplate code or scripts (e.g., Dockerfiles, CI/CD pipeline configurations). This speeds up development and reduces manual errors.\n\n\nPredictive Analytics &amp; Anomaly Detection: AI algorithms analyze vast amounts of operational data (logs, metrics, traces) to predict potential system outages, performance bottlenecks, or security vulnerabilities before they occur. It can also detect unusual patterns that indicate active problems or attacks.\n\n\nAutomated Testing &amp; Quality Assurance: AI tools can generate smart test cases, prioritize tests, analyze test results to pinpoint root causes of failures, and even automate UI testing. This accelerates testing cycles, improves accuracy, and helps “shift left” bug detection.\n\n\nEnhanced Monitoring &amp; Incident Management (AIOps): AI powers AIOps platforms that correlate events, reduce alert noise, automatically identify the root cause of incidents, and even suggest or trigger automated remediation actions. This significantly reduces Mean Time To Resolution (MTTR).\n\n\nResource Optimization &amp; Cost Management: AI analyzes usage patterns and forecasts resource demand to intelligently scale infrastructure (e.g., cloud instances, Kubernetes pods) up or down automatically. This ensures optimal performance while minimizing cloud spending.\n\n"},"DevOps/SSL-or-TLS-Offloading":{"slug":"DevOps/SSL-or-TLS-Offloading","filePath":"DevOps/SSL or TLS Offloading.md","title":"SSL or TLS Offloading","links":[],"tags":[],"content":"SSL Offloading (more accurately called TLS Offloading as SSL is an older, less secure protocol that TLS superseded, but the term “SSL” stuck) is a technique where the process of encrypting and decrypting secure network traffic (HTTPS) is moved from your backend web servers to a dedicated device, typically a Load Balancer (LB).\nImagine your website or application as a secure conversation between your user’s browser and your servers.\n\n\nThe “Cost” of Encryption:\n\nWhen a user connects to your website via HTTPS, their browser and your web server perform a complex cryptographic handshake (exchanging keys, verifying certificates) and then continuously encrypt and decrypt all data exchanged.\nThis encryption/decryption process is CPU-intensive. Every secure connection adds a load to your web servers.\n\n\n\nWhat “Offloading” Means:\n\n“Offloading” literally means taking this heavy encryption/decryption workload “off” your backend web servers and shifting it to another device (the Load Balancer).\n\n\n\nHow SSL Offloading Works (The Process):\n\nClient Connects Securely to LB: A user’s web browser initiates an HTTPS connection to your Load Balancer (e.g., your www.example.com domain, which points to the LB). The LB presents your website’s SSL/TLS certificate to the browser.\nLB Decrypts Traffic: The Load Balancer performs the cryptographic handshake and decrypts the incoming secure traffic from the client.\nLB Forwards Unencrypted to Backend (Typically): The Load Balancer then forwards the now unencrypted (plain HTTP) traffic to one of your backend web servers.\nBackend Processes Request: Your backend web server receives the request, processes it, and generates a response in plain HTTP.\nLB Encrypts Response: The Load Balancer receives the plain HTTP response from the backend, encrypts it, and then sends it back securely over HTTPS to the client’s browser.\n\nSee the flow: Client (HTTPS) &lt;---(encrypted)---&gt; Load Balancer &lt;---(unencrypted)---&gt; Backend Server\nWhy is it Done? (The Benefits)\n\n\nReduced Backend Server Load (Performance):\n\nYour web servers no longer have to spend valuable CPU cycles on encryption/decryption. They can dedicate all their resources to processing application logic, serving content, and handling more concurrent requests. This means better performance and scalability for your applications.\n\n\n\nSimplified Backend Configuration:\n\nYour backend servers no longer need to manage SSL/TLS certificates, private keys, or perform cryptographic operations. They just serve plain HTTP, making their setup and maintenance simpler.\n\n\n\nCentralized Certificate Management:\n\nAll your SSL/TLS certificates are managed in one place (the Load Balancer). This simplifies renewal, updates, and overall security posture, as you don’t need to deploy certificates to every single backend server.\n\n\n\nImproved Security Posture (in some cases):\n\nYour private keys (the most sensitive part of your certificate) reside only on the Load Balancer, which is typically a highly secured and hardened appliance or service. Your less-secure backend application servers don’t need direct access to them.\n\n\n\nEnhanced Load Balancing Decisions:\n\nBecause the traffic is decrypted at the LB, a Layer 7 Load Balancer can inspect the actual content of the request (like URL paths, HTTP headers, cookies) to make more intelligent routing decisions, something it couldn’t do if the traffic remained encrypted.\n\n\n\nWhere does it happen?\nSSL Offloading is a primary feature of Layer 7 (Application) Load Balancers, like AWS’s Application Load Balancer (ALB), NGINX configured as an L7 proxy, HAProxy, Envoy, etc. An L4 Load Balancer (like AWS NLB) cannot perform SSL offloading because it operates below the application layer where SSL/TLS encryption/decryption takes place."},"DevOps/Tools/Ansible":{"slug":"DevOps/Tools/Ansible","filePath":"DevOps/Tools/Ansible.md","title":"Ansible","links":[],"tags":[],"content":"\nAnsible is an open-source IT automation tool.\nIt has many built-in modules which are dedicated for each task.\nAnsible module:\n\nCore Module — come with Ansible by default.\nCustom Module — we can create our own module.\n\n\nIt is used for configuration management, application deployment etc.\nIt is written in Python language.\nPorts:\n\nPort 22 is used to connect with remote Linux machine - uses SSH protocol\nPort 5985 is used to connect with remote Windows machine - HTTP - uses WinRM client\nPort 5986 is used to connect with remote Windows machine - HTTPS - uses WinRM client\n\n\nAnsible Controller - only Linux machine can be used here\nRemote Machine  - can be any like Windows or Linux.\n\nBasics of automation using Ansible | Automate any task\n\n3 components -\n\nInventory file - where we have remote machine details.\nAnsible Playbook - where we mention the details what need to be done.\nCommand - command we need to execute\n\ninventory.txt file\nremote_server ansible_host=10.122.28.134 ansible_ssh_user=centos ansible_ssh_pass=sshpassword\nPlaybook\n\nCommand:\nansible-playbook playbook.yml -i inventory.txt -vv\nAnsible Role\n\n\nIf we want to reuse and share any playbook with the community then “ansible role” is the way to go.\nAnsible role is a standard way of structuring &amp; developing ansible playbook.\nThere are mainly two ways through which we can create ansible role.\n\nBy using ansible-galaxy command line tool - ansible-galaxy init your_role_name\nBy manually creating folder structure like ansible role’s directory structure\n\n\n\nFiles/directories inside the role directory\n\n\nExplanation of Key Components\n\nREADME.md : A markdown file that describes the role, its purpose, and how to use it.\ndefaults/ : Contains default variables for the role. These variables can be overridden by user-provided variables.\nfiles/ : Contains files that you want to copy to the remote host. These files can be referenced in your tasks.\nhandlers/ : Contains handlers, which are special tasks that are only run when notified by another task. Handlers are typically used for restarting services or reloading configurations.\nmeta/ : Contains metadata about the role, such as dependencies, author information, and versioning. The main.yml file in this directory is used to define role dependencies.\ntasks/ : Contains the main tasks that the role will execute. The main.yml file is the entry point for the role’s tasks.\ntemplates/ : Contains Jinja2 templates that are rendered and placed on the remote host. Templates are often used for configuration files.\nvars/ : Contains variables that are specific to the role. These variables take precedence over default variables.\n\nAnsible Vault\n\nAnsible vault is a feature of Ansible which help us to encrypt sensitive information (e.g., credential, key etc.), playbook (YAML file), inventory file etc.\n\nIt uses ansible-vault command line tool.\nBasic commands:\n1 ansible-vault create &lt;file_name&gt; - this is for creating an encrypted file.\n\nansible-vault decrypt &lt;file_name&gt; - this is for decrypting any ansible vault encrypted file.\nansible-vault edit &lt;file_name&gt; - this is used to edit the encrypted file\nansible-vault view &lt;file_name&gt; - this is for viewing any encrypted pipeline\nansible-vault encrypt &lt;file_name&gt; - this is for encrypting any existing file.\nansible-vault encrypt_string &lt;my_string&gt; - this is used to encrypt any string\n\n\n"},"DevOps/Tools/Service-Mesh-Tools":{"slug":"DevOps/Tools/Service-Mesh-Tools","filePath":"DevOps/Tools/Service Mesh Tools.md","title":"Service Mesh Tools","links":[],"tags":[],"content":"Popular tools\n\nIstio - Most popular\nLinkerd\nAWS App Mesh\nHashiCorp Consul Connect\n\n“Why use a service mesh tool like Istio in Kubernetes?”\n✅ Architect-Level Answer\n\n“Istio is used in Kubernetes to provide secure, reliable, and observable communication between microservices — without changing application code. It brings centralized control over service-to-service traffic, enabling platform teams to enforce security, routing, and policy at the infrastructure layer.”\n\n📘 Deeper Explanation for Architects\nKubernetes handles service orchestration and discovery, but it doesn’t natively provide:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRequirementNative K8s Support?Istio Support?End-to-end mTLS encryption❌ Manual✅ AutomaticFine-grained traffic routing❌ Limited (Ingress)✅ Full control (VirtualService)Built-in zero trust security❌✅ With PeerAuth + AuthPolicyObservability (metrics, tracing)❌ (needs setup)✅ Out-of-the-boxCentralized policy enforcement❌✅ With APIs + CRDs\n🧩 Key Benefits for Platform Architects\n🔒 1. Security / Zero Trust\n\nIstio enables mTLS by default — encrypts all service-to-service traffic.\nImplements identity-based access between services (AuthorizationPolicy).\n\n🔁 2. Traffic Management\n\nPerform canary deployments, traffic splitting, retries, failovers, and circuit breakers without touching code.\nDefine routing rules with headers, paths, percentages using VirtualService.\n\n📊 3. Observability\n\nBuilt-in metrics, logging, and distributed tracing.\nIntegrates with Prometheus, Grafana, Jaeger, etc.\n\n🧠 4. Consistent Cross-Cutting Policies\n\nApply security, telemetry, rate limits, and fault tolerance uniformly across all services.\nActs as a policy layer, separating concerns from app logic.\n\n🧱 5. Application-Agnostic Architecture\n\nNo language or framework dependency — works for Java, Python, Go, Node.js etc.\nWorks even with legacy services running in the cluster.\n\n🏗️ Real-Life Use Case in Enterprise:\n\n“We used Istio to securely expose internal APIs to the internet using an ingress gateway. We enabled mutual TLS for backend traffic, restricted inter-service access using AuthorizationPolicy, and routed 10% of user traffic to a new version using VirtualService — all without changing any application code.”\n\n\n🎯 Summary for Interview / Architecture Doc\n\n“Istio adds a powerful control plane to Kubernetes networking — enabling secure, observable, and controlled service-to-service communication. It helps platform teams standardize traffic behavior and enforce cross-cutting concerns, while keeping developers focused on writing business logic.”\n\n1. 🔄 Standardizing Traffic Behavior (Platform Concern)\nPlatform teams are responsible for enforcing consistent rules across all services:\n\nHow traffic is routed (e.g., canary, blue-green, weighted rollout)\nHow timeouts, retries, and circuit breakers are handled\nHow failures are detected and reported\n\n👉 Without Istio: Each development team must implement this logic inside their own code, which is error-prone and inconsistent.\n👉 With Istio: The Envoy sidecar proxies handle all this at the network layer, outside the app, using declarative configs (VirtualService, DestinationRule).\n2. 🛡️ Cross-Cutting Concerns (Security, Telemetry, Policy)\nThese are concerns that affect all services:\n\nSecurity (e.g., mTLS encryption, who can talk to whom)\nObservability (logs, metrics, traces)\nPolicy enforcement (rate limiting, RBAC, authz)\n\n👉 Istio enables uniform enforcement of these concerns across the entire mesh, regardless of language, framework, or team.\nThis avoids:\n\nDuplication of effort\nInconsistent implementations\nSecurity gaps due to human error\n\n3. 👨‍💻 Freeing Developers to Focus on Business Logic\nDevelopers no longer need to:\n\nWrite custom code for retry/backoff logic\nHandle service discovery or failovers\nWorry about metrics collection or request tracing\nManually manage API authentication between microservices\n\n👉 Their job becomes simpler and faster: just write the business logic, and infrastructure-level concerns are handled by the mesh.\n🏗️ Example in Practice:\nSuppose your company has 100 microservices written in multiple languages.\n\nWith Istio:\n\nPlatform team defines VirtualService and DestinationRule for rollout control.\nPeerAuthentication enables mTLS everywhere by default.\nAuthorizationPolicy ensures that only the right services can call sensitive APIs.\nTracing and metrics are automatically collected into Prometheus/Grafana.\n\n\n\n\n✅ All of this without changing a single line of application code.\n\n\n🔑 Conclusion\n\nIstio abstracts and centralizes platform responsibilities — enabling safer, more observable, and secure service communication — while developers stay focused on building features, not infrastructure.\n\nIstio Ingress Gateway\n\nUsing an Istio Ingress Gateway is a significant reason to choose Istio, but it’s typically not the sole or primary reason for adopting a full service mesh.\nHere’s why:\n\n\nIs it a reason to use Istio? Yes, a significant one.\n\nThe Istio Ingress Gateway provides advanced traffic management capabilities at the edge of your cluster that standard Kubernetes Ingress controllers (like Nginx Ingress) often lack out-of-the-box. These include:\n\nWeighted Routing: Splitting traffic to different versions (e.g., for canary releases that originate from outside the cluster).\nRequest Mirroring: Sending a copy of live traffic to a new version for testing without impacting users.\nTraffic Shaping: Applying timeouts, retries, circuit breakers to incoming requests.\nAuthentication/Authorization: Centralized enforcement of policies for external access.\nObservability: Full tracing and metrics for incoming requests, integrated with the mesh’s overall observability.\n\n\nIt brings your external traffic control under the same policy and observability umbrella as your internal service-to-service traffic, centralizing management.\n\n\n\nIs it a secondary reason? More accurately, it’s an integrated reason.\n\nWhile the core value of Istio lies in managing internal service-to-service communication, the Ingress Gateway extends these powerful features to traffic entering the mesh. It completes the picture, providing end-to-end control and observability.\nYou wouldn’t typically deploy Istio just for its Ingress Gateway if you didn’t also need its service mesh capabilities for internal services. The real power is in the consistency and unified control plane it offers from external entry points all the way to internal microservices.\n\n\n\nCan Ingress Gateway be used by Nginx also?\n\nYes, Nginx is a very popular standalone Ingress Controller for Kubernetes.\nA standard Nginx Ingress Controller handles basic routing, SSL termination, and load balancing for external traffic.\nHowever, it does not offer the advanced L7 traffic management, mTLS, or integrated tracing that an Istio Ingress Gateway provides without significant custom configuration or additional tools.\nYou could use an Nginx Ingress Controller without Istio, or even alongside Istio (though that adds complexity).\n\n\n\nIn short: Istio’s Ingress Gateway is a powerful extension of its service mesh capabilities to the cluster edge, offering advanced traffic control and policy enforcement not typically found in standalone Ingress controllers like Nginx. It’s a strong feature of Istio, but its value is maximized when integrated with the mesh’s internal service management.\nArchitecture / Components of Istio:\n\n\n🛡️ 1. PeerAuthentication\n\nPurpose: Enforces mTLS (mutual TLS) for pod-to-pod communication.\nIt defines how traffic must be secured between services.\nExample:\n\napiVersion: security.istio.io/v1beta1\nkind: PeerAuthentication\nspec:\n  mtls:\n    mode: STRICT\n\n✅ Why this matters:\nIstio sidecars (Envoy) use this config to encrypt traffic with client/server identity verification.\n🔐 2. AuthorizationPolicy\n\nPurpose: Defines who can access this service and under what conditions.\nWorks at L7 (HTTP) or L4 (TCP) levels.\nIt matches:\n\nSource service accounts\nNamespace\nPaths, methods, ports, etc.\n\n\nExample:\n\nkind: AuthorizationPolicy\nspec:\n  selector:\n    matchLabels:\n      app: backend\n  rules:\n  - from:\n    - source:\n        principals: [&quot;cluster.local/ns/frontend/sa/frontend-sa&quot;]\n\n✅ Why this matters:\nThis enforces Zero Trust security between internal microservices.\n📊 3. Telemetry Stack\n\nPurpose: Collects:\n\nMetrics (Prometheus)\nLogs (Fluentd / Loki)\nTraces (Jaeger / Zipkin)\n\n\nIstio’s sidecar proxies (Envoy) automatically collect this without code changes.\n✅ Why this matters:\nYou get deep observability (latency, errors, routes, retries, etc.) for every service-to-service hop.\n"},"DevOps/Tools/Terraform":{"slug":"DevOps/Tools/Terraform","filePath":"DevOps/Tools/Terraform.md","title":"Terraform","links":[],"tags":[],"content":"Important commands\n\nterraform init - It initializes terraform working directory and downloads the provider plugins. Once we run this command it will create .terraform directory.\n\nterraform validate\nterraform plan\nterraform apply\n\nterraform apply —auto-approve\n\n\nterraform destroy\n\nterraform destroy —auto-approve\n\n\n\nterraform import\n\nThe terraform import command is used to bring an existing real-world infrastructure resource under Terraform management.\nExample:\nYou manually created an AWS EC2 instance, but now you want Terraform to manage it. Instead of destroying and recreating it via Terraform, you import it into your state.\nWhat it does:\n\nAdds a mapping between your Terraform resource block and the existing real-world resource in the state file (terraform.tfstate).\nDoes NOT create or modify any resource.\nYou must already have the corresponding .tf code written — or write it after import to match the imported resource.\n\nExample:\nterraform import aws_instance.my_server i-1234567890abcdef0\nThis tells Terraform:\n\n“Hey, the EC2 instance with ID i-1234567890abcdef0 should be tracked as aws_instance.my_server.”\n\n🚨 Important:\n\nAfter import, Terraform knows about the resource, but it doesn’t write the .tf code for you — you must create the matching resource block manually.\nIf .tf code doesn’t match the real resource, terraform plan may show unexpected changes.\n\nterraform out\n\nTo retrieve information from any existing resource\nterraform refresh\n\n\nWhen we do any manual changes in the resources from portal (where resource was deployed by terraform), in that case if we directly run the terraform code, all those changes will be destroyed.\nTo avoid this, we need to two things -\n\nTo reflect that changes in state file we need to run “terraform refresh”.\nAnd after that we need to manually implement the changes in terraform files, Otherwise there will be difference in current state (your terraform files) and desired state (your state file or .tfstate file). As your state file is already updated so you need to match the state file (current state) with desired state (that is your config mentioned in .tf files)\n\n\n\nterraform state mv\n\nWhen we change any resource name in terraform files, then to reflect that change in terraform state file we need to run this command. Here the context is, we are not changing any real world infra name (which might exist in your cloud like Azure or AWS, etc.), but we are changing the logical name which we used to point that resource. In variable or tfvars files we won’t change the original name which we want to give the resource (or the name that will be visible in web portals).\nWhen you change the name of a resource in your Terraform code (e.g., from aws_instance.old_name to aws_instance.new_name), Terraform doesn’t automatically recognize it as the same resource — it will think the old one was deleted and a new one needs to be created.\nTo preserve the existing infrastructure and update the state file to reflect the new name without recreating the resource, you use:\nterraform state mv aws_instance.old_name aws_instance.new_name\n🔍 Why use it?\n\nIt updates the Terraform state file to point the new logical name to the existing resource.\nPrevents unnecessary destruction and recreation of infrastructure.\n\nThe real-world resource remains untouched and unaffected.\nWhen you run:\nterraform state mv aws_instance.old_name aws_instance.new_name\nYou’re only telling Terraform:\n\n“Hey, this existing resource (already provisioned) should now be tracked under this new name in the state file.”\n\n🔒 What happens internally:\n\nNo changes are made to the actual cloud resource (e.g., EC2 instance, S3 bucket, etc.).\nOnly the Terraform state file (terraform.tfstate) is updated to reflect the new logical name.\nDuring the next terraform plan, Terraform sees no changes needed for that resource.\n\n🔄 Without state mv, Terraform would:\n\nThink the old resource was deleted.\nTry to create a new one with the new name.\nResult: Unnecessary resource recreation and potential downtime.\n"},"System-Design/API":{"slug":"System-Design/API","filePath":"System Design/API.md","title":"API","links":[],"tags":[],"content":"✅ What is an API?\nAPI = Application Programming Interface\nIt’s like a waiter in a restaurant — takes your order, gives it to the kitchen, brings food back.\n\n✅ Example:\nYour shopping app → calls Payment API → to process card payment.\nIf API says “Success”, payment goes through.\n\n✅ When can API crash?\n\nToo many users (overload)\nBug in code\nDatabase down\nNetwork timeout\nThird-party system fails (e.g. bank)\n\n\n✅ Why it crashes on Black Friday?\nMillions hit the site at once → Server can’t handle load → Boom! Crash.\n\n💡 In short:\nAPI is how apps talk.\n✅ Example of API:\nWhen you use a weather app, it might call this API:\napi.weather.com/current\nThis is an API endpoint — it sends a request to a server and gets back data like:\n{   &quot;city&quot;: &quot;Delhi&quot;,    \t&quot;temperature&quot;: &quot;34°C&quot;,    \t&quot;condition&quot;: &quot;Sunny&quot;    }\n\n✅ How API is different from a URL:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeatureAPI URLRegular URLPurposeMeant for machines to get/send dataMeant for humans to view websitesReturnsData (JSON, XML)Web page (HTML, images)Exampleapi.weather.com/...https://www.weather.com/delhiUserApp or codeBrowser or user\n\n💡 Think of it like this:\n🔹 API URL = backend data source for apps\n🔹 Regular URL = frontend page for users\nMajor API types or styles\n\nREST:\nCore idea is treating everything as resource. A resource being a user profile, a article, a product, etc. Each piece of resource get a unique web address (URL). Interaction is done through standard web action basically HTTP.\nIt is stateless. Each request must contain all the info, it does remember past requests.\nStateless is the reason it is scalable and due to which it is widely used in public web APIs.\nRPC (Remote Procedure Call):\nCalling a function or procedure on remote machine. It is great for internal efficient call.\nGraphQL:\nInstead of sending all data, it sends specific data. Basically minimizing data transfer is key. It is used when client needs fine grain control over data transfer."},"System-Design/All-Gateways":{"slug":"System-Design/All-Gateways","filePath":"System Design/All Gateways.md","title":"All Gateways","links":[],"tags":[],"content":"🌐 1. NAT Gateway (Network Address Translation Gateway)\n🔹 Purpose:\nAllows private subnet resources (like EC2 instances or Kubernetes pods) to initiate outbound internet access without being publicly exposed.\n✅ Use When:\n\n\nYou have private subnets (no public IPs) that need internet access (e.g., for updates, APIs).\n\n\nYou want outbound internet traffic without inbound exposure.\n\n\n🛠️ Used In:\n\nAWS: NAT Gateway in VPC\nAzure: NAT Gateway for subnets\nGCP: Cloud NAT\n\n📍 Where:\n\nBetween private subnet and internet\nSits in public subnet routing internet-bound traffic from private resources\n\n\n🌍 2. Internet Gateway\n🔹 Purpose:\nAllows public subnets/resources (with public IPs) to communicate directly with the internet, both inbound and outbound.\n✅ Use When:\n\nResources (e.g., public EC2, ALB, Ingress Controller) need to be accessible from the internet\nLoad balancers, bastion hosts, etc.\n\n🛠️ Used In:\n\nAWS VPC: Internet Gateway\nAzure: Public IP and routing\nGCP: Default route with external IP\n\n📍 Where:\n\nAttached to VPC, required for public subnets\n\n\n🧭 3. API Gateway\n🔹 Purpose:\nManages, authenticates, monitors, throttles, and secures API calls. It acts as the entry point for external clients to backend services.\n✅ Use When:\n\nYou expose microservices via REST/HTTP APIs or WebSockets\nNeed features like rate-limiting, token validation, CORS, logging, etc.\n\n🛠️ Used In:\n\nAWS: API Gateway\nAzure API Management\nGCP: API Gateway\nAlso: Kong, Tyk, Apigee\n\n📍 Where:\n\nAt application layer, in front of microservices\n\n\n📥 4. Ingress Gateway (Layer 7)\n🔹 Purpose:\nAccepts external traffic into a Kubernetes cluster, routes to appropriate service inside the cluster (often using L7 HTTP rules).\n✅ Use When:\n\nYou deploy Kubernetes and need HTTP/S access to services\nYou use Istio, NGINX Ingress Controller, or Traefik\n\n🛠️ Used In:\n\nKubernetes clusters\nTypically with Istio or Ingress Controller\n\n📍 Where:\n\nInside cluster (as service), exposed via LoadBalancer or NodePort\n\n\n📤 5. Egress Gateway (Layer 7 or 4)\n🔹 Purpose:\nManages outbound traffic from Kubernetes workloads to external services (e.g., internet or external APIs), applying policies, TLS termination, logging, etc.\n✅ Use When:\n\nYou want controlled and observable outbound traffic\nRestrict which services can talk to external APIs\nApply MTLS, policies, DNS control\n\n🛠️ Used In:\n\nIstio egress gateway\nService Mesh (Linkerd, Consul)\nSquid proxy setups\n\n📍 Where:\n\nInside cluster\nBetween cluster and external world (Layer 7 egress control)\n\n\n🔁 Summary Table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGateway TypeDirectionUse CasePlatformNAT GatewayOutboundPrivate subnet → Internet (no public IP)AWS, Azure, GCPInternet GatewayInbound + OutPublic resources needing internet accessAWS, Azure, GCPAPI GatewayInboundClient → Managed API (rate-limit, auth, etc.)AWS API Gateway, Azure API MgmtIngress GatewayInboundInternet → K8s Services (via HTTP/S routing)Istio, NGINX, TraefikEgress GatewayOutboundK8s Services → Internet (with MTLS, audit, policies)Istio, Consul, Service Mesh\n\nInternet Gateway : For direct VPC-to-internet communication, attached to VPC\nNAT Gateway : For outbound internet access from private subnets, requiring a public subnet.\nAPI Gateway : For managing and routing API requests to backend services.\nIngress Gateway : For routing external traffic to services within a Kubernetes cluster.\nEgress Gateway : For managing and controlling outbound traffic from a network or cluster.\n\nInternet Gateway Attachment\n\n\nVPC Level :\n\nAn Internet Gateway is associated with a specific Virtual Private Cloud (VPC).\nThis association allows all subnets within the VPC to potentially access the internet, depending on their configuration.\n\n\n\nSubnet Configuration :\n\nTo enable internet access for instances in a subnet, the subnet must be configured as a public subnet.\nA public subnet has a route table that includes a route to the Internet Gateway for outbound traffic.\nThis route table directs traffic destined for the internet to the IGW.\n\n\n\nPrivate Subnets :\n\nInstances in private subnets do not have direct access to the internet.\nTo allow outbound internet access from private subnets, a NAT Gateway or NAT Instance is used.\nThe private subnet’s route table is configured to route outbound internet traffic to the NAT Gateway or NAT Instance.\n\n\n\nKey Points\n\nInternet Gateway : Attached to the VPC, enabling potential internet access for all subnets within that VPC.\nPublic Subnets : Have a route table with a route to the IGW, allowing direct internet access.\nPrivate Subnets : Use a NAT Gateway or NAT Instance to access the internet indirectly.\n"},"System-Design/Articles--and--YT-Tutorials":{"slug":"System-Design/Articles--and--YT-Tutorials","filePath":"System Design/Articles & YT Tutorials.md","title":"Articles & YT Tutorials","links":[],"tags":[],"content":"YT channels:\n\nHello Interview\nGaurav Sen\nSystem Design Fight club\nJordan has no life\nExponent System Design\n\nHow Canva Migrated 100M+ MAU Data with Zero Downtime Using DMS\nwww.techopsexamples.com/p/how-canva-migrated-100m-mau-data-with-zero-downtime-using-dms\nHexagonal Architecture in AWS\nwww.techopsexamples.com/p/hexagonal-architecture-in-aws\nHow MxPlayer Scales with EKS SPOT Instances\nwww.techopsexamples.com/p/how-mxplayer-scales-with-eks-spot-instances\nImplementing AWS Single Sign-On (SSO) Crash Course\nwww.techopsexamples.com/p/implementing-aws-single-sign-on-sso-crash-course\nAWS Internet Gateway vs NAT Gateway – Which One to Choose?\nwww.techopsexamples.com/p/aws-internet-gateway-vs-nat-gateway-which-one-to-choose\nAWS VPC Network Segmentation Break Down\nwww.techopsexamples.com/p/aws-vpc-network-segmentation-break-down\n\nHello Interview Deep Dives:\nwww.youtube.com/playlist\nHello System Design Examples -\nwww.youtube.com/playlist\nCrash course - youtu.be/F2FmTdLtb_4"},"System-Design/Authentication":{"slug":"System-Design/Authentication","filePath":"System Design/Authentication.md","title":"Authentication","links":[],"tags":[],"content":"➡️ Authentication can be handled in two primary ways:\n\n✅ 1. Istio-Level (Gateway + VirtualService-based routing)\n(Less common practice)\nIf we are using service mesh tool like istio, then we can use istio-gateway as ingress gateway which would be entry point to the cluster.\nNext, with Istio we must need to have VirtualService. VirtualServices are connected to the Istio-gateway. In the virtualservices we need to provide path (URL context path).\nWe need to have one virtualservice which will have path like /auth or etc. to forward the traffic to some authentication tool like keycloak for user authentication.\n\n🔸 You define authentication policies and routing logic via Istio itself.\n🔸 Example:\n\nIf request is /auth → route to Keycloak\nUse RequestAuthentication + AuthorizationPolicy\n\n\n👉 The below approach externalizes auth logic from app code.\n✅ 2. Application-Level Authentication\n🔸 The application itself (application logic) handles authentication like this:\n\nA user hits /app\nThe frontend app (Test-UI) checks:\n\nIf the user has a valid token/session/cookie\nIf not → it redirects the user to Keycloak (e.g., using OAuth2/OpenID Connect flow)\n\n\nAfter login, Keycloak redirects the user back to /app with an access token\nApp uses the token to make further API calls\n\n💡 This pattern is called a frontend-driven auth flow and is very common when using:\n\nKeycloak\nAuth0\nOkta\nAzure AD, etc.\n\n\n\n🔍 Real-World Behavior\n\nThe Istio Gateway only forwards traffic to the app.\nThe Test-UI frontend app:\n\nDetects unauthenticated users\nTriggers the redirect to Keycloak (/auth/realms/...)\nHandles the auth callback\n\n\nNo separate VirtualService is required for Keycloak, because the auth redirection is initiated by app logic.\n\n🧠 Summary\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAuth LocationLogic Lives Where?Keycloak Routing Needed? Like having /auth path definedIstio-basedDeclarative YAML (VirtualService, AuthPolicy)✅ YesApp-drivenInside frontend/backend code❌ No.App calls Keycloak directly."},"System-Design/Basics-of-System-Design":{"slug":"System-Design/Basics-of-System-Design","filePath":"System Design/Basics of System Design.md","title":"Basics of System Design","links":[],"tags":[],"content":"System Design Fundamentals (for DevOps):\n\n\nUnderstanding Requirements:\n\nFunctional: What the system does (e.g., handles user login, processes payments).\nNon-Functional (NFRs): How well the system does it (e.g., 99.99% uptime, 1000 transactions/sec, 200ms latency). NFRs drive design choices.\n\n\n\nCore Pillars:\n\nScalability: Ability to handle increasing load (users, data).\n\nVertical Scaling: More power to single machine (bigger CPU, RAM).\nHorizontal Scaling: More machines (add servers). This is preferred for cloud-native.\n\n\nReliability / High Availability (HA): System remains operational despite failures.\n\nRedundancy: Duplication of components (e.g., multiple servers, databases).\nFailover: Automatic switch to backup components.\n\n\nPerformance / Latency: How fast the system responds.\n\nCaching: Store frequently accessed data closer to user/application.\nLoad Balancing: Distribute traffic across servers.\nOptimizing Databases/Queries.\n\n\nSecurity: Protecting data and access (encryption, access control, firewalls).\nMaintainability / Operability: Ease of management, monitoring, troubleshooting, updates.\n\nObservability: Logging, Monitoring, Tracing.\nAutomation: CI/CD, Infrastructure as Code (IaC).\n\n\nCost Optimization: Achieving goals efficiently within budget.\n\n\n\nFundamental Components &amp; Patterns:\n\nLoad Balancers: Distribute incoming network traffic.\nDatabases:\n\nSQL (Relational): Structured data, strong consistency (e.g., PostgreSQL, MySQL).\nNoSQL (Non-Relational): Flexible schemas, scalability, availability (e.g., MongoDB, Cassandra, Redis).\n\n\nCaching Layers: Improve read performance (e.g., Redis, Memcached).\nMessage Queues: Decouple services, handle asynchronous communication (e.g., Kafka, RabbitMQ, SQS).\nMicroservices Architecture: Break down large applications into smaller, independent services.\nContainers &amp; Orchestration: Package applications (Docker) and manage them at scale (Kubernetes).\nAPIs (REST/gRPC): How services communicate.\n\n\n\nTrade-offs:\n\nEvery design decision involves compromises (e.g., consistency vs. availability, performance vs. cost, complexity vs. scalability). A Senior DevOps Engineer needs to identify and navigate these.\n\n\n\nEssentially, it’s about applying these principles and components to create robust, automated, and observable systems that reliably deliver software value."},"System-Design/HTTP-Request-vs-REST-API-Request":{"slug":"System-Design/HTTP-Request-vs-REST-API-Request","filePath":"System Design/HTTP Request vs REST API Request.md","title":"HTTP Request vs REST API Request","links":[],"tags":[],"content":"HTTP is just way to transfer data, but the structure of data which is being transferred is defined by the types of API like REST, RPC, GraphQL.\n🔑 Here’s the clear distinction:\n\nHTTP = just a transport protocol (like a delivery truck)\nAPI styles like REST, RPC, GraphQL = define the structure, rules, and behavior of the data being transferred\n\n\n🧠 Think of it like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLayerRoleHTTPLow-level transport: how messages are sent and receivedREST / RPC / GraphQLHigh-level API contract: what data is sent, and how it’s organized &amp; interpreted\n\n🔧 Example:\nHTTP request:\nPOST /createUser HTTP/1.1 Content-Type: application/json  { &quot;name&quot;: &quot;John&quot;, &quot;email&quot;: &quot;john@example.com&quot; }\n\nHTTP handles: connection, status codes, headers, body\nREST (or GraphQL or RPC) tells us:\n\nWhat does /createUser mean? What structure is expected in the JSON? What response to expect?\n\n\n\n\n✅ So:\n\nHTTP = wire\nAPI = language spoken over the wire\n"},"System-Design/Introduction":{"slug":"System-Design/Introduction","filePath":"System Design/Introduction.md","title":"Introduction","links":[],"tags":[],"content":"Try to understand system design deeply is crucial in the AI era. It’s not enough to just take a course and build simple models (like “boxes” with databases); you need to grasp fundamental computer science concepts such as:\n\nHow networks work\nHow databases function\nOperating systems\nBrowsers\nProgramming languages\nServers and APIs\nThis knowledge is becoming essential, not optional, as interviewers now expect candidates to understand these systems to handle complex, real-world applications effectively. In short, mastering system design means understanding the entire ecosystem behind software, which helps you build smarter, scalable, and maintainable applications beyond just coding basics.\n"},"System-Design/L3,-L4--and--L7-Traffic-in-Architecture":{"slug":"System-Design/L3,-L4--and--L7-Traffic-in-Architecture","filePath":"System Design/L3, L4 & L7 Traffic in Architecture.md","title":"L3, L4 & L7 Traffic in Architecture","links":[],"tags":[],"content":"📚 OSI Layer Refresher\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLayerNameExample ProtocolsTypical Use CasesL3Network LayerIP, ICMPRouting, Pod IP traffic, network policiesL4Transport LayerTCP, UDPPort-based access (e.g., allow TCP 80)L7Application LayerHTTP, HTTPS, gRPC, DNSAPI calls, web requests, database queries\n\n🔷 What is L3 Traffic?\nL3 = IP-level traffic\n\nOperates at IP address level: source and destination IPs.\nNetwork routing decisions are made at L3.\nSecurity policies (like Kubernetes NetworkPolicies) use CIDRs, namespaces, and labels to control L3.\n\n✅ Examples:\n\nPod A (IP: 10.244.0.5) sends traffic to Pod B (10.244.0.9) regardless of what protocol is used.\nAllow or deny traffic based on IP/CIDR range.\n\n# Kubernetes L3 NetworkPolicy (example)\negress:\n  - to:\n    - ipBlock:\n        cidr: 10.244.0.0/16\n\n🔷 What is L7 Traffic?\nL7 = Application-aware traffic\n\nUnderstands and inspects application-level protocols (HTTP, HTTPS, gRPC, etc).\nYou can filter by method, path, header, host, etc.\nTypically enforced by service mesh (e.g., Istio, Linkerd), API gateways, or web proxies like NGINX/Envoy.\n\n✅ Examples:\n\nOnly allow POST /payment to payments.example.com\nBlock /admin path from non-authenticated users\nAllow only certain HTTP headers or tokens\n\n# Istio AuthorizationPolicy — L7 aware\nrules:\n  - to:\n      - operation:\n          methods: [&quot;GET&quot;]\n          paths: [&quot;/public&quot;]\n\n\nAPI calls (HTTP/gRPC) between services are L7 traffic, transported over L4 TCP, and routed at L3.\nUser/browser → HTTPS (L7)\n                ↓\n           TCP (L4)\n                ↓\n            IP (L3)\n\n\n✅ Example:\nA frontend service calling http://backend:8080/api/data:\n\nL3: IP routing between pods\nL4: TCP connection on port 8080\nL7: HTTP GET /api/data call (can be L7-inspected)\n\n📌 Summary Table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommunicationL3L4L7NotesPod-to-Pod (by IP)✅✅❌Raw IP trafficHTTP API calls (internal apps)✅✅✅Common in microservices (HTTP, gRPC)HTTPS from browser✅✅✅Typical L7 (TLS + HTTP)DNS query to CoreDNS✅✅✅DNS is L7 (app-layer protocol)UDP metrics/logs✅✅❌UDP is L4, no L7 parsing\n\n🧠 Architect-Level Insight\n\nL3/L4: Used by Kubernetes NetworkPolicies, Calico, Cilium in basic mode\nL7: Used by Istio, NGINX Ingress, Envoy, API Gateways\nFor zero-trust, use L3+L7 together — start with deny-all L3, allow via L7 AuthZ policies\n\n\n✅ Traffic Flow Description:\n\nUser → Ingress Gateway:\n\n🔒 HTTPS (L7) initiated from browser\nLands at Ingress Gateway (L7 termination)\n\n\nIngress Gateway → Frontend Service:\n\nRouted via Istio VirtualService (L7)\nForwarded as HTTP over TCP (L4 + L7)\n\n\nFrontend Pod → Backend Pod:\n\nInternal HTTP API call (L7)\n\n\nBackend Pod → Database Pod:\n\nSQL Query over TCP (L4 only)\n\n\nResponse Path (dotted lines):\n\nReverse of above: DB → Backend → Frontend → Ingress → User\n\n\n\n\n\nExplanation:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFlowProtocolOSI Layer(s)ReasonHTTPSL7ApplicationUser-level communicationHTTP over TCPL4 + L7Transport + ApplicationApplication logic over reliable TCPTCP (e.g., SQL)L4TransportRaw data, no app-layer logic like HTTPVirtualService RoutingL7ApplicationIstio routes based on URL/path/headersK8s Service routingL4TransportIP:Port based routing only\n✅ 1. Browser → Istio Ingress Gateway\n\nProtocol: HTTPS (encrypted)\nLayer: L7\n✅ Why: This is an application-layer request — TLS/SSL encrypted HTTP (L7).\n🔓 Istio Ingress Gateway terminates SSL → converts to plain HTTP (still L7).\n\n\n✅ 2. Istio Gateway → VirtualService → Frontend Service\n\nProtocol: HTTP\nLayer: L7\n✅ Why: The VirtualService matches on host, path, etc. — these are L7 HTTP headers.\nSo, Istio routes the request using L7 logic (e.g., /login, /profile, etc.)\n\n\n✅ 3. Frontend Service → Frontend Pod\n\nProtocol: HTTP over TCP\nLayer: L4 + L7\n✅ Why:\n\nThe Service object routes at L4 using IP:Port.\nBut inside the request, it’s still HTTP, so L7 application logic exists.\nK8s doesn’t inspect HTTP headers — it’s TCP load-balancing (L4).\nSo it’s: L4 for routing, L7 for app data.\n\n\n\n\n✅ 4. Frontend Pod → Backend Service → Backend Pod\n\nSame as above.\nProtocol: HTTP API\nLayer: L4 + L7\n✅ The Service handles L4 routing (no knowledge of HTTP headers).\nBut app-level logic (e.g., fetchUser()) is still L7.\n\n\n✅ 5. Backend Pod → DB Service / Pod\n\nProtocol: e.g., TCP, PostgreSQL\nLayer: L4\n❗ Why not L7?\n\nSQL is not considered application-layer HTTP.\nThere’s no URL or headers to inspect.\nNo VirtualService involved.\nThis is raw TCP — Layer 4.\n\n\n\n\n🧠 Fundamental Concept:\n| Istio Components (Gateway, VirtualService) handle traffic at L7 |\n| Kubernetes Services operate at L4 |\n| App-to-App communication is usually HTTP over TCP → both L4 &amp; L7 |\n| App-to-DB communication is usually pure TCP (L4) |\nBrowser (HTTPS L7) \n   → Istio Gateway (SSL offload)\n      → VirtualService (L7 route)\n         → K8s Service (L4)\n            → Frontend Pod (App logic L7)\n               → Backend Service (L4)\n                  → Backend Pod (App logic L7)\n                     → DB Service (L4)\n                        → DB Pod (TCP L4)\n\n"},"System-Design/LoadBalancers":{"slug":"System-Design/LoadBalancers","filePath":"System Design/LoadBalancers.md","title":"LoadBalancers","links":["DevOps/SSL-or-TLS-Offloading"],"tags":[],"content":"Load balancers can be of two types -\n\nLayer 4 or Transport Layer Load Balancer\nLayer 7 or Application Layer Load Balancer, where SSL or TLS offloading can happen - SSL or TLS Offloading\n\nIn other words, Load balancer can work as any of two layers of OSI model - Layer 4 or Layer 7.\nExample -\n\nNetwork Load Balancer (NLB) works in Layer 4. AWS NLB is an example\nApplication Load Balancer (ALB) works in Layer 7. AWS ALB is an example.\n\nComparison between L4 and L7 load balancers\n&lt;img src = /assets/Pasted-image-20250624-1.png&gt;\nOSI model\n&lt;img src = /assets/Pasted-image-20250622-32.png width=500&gt;\nPhysical Layer is L1 or Layer 1\nData Link Layer is L2 or Layer 2\n…\n…\nApplication Layer is L7 or Layer 7\nTransport Layer (L4)\n\n&lt;img src = /assets/Pasted-image-20250622-40.png&gt;\nThough NLB uses the IP address (L3 info), its core job is managing connections using port numbers, and ensuring those connections are balanced and persistent to the right server and ensuring connections are healthy beforehand by sending SYNC, ACK, etc signal which is an L4 (Transport Layer) responsibility.\n\nQ.\nIn General to do load balancing in two ec2 box which types of load balancer is used in AWS?\nIn AWS, when load balancing traffic across two or more EC2 instances, you can utilize Application Load Balancer (ALB), Network Load Balancer (NLB), or Classic Load Balancer (CLB). The choice depends on your specific needs, such as the type of traffic (HTTP/HTTPS, TCP/UDP), performance requirements, and advanced routing needs. \nHere’s a breakdown of the options:\n\n\nApplication Load Balancer (ALB):\nBest for HTTP and HTTPS traffic, supporting advanced routing features like path-based routing and host-based routing. It’s well-suited for modern application architectures, including those with microservices and containerized applications. \n\n\nNetwork Load Balancer (NLB):\nIdeal for high-performance, low-latency applications, particularly those using TCP or UDP protocols. NLB handles millions of requests per second with ultra-low latencies and supports static IP addresses. \n\n\nClassic Load Balancer (CLB):\nA legacy option that provides basic load balancing across EC2 instances, operating at both the request and connection level. While still functional, it’s generally recommended to use ALB or NLB for new deployments. \n\n\nIn summary:\n\nFor HTTP/HTTPS traffic with advanced routing, use an ALB. \nFor high-performance TCP/UDP traffic, use an NLB. \nFor basic load balancing, CLB is an option, but ALB or NLB are generally preferred.\n\n\nVarious configurable tools like NGINX, HAProxy, Envoy Proxy can be used as L4 or L7 load balancer based on the configuration.\nFor NGINX specifically:\n\nL4 (Stream module): NGINX uses its stream module for TCP/UDP load balancing. In this configuration, it’s operating at Layer 4, making decisions purely based on IP addresses and port numbers.\n\n\t# Example NGINX L4 (Stream) configuration\nstream {\n    upstream backend_servers {\n        server 192.168.1.10:8000;\n        server 192.168.1.11:8000;\n    }\n\n    server {\n        listen 8000; # Listen on TCP port 8000\n        proxy_pass backend_servers;\n    }\n}\n\n\nL7 (HTTP module): NGINX’s default http module is used for Layer 7 load balancing. Here, it can inspect HTTP headers, path, cookies, etc., to route requests.\n\nExample NGINX L7 (HTTP) configuration\nhttp {\n    upstream web_servers {\n        server 199.1.1.10;\n        server 199.1.1.11;\n    }\n\n    server {\n        listen 80;\n        location / {\n            proxy_pass http://web_servers;\n            # Can add L7 rules here, like based on path or headers\n        }\n    }\n}\n\nWhile specialized cloud load balancers like AWS NLB are only L4 and ALB are only L7, many other load balancing solutions (especially software-based ones) offer the flexibility to operate at either L4 or L7 based on their configuration and the needs of the traffic."},"System-Design/Networking-(Basics)":{"slug":"System-Design/Networking-(Basics)","filePath":"System Design/Networking (Basics).md","title":"Networking (Basics)","links":[],"tags":[],"content":"1. OSI Model &amp; TCP/IP Model (Conceptual Understanding)\n\n\nWhat they are: Both the OSI (Open Systems Interconnection) model and the TCP/IP model are conceptual frameworks that describe how network communication works. They break down the complex process of sending data from one computer to another into a series of distinct, manageable layers. Each layer performs a specific function and communicates only with the layers directly above and below it.\n\n\nThe Difference in Focus:\n\nThe OSI Model is more theoretical and consists of 7 layers (Physical, Data Link, Network, Transport, Session, Presentation, Application). It’s a very detailed blueprint for network functions.\nThe TCP/IP Model is more practical and implementation-focused, typically described with 4 or 5 layers (Network Access, Internet, Transport, Application; sometimes Physical is added as a 5th, or Network Access is split). This is the model that the actual internet protocols (like TCP and IP) are based on.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOSI ModelTCP/IP ModelIt stands for Open Systems Interconnection.It stands for Transmission Control and Internet Protocol.It is a theoretical framework for the computer environment**.**It is a customer service model that is used for data information transmission.In the OSI model, there are 7 Layers4 Layers are present in the TCP/IP modelLow in useTCP/IP model is mostly usedThis model is an approach in VerticalThis model is an approach in horizontalIn this model, delivery of package is a guaranteeIn this model, delivery of package is not assuredThe protocol is hidden in OSI and can be easily substituted and changes in technology.In this model, replacing tool is not easy as like OSI\n\n\nRelevance for a Senior DevOps Engineer (Troubleshooting Focus):\n\nStructured Troubleshooting: You don’t need to memorize every detail of every layer. Instead, think of them as a checklist or a mental map for debugging. When a problem occurs, this layered understanding helps you systematically pinpoint the area of the network stack where the issue likely resides.\nExample Scenarios:\n\nApplication Layer (Layer 7 in OSI/TCP-IP):\n\nIf your web service is returning 500 Internal Server Errors or an API call fails with a specific error message, you’re likely troubleshooting at the application layer. This involves checking your code, application logs, or web server configuration (e.g., Nginx, Apache, IIS).\n\n\nTransport Layer (Layer 4 in OSI, Transport in TCP/IP):\n\nIf your application can’t connect to a database service on a specific port (e.g., 3306 for MySQL) or an HTTP request never completes, you’re looking here. Issues often involve firewalls, security groups, or network ACLs blocking a specific port, or a service simply not listening on that port. Tools like telnet or netcat are useful here.\n\n\nNetwork Layer (Layer 3 in OSI, Internet in TCP/IP):\n\nIf servers can’t ping each other, or traffic isn’t reaching the correct destination IP address, the problem is often here. This points to IP addressing issues, incorrect subnet configurations, or faulty routing tables in your VPC or on a server. ping and traceroute are key tools.\n\n\nData Link/Physical Layers (Lower layers):\n\nWhile less common for pure DevOps in the cloud (where AWS/Azure manage the physical infrastructure), understanding these layers means acknowledging that underlying issues like faulty network interfaces, misconfigured VLANs, or even physical cable problems can exist (though rare in a fully cloud-managed environment).\n\n\n\n\n\n\n\nWhy TCP/IP is More Practical?\n\nBecause it directly maps to the protocols you use daily (TCP, UDP, IP, HTTP, DNS), the TCP/IP model’s four-layer structure is generally more intuitive and practical for day-to-day DevOps work and cloud networking configurations. You’ll often hear discussions about “Layer 7 routing” (Application Load Balancers) or “Layer 4 blocking” (Network Security Groups) directly referencing these concepts.\n\n\n\nVarious Application Layer Protocols:\n\n\n\n2. IP Addressing (IPv4 &amp; IPv6)\n\nDescription: The unique numerical label assigned to each device connected to a computer network.\n\nIPv4: (e.g., 192.168.1.1) The most common version, but addresses are running out.\nIPv6: (e.g., 2001:0db8:85a3:0000:0000:8a2e:0370:7334) The newer standard, designed for a vast number of addresses.\n\n\nRelevance:\n\nPublic vs. Private IPs: Understanding that private IPs are used within your cloud VPCs (e.g., 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16) and public IPs are for internet communication.\nNAT (Network Address Translation): How private IPs communicate with the internet (e.g., via NAT Gateways in AWS).\nCIDR Notation (Classless Inter-Domain Routing): (e.g., 10.0.0.0/24 means a range of IPs in that network). Crucial for defining subnets and VPC ranges in cloud environments.\n\n\n\n\n3. Subnetting\n\nDescription: Dividing a large network into smaller, more manageable subnetworks.\nRelevance: Essential for:\n\nNetwork Segmentation: Isolating different environments (e.g., web servers in one subnet, databases in another, each with different security rules).\nCloud VPCs (Virtual Private Clouds): You’ll constantly define subnets (public, private) within your VPCs.\nAvailability Zones: Spreading subnets across multiple AZs for high availability.\n\n\n\n\n4. Routing\n\nDescription: The process of forwarding network packets from one network to another based on IP addresses. Routers use routing tables to decide the best path.\nRelevance:\n\nVPC Route Tables: You’ll configure these to direct traffic within your VPCs, to the internet, or to other connected networks (e.g., VPNs, Transit Gateways).\nDefault Gateway: How a device knows where to send traffic that isn’t on its local network.\n\n\n\n\n5. DNS (Domain Name System)\n\nDescription: The “phonebook of the internet.” Translates human-readable domain names (like google.com) into computer-readable IP addresses.\nRelevance:\n\nService Discovery: How your applications find other services (e.g., a microservice finding a database).\nLoad Balancer Integration: DNS records often point to load balancers.\nGlobal Traffic Management: Directing users to the nearest or healthiest application endpoint (e.g., Route 53, Azure DNS).\nTroubleshooting: dig or nslookup are common tools for debugging connectivity.\n\n\n\n\n6. TCP (Transmission Control Protocol) &amp; UDP (User Datagram Protocol)\n\nDescription:\n\nTCP: Connection-oriented, reliable, ordered delivery. (e.g., web Browse, email, file transfers). If packets are lost, TCP re-sends them.\nUDP: Connectionless, unreliable, faster delivery. (e.g., streaming video, online gaming, DNS queries). If packets are lost, they’re usually not re-sent.\n\n\nRelevance:\n\nPort Numbers: Crucial for opening firewall rules (e.g., TCP port 80 for HTTP, TCP port 22 for SSH, UDP port 53 for DNS).\nApplication Requirements: Knowing which protocol your application uses for communication helps in troubleshooting network issues.\n\n\n\n\n7. Firewalls, Security Groups, &amp; Network ACLs\n\nDescription: Mechanisms to control network traffic based on defined rules.\n\nFirewall: General term for a network security system.\nSecurity Groups (e.g., AWS): Stateful virtual firewalls that control traffic to and from instances at the instance level. They remember outbound connections to allow return traffic automatically.\nNetwork ACLs (e.g., AWS): Stateless packet filtering at the subnet level. Both inbound and outbound rules must be explicitly defined for return traffic.\n\n\nRelevance:\n\nPrimary Security Control: DevOps engineers constantly configure these to secure applications and infrastructure (e.g., only allow SSH from specific IPs, only allow web traffic on port 80/443).\nTroubleshooting Connectivity: Often the first place to check when an application or service can’t communicate.\n\n\n\n\n8. Load Balancing\n\nDescription: Distributing incoming network traffic across multiple servers to ensure high availability, scalability, and performance.\nRelevance:\n\nApplication Load Balancers (ALB / L7): Operate at the application layer (HTTP/HTTPS). Great for web apps, microservices, path-based routing.\nNetwork Load Balancers (NLB / L4): Operate at the transport layer (TCP/UDP). Ideal for extreme performance, static IP addresses.\nHealth Checks: Critical for ensuring traffic is only sent to healthy instances.\nIngress for Container Orchestration: Kubernetes Ingress controllers or Service LoadBalancers often provision cloud load balancers.\n\n\n\n\n9. VPNs (Virtual Private Networks) &amp; Direct Connect/ExpressRoute\n\nDescription:\n\nVPN: Creates a secure, encrypted connection over a public network (like the internet) to connect your on-premises network to your cloud VPC.\nDirect Connect (AWS) / ExpressRoute (Azure): Dedicated, private network connections from your on-premises data center directly to the cloud provider’s network, offering more consistent bandwidth and lower latency than a VPN over the internet.\n\n\nRelevance: Essential for hybrid cloud architectures, where applications and data span both on-premises and cloud environments.\n\n\n10. Proxies (Forward &amp; Reverse)\n\nDescription:\n\nForward Proxy: Sits between clients and the internet, forwarding client requests. (e.g., for corporate internet access control/caching).\nReverse Proxy: Sits in front of one or more web servers, directing client requests to the appropriate server. (e.g., Nginx, HAProxy, API Gateways, Load Balancers often act as reverse proxies).\n\n\nRelevance: Used for:\n\nSecurity: Hiding internal server IPs.\nLoad Balancing: Distributing requests.\nCaching: Improving performance.\nSSL Termination: Handling encryption/decryption.\nAPI Gateways: Routing and managing API requests.\n\n\n\n\n11. Container Networking Basics (e.g., Docker, Kubernetes)\n\nDescription: How individual containers communicate with each other and with the outside world.\nRelevance:\n\nBridge Networks: Default way Docker containers on the same host communicate.\nOverlay Networks: How containers across different hosts (e.g., Kubernetes pods) communicate as if they were on the same network. It requires Overlay plugin, for example Azure CNI is not a overlay plugin, it uses VNET’s routes.\nService Meshes (e.g., Istio, Linkerd): Add advanced networking features (traffic management, security, observability) to microservices.\nCNI (Container Network Interface): The standard used by Kubernetes for network plugins.\n\n\n\n\n12. Network Troubleshooting Tools\n\nRelevance: A Senior DevOps engineer should be familiar with basic network troubleshooting commands:\n\nping: Test connectivity to a host.\ntraceroute/tracert: Show the path packets take to a destination.\nnetstat: Display network connections, routing tables, interface statistics.\ndig/nslookup: Query DNS servers for name resolution.\ntelnet/nc (netcat): Test if a port is open on a remote host.\ntcpdump/Wireshark (conceptual): For deep packet inspection (though rarely done from scratch in production, understanding its output is useful).\n\n\n"},"System-Design/Practical-application-of-System-Design":{"slug":"System-Design/Practical-application-of-System-Design","filePath":"System Design/Practical application of System Design.md","title":"Practical application of System Design","links":[],"tags":[],"content":"\n\nDistributed Systems Concepts:\n\n\nElaboration: Modern applications are almost always distributed, meaning components run on multiple machines, often across different geographical locations. Understanding distributed systems is fundamental to building resilient and scalable services.\n\n\nKey Aspects:\n\n\nCAP Theorem: This fundamental theorem states that a distributed data store/data storage/database system cannot simultaneously guarantee Consistency (all clients see the same data at the same time), Availability (every request receives a response, without guarantee of latest data), and Partition Tolerance (the system continues to operate despite network failures). You must choose two. For DevOps, this means understanding the trade-offs your chosen database or messaging system makes (e.g., opting for AP (Availability + Partial Tolerance) in NoSQL for high availability during network partitions, or CP (Consistency +  Partial Tolerance) in relational databases for strong consistency).\n\n\nDistributed Consensus: Algorithms like Paxos or Raft are used when multiple servers in a distributed system need to agree on a single value or state. While you might not implement them, understanding how they enable highly available services (like distributed databases, Kubernetes control plane, or Kafka) is crucial for troubleshooting and architecting robust systems.\n\n\nDistributed Transactions: Managing operations that span multiple services (e.g., ordering an item, processing payment, updating inventory). Common patterns include Two-Phase Commit (often complex and slow) or the Saga pattern (a sequence of local transactions, where each transaction updates data within its own service and publishes an event to trigger the next step, with compensating transactions for failures). Understanding how to ensure data integrity without monolithic transactions is key.\n\n\nIdempotency: Designing operations so that if they are executed multiple times (e.g., due to retries in a distributed system), the result is the same as if it were executed only once. This is vital for reliable asynchronous communication and fault tolerance.\n\n\n\n\n\n\nArchitectural Patterns &amp; Styles:\n\nElaboration: These are proven solutions to common problems in software architecture. Knowing when and why to apply them is crucial for effective design.\nKey Aspects:\n\n\nMonolith vs. Microservices: Understand the trade-offs: Monoliths are simpler to deploy initially but harder to scale specific parts and develop independently. Microservices offer independent deployability, scalability, and technology choice but introduce complexity in communication, data consistency, and operational overhead. DevOps engineers are key in managing this complexity via CI/CD, monitoring, and orchestration.\n\n\nEvent-Driven Architectures (EDA): Systems communicate by producing and consuming events (messages). This decouples services, improves responsiveness, and facilitates real-time data processing. Learn about Pub/Sub models, message brokers (like Kafka, RabbitMQ, Redis Streams, AWS SQS/SNS), and stream processing (e.g., Kafka Streams, Flink) for analytics or real-time reactivity.\n\n\nServerless Architectures (FaaS, BaaS): Focus on deploying individual functions or leveraging managed backend services without managing servers directly. Understand their benefits (auto-scaling, pay-per-execution) and limitations (cold starts, vendor lock-in, debugging challenges) for specific use cases.\n\n\nLayered Architecture: The classic n-tier design (Presentation, Business Logic, Data Access, Database) is still fundamental. Understanding how responsibilities are separated helps in designing robust and maintainable systems, even when applied to microservices (where each service might have its own internal layers).\n\n\n\n\n\n\nData Persistence Strategies (Deeper Dive):\n\nElaboration: Beyond just “using a database,” it’s about choosing the right database for the right problem and scaling it effectively.\nKey Aspects:\n\n\nDatabase Selection Criteria: Understand the strengths and weaknesses of different database types: SQL (Relational) like PostgreSQL, MySQL (strong consistency, complex queries, joins) vs. NoSQL (Non-Relational) like MongoDB (document), Cassandra (column-family), Redis (key-value/cache), Neo4j (graph). This involves analyzing data structure, consistency needs, scalability requirements, and query patterns.\n\n\nDatabase Sharding &amp; Replication: For scaling databases horizontally, understand sharding (distributing data across multiple database instances based on a key) and replication (creating copies of data for high availability and read scalability). Know common replication patterns like leader-follower (primary-replica) and multi-leader setups.\n\n\nData Consistency Models: Differentiate between Strong Consistency (all reads return the most recently written data), Eventual Consistency (data will eventually become consistent, but might return stale data temporarily), and others. Understand which database types offer which models and when to choose one over another.\n\n\nData Lake vs. Data Warehouse vs. Data Mart: Understand these concepts for storing and managing large volumes of data for analytics. A data lake stores raw, uncurated data; a data warehouse stores structured, curated data for business intelligence; data marts are subsets of data warehouses for specific departments.\n\n\n\n\n\n\nNetworking Fundamentals for Distributed Systems:\n\nElaboration: A solid grasp of networking is paramount, as distributed systems are fundamentally networks of communicating services.\nKey Aspects:\n\n\nNetwork Topologies: Understand common network layouts (e.g., star, mesh) and their implications for latency, fault tolerance, and security within a data center or cloud VPC.\n\n\nDNS (Domain Name System) &amp; Global Load Balancing: How DNS resolves hostnames to IPs globally, and how global load balancers use DNS or other mechanisms to route users to the nearest or healthiest data center or service instance. This is vital for global applications and disaster recovery.\n\n\nCDNs (Content Delivery Networks): How CDNs cache static and dynamic content at edge locations worldwide to reduce latency and load on origin servers, improving user experience and scalability.\n\n\nFirewalls, VPCs (Virtual Private Clouds), Subnets, Routing Tables, Network ACLs: Deep understanding of how network segmentation works in cloud environments, how traffic flows, and how to secure it at various layers. This is critical for network isolation and defining security boundaries for your services.\n\n\n\n\n\n\nAdvanced Observability &amp; Monitoring Strategy:\n\n\nElaboration: Beyond basic uptime checks, observability focuses on understanding the internal state of a system through its external outputs.\n\n\nKey Aspects:\n\n\nSLOs (Service Level Objectives), SLAs (Service Level Agreements), SLIs (Service Level Indicators): Understand how to define, measure, and manage these critical metrics for reliability and performance. SLIs are raw data (e.g., error rate, latency); SLOs are targets based on SLIs (e.g., 99.9% availability); SLAs are contractual agreements.\n\n\nDistributed Tracing: The ability to follow a single request as it propagates through multiple services in a distributed system. Tools like OpenTelemetry allow you to trace the full lifecycle of a request, invaluable for debugging latency issues and understanding inter-service dependencies.\n\n\nAdvanced Alerting &amp; On-Call Strategies: Moving beyond simple threshold-based alerts to more intelligent, context-aware alerting (e.g., anomaly detection, predictive alerting). Also, understanding on-call rotations, runbooks, and incident management best practices.\n\n\nDashboards &amp; Visualization: Designing effective and actionable dashboards that provide real-time insights into system health, performance trends, and operational metrics for various stakeholders.\n\n\n\n\n\n\nDisaster Recovery (DR) &amp; Business Continuity Planning (BCP):\n\nElaboration: Planning for worst-case scenarios to ensure critical business functions can continue during and after a major outage.\nKey Aspects:\n\n\nRTO (Recovery Time Objective): The maximum tolerable duration of service interruption. How quickly must the system be up again?\n\n\nRPO (Recovery Point Objective): The maximum tolerable amount of data loss measured in time. How much data can you afford to lose?\n\n\nDR Strategies: Understand various approaches like:\n\n\nBackup &amp; Restore: Basic, higher RTO/RPO.\n\n\nPilot Light: Core services running in DR region, full scale-up on demand.\n\n\nWarm Standby: Minimal instances running in DR region, quicker scale-up.\n\n\nMulti-Region Active-Active: Services running simultaneously in multiple regions for highest availability and lowest RTO/RPO.\n\n\n\n\nRegular DR Drills: The importance of routinely testing disaster recovery plans to ensure they work as expected and identify weaknesses.\n\n\n\n\n\n"},"System-Design/Proxy":{"slug":"System-Design/Proxy","filePath":"System Design/Proxy.md","title":"Proxy","links":[],"tags":[],"content":"Reference Diagram:\n\n\n🧭 1. What is a Proxy Server?\nA proxy server is an intermediary system between a client and the destination server. It relays traffic, either from the client to the internet or from the internet to internal services.\nIt modifies, filters, logs, or secures traffic depending on its configuration and type.\n🔁 2. Two Main Types of Proxies\n✅ Forward Proxy:\n\nClient-side proxy - Used by clients to access external services.\nSits between users and the internet.\nHides the user’s identity.\nControls outbound traffic.\nOften configured in corporate or restricted networks.\nClient → Forward Proxy → Target Server\nUsed for:\n\nContent filtering\nIP Anonymity\nRestricting access - Blocking access to certain websites\nLogging traffic and monitoring\n\n\nCommon Tools: Squid, Privoxy\n\n🔹 Example:\nA school computer accesses the internet via Squid proxy to block social media websites.\n✅ Reverse Proxy:\n\nServer-side proxy - Used by servers to manage inbound traffic.\nSits in front of backend servers.\nHides backend details and load balances traffic (or) Hides backend/internal servers from external users.\nControls inbound access to internal applications.\nUser (Internet) → Reverse Proxy → Internal Server\nUsed for:\n\nSSL termination\nLoad balancing\nWeb acceleration\nApplication firewall\nCaching content\nRouting based on URL path (e.g., /api, /auth)\n\n\nCommon Tools: NGINX, HAProxy, Envoy\n\n🔹 Example:\nNGINX receives user requests and forwards them to one of several app servers behind the scenes.\n☁️ 3. Cloud Examples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCloudForward ProxyReverse ProxyAWSCustom EC2 with SquidALB, CloudFront, NLB with target groupsAzureAzure Firewall with web categoriesAzure App Gateway, Front DoorGCPConfigure NAT + custom proxyHTTPS Load Balancer, Cloud CDN"},"System-Design/Scaling":{"slug":"System-Design/Scaling","filePath":"System Design/Scaling.md","title":"Scaling","links":[],"tags":[],"content":"Scaling -\n\nHorizontal Scaling\nVertical Scaling\n\nWe may ask, why can’t we have a one big computer with lot of CPU and RAM to server the thousands of request?\nProblems are -\n\nWhen there are high traffic, if we want to do vertical scaling, in other words if we want to increase CPU &amp; RAM, then we need to shut down that big computer and then install the new hardware or increase the CPU/RAM and then start it back. That is we will have downtime here, which is very bad.\nWhen the there is low traffic, that time also that bog machine need to be ON, though the traffic is less still we need to keep that big computer has to be ON. That is cost wise it is very bad to have a single big computer.\n\nTo avoid the above issues, we need to use Horizontal scaling.\nAdvantage of Horizontal scaling -\n\nTo do Horizontal Scaling, we don’t need to stop the existing running server. We just need to start new server and deploy the application code to it and add this server to the backend of load balancer.\nIt gives zero downtime\nLB is true enabler of horizontal scaling.\n\nExtra little cost in Horizontal scaling -\n\nWe need a load balancer.\n\nLoad Balancer (LB) supports various way to distribute the traffic.\n\nRound Robbin.\nLeast connection - it means LB will forward the traffic to that server which has less busy.\nIP hash -\netc.\n\nLB will not blindly forward the traffic to any server. First it does health check. It checks weather is server is up or not, any port, etc.\n\nWhat if LB crashes?\n\nCrashing LB gives single point of failure.\nIf we run LB on a single machine, then there is single point of failure.\nBut if we use Cloud provider’s managed LB services, then there is no chance of crashing of LB. They are not in single machine, they run in multiple machine in multiple region which makes highly available.\n\n\n\n"},"System-Design/Service-Mesh-Tools":{"slug":"System-Design/Service-Mesh-Tools","filePath":"System Design/Service Mesh Tools.md","title":"Service Mesh Tools","links":["DevOps/Tools/Service-Mesh-Tools"],"tags":[],"content":"Service Mesh Tools\n\nRegarding the above diagram -\n🔍 Should We Have an Envoy Sidecar for the DB Layer?\n✅ Technically Possible:\nYes, Istio can inject an Envoy sidecar into a database pod (like PostgreSQL or MySQL), just like any other pod. This means:\n\nAll inbound/outbound traffic to the DB pod would flow through Envoy.\nYou can apply mTLS, authorization policies, traffic control, etc., on DB access.\n\n\n❗ However, in real-world practice:\n🔹 Most production clusters avoid injecting Envoy into DB pods.\nWhy?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConcernReason🧠 OverheadAdds CPU/memory load to DB pod which is already I/O intensive.💥 LatencyIntroduces minimal, but non-zero, latency for DB operations.🛠️ ComplexityHarder to debug DB performance when traffic is wrapped inside proxy logic.🔐 SecurityIstio mTLS between app → Envoy (App Side) → DB Service is usually sufficient."},"System-Design/System-Design-For-DevOps":{"slug":"System-Design/System-Design-For-DevOps","filePath":"System Design/System Design For DevOps.md","title":"System Design For DevOps","links":["System-Design/Basics-of-System-Design","System-Design/Practical-application-of-System-Design","System-Design/Networking-(Basics)"],"tags":[],"content":"For a DevOps engineer, the core focus in system design and architecture should be on Operability and Lifecycle Management.\nThis means ensuring the system is designed from the ground up to be:\n\nEasy to Deploy &amp; Release (Automation): Think CI/CD, Infrastructure as Code.\nEasy to Monitor &amp; Troubleshoot (Observability): Comprehensive logging, metrics, tracing.\nResilient &amp; Reliable: Can handle failures and recover quickly (High Availability, Disaster Recovery)\nScalable &amp; Performant: Can grow with demand and deliver expected speed.\nSecure: Built with security embedded, not as an afterthought.\nCost-Efficient: Designed for optimal resource utilization.\n\nEssentially, a DevOps engineer’s core contribution to system design is to create architectures that are not just functional, but also robust, automated, and transparent for their entire journey from code to production and beyond.\nSystem Design Basics - Basics of System Design\nApplication of System Design in Practical - Practical application of System Design\nBasics of Networking - Networking (Basics)"},"System-Design/TLS-Termination":{"slug":"System-Design/TLS-Termination","filePath":"System Design/TLS Termination.md","title":"TLS Termination","links":[],"tags":[],"content":"\n\nIn our project, traffic flow path is -\n(covering only major hops)\nClient (Browser)\n   ↓\nCDN (CloudFront)\n   ↓\nPalo Alto\n   ↓\n  ALB\n   ↓\n  NLB\n   ↓\nIstio Ingress Gateway (EKS)\n\nHere,\n\nTLS offloading happens at Istio gateway level.\nWe have certificate at 4 places.\n\nAt CloudFront (CDN)\nAt ALB\nAt NLB\nIn EKS cluster via external secret\n\n\nAt CDN, ALB &amp; NLB we don’t do TLS offloading still we will need the certificate, otherwise TLS handshake will fail.\nAnd since it is same traffic which comes through all the hops, so all the hops like CDN, ALB, NLB &amp; EKS must have same TLS certificate.\n\nTLS Termination Point:\n\n“TLS termination happens at the Istio Gateway”\n✔️ So traffic stays encrypted all the way until it reaches the Istio ingress gateway.\n\nThat means:\n\nAll previous layers are doing TLS passthrough\nOnly the Istio ingress gateway decrypts the TLS traffic\n\n\n🔑 So Where Must Certificates Be Present?\nLet’s go hop-by-hop 👇\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLayerTLS TerminationNeeds Cert?Why?✅ CDN (CloudFront)❌ No✅ YesMust complete TLS handshake with browser (TLS passthrough mode)✅ ALB❌ No✅ YesStill participates in TCP+TLS handoff; must complete handshake✅ NLB❌ No✅ Yes*Needed if it’s listener of the TLS protocol✅ Istio Gateway✅ Yes✅ YesFinal TLS termination and decryption happens here\n🔸 *If the NLB is configured with TLS listeners (Layer 4 + TLS), it must have a certificate. If it’s a pure TCP passthrough, it may not need it.\n\n\nYou can say in the project they placed the certificate in 3 or 4 places, depending on how the NLB is configured.\n\nMost likely: ✅ 4 places\n\n📌 CloudFront\n📌 ALB\n📌 NLB (if TLS listener)\n📌 Istio Ingress Gateway\n\n\nEach layer still participates in the TLS handshake, even if not terminating it — so the certificate is required.\n\n\nNote:\nEven if a layer (like CloudFront or ALB) is not decrypting the traffic (i.e., TLS passthrough), it still:\n\nActs as the TLS termination point from the client’s perspective\nNeeds to respond to the TLS handshake from the browser\nMust prove its identity using a valid certificate\n\n🧠 TLS requires both sides to exchange certificates during the handshake.\nIf the CDN or ALB doesn’t have a cert, the handshake fails."},"System-Design/Token-Based-Authentication---Part-1":{"slug":"System-Design/Token-Based-Authentication---Part-1","filePath":"System Design/Token Based Authentication - Part 1.md","title":"Token Based Authentication - Part 1","links":[],"tags":[],"content":"What is a Request Header?\nImagine you’re sending a physical letter. The request header is like the envelope of your letter. It contains crucial metadata and instructions about the letter itself, its sender, its recipient, and how it should be handled, before you even look at the actual content (the body of the letter).\nIn the context of web communication (specifically HTTP/HTTPS):\n\nMetadata: Request headers are key-value pairs that carry supplementary information about the request being sent from a client (like your web browser or an application) to a server.\nPurpose: They provide context and instructions necessary for the server to understand and process the request correctly.\n\nBasically a header is metadata sent along with the request. It doesn’t contain actual data (like your login form info) — instead, it provides extra context to the server about:\n\nWhat the request is\nHow to process it\nWho’s sending it\nWhat format to expect\nWhether the user is authenticated, etc.\n\n\n🔷 Where Are Headers Found?\nEvery HTTP request (e.g., from a browser, curl, Postman, or any API client) can have headers.\n\n🔷 Example of HTTP Request with Headers\nGET /profile HTTP/1.1\nHost: example.com\nAuthorization: Bearer eyJhbGciOiJIUzI1NiIsInR...\nContent-Type: application/json\nUser-Agent: Mozilla/5.0\nAccept: */*\n\nHere:\n\nAuthorization → for auth tokens (like JWT)\nContent-Type → format of request body (JSON, form, etc.)\nUser-Agent → browser or client making the request\nAccept → response format client expects\n\n🔷 Types of Requests That Have Headers\n✅ All HTTP methods:\n\nGET\nPOST\nPUT\nDELETE\nPATCH\netc.\n\nEven a simple browser visit (GET /index.html) sends headers behind the scenes (like cookies, user-agent, etc.).\n\n✅ TL;DR\n\nHeader = extra info sent along with request (not the body).\nAll HTTP(S) requests can have headers.\nUsed for auth tokens, content type, client info, etc.\n\n🔷 Why Token?\nIn a typical application, authentication means confirming who the user is. Traditionally, this happens using username and password. But:\n\nYou don’t want to send credentials with every request — it’s insecure.\nYou want a way to verify identity for each API request without logging in again.\nYou want something the client can hold and present on every request, and the server can trust and verify.\n\n👉 This “something” is called a token — a short-lived, verifiable proof of identity.\n\n🔷 How Token Enables Authentication\n\nThe user logs in once (e.g., with username/password).\nThe authentication server verifies credentials.\nIf valid, it generates a token and gives it to the user/client.\nFrom now on:\n\nEvery request includes the token in the header (Authorization: Bearer &lt;token&gt;).\nThe server checks if the token is valid and not expired.\nIf valid → request is allowed.\n\n\n\n🔁 No need to re-login. The token replaces credentials for the duration of its lifetime.\n\n🔷 What is JWT (JSON Web Token)?\nJWT is one type of token — a standard format that is:\n\nSelf-contained (has all necessary user info inside it),\nDigitally signed (so it can’t be tampered with),\nAnd often used in stateless authentication systems.\n\n✅ Structure:\nJWT is just a Base64-encoded string, made of 3 parts:\nHeader.Payload.Signature\n\nHeader:\n\nType (JWT)\nSigning algorithm (e.g., HS256)\n\n{\n  &quot;alg&quot;: &quot;HS256&quot;,\n  &quot;typ&quot;: &quot;JWT&quot;\n}\n\nPayload:\n\nContains the actual claims like:\n\nsub (subject, e.g., user ID)\nname\nroles\nexp (expiration timestamp)\n\n\n\n{\n  &quot;sub&quot;: &quot;123456&quot;,\n  &quot;name&quot;: &quot;John Doe&quot;,\n  &quot;roles&quot;: [&quot;admin&quot;],\n  &quot;exp&quot;: 1720000000\n}\n\nSignature:\n\nCreated by hashing the header + payload with a secret or private key.\nThis ensures data integrity — if even 1 byte is changed, the signature won’t match.\n\n🔒 How JWT Enables Authentication\nWhen a client sends a JWT with a request:\n\nThe server:\n\nExtracts the JWT from the request header.\nDecodes the JWT.\nValidates the signature using the shared secret or public key.\nChecks if the token is expired (exp claim).\n\n\nIf all checks pass → request is authenticated.\n\nNo need to query the database every time — it trusts the signed token."},"System-Design/Token-Based-Authentication---Part-2":{"slug":"System-Design/Token-Based-Authentication---Part-2","filePath":"System Design/Token Based Authentication - Part 2.md","title":"Token Based Authentication - Part 2","links":[],"tags":[],"content":"Here we will see an end-to-end process.\nReference diagram:\n(The below diagram does not show service &amp; virtualservice. In actual case, these two components are also included).\n\n✅ End-to-End Flow: Authentication via Keycloak and JWT\n\n🟢 Step 1: User Makes Request\nUser Browser → HTTPS Request to /app → Istio Ingress Gateway\n\nThe user visits the app (e.g., yourapp.com/app)\nSince user is unauthenticated, they are not carrying a valid token (JWT)\n\n\n🟠 Step 2: Gateway Forwards Request\nIstio Ingress Gateway →VirtualService → Test-UI App service → Test-UI App (Frontend App in K8s)\n\nThe Istio Gateway routes the request to the correct Kubernetes service (Test-UI) via VirtualService.\nAt this point, the app realizes: “this user is not authenticated”\n\n\n🟡 Step 3: App Redirects to Keycloak\nTest-UI App → Redirect to /auth → Keycloak Server\n\nThe app sends a redirect response to the browser\nThe browser is now redirected to Keycloak’s login page\n\n\n🟡 Step 4: User Logs In\nUser Browser → Enters credentials → Keycloak\n\nThe user enters their username/password\nKeycloak verifies credentials\n\n\n🟡 Step 5: Keycloak Issues JWT Token\nKeycloak → JWT Token → Redirects user back to app (/app)\n\nIf credentials are correct:\n\nKeycloak generates a JWT token\nSends it to the browser as part of the redirect URL (or in a cookie/local storage)\n\n\nThe browser now holds the token\n\n\n🟢 Step 6: Browser Sends Token with Every Request\nUser Browser → New request to /app with Authorization: Bearer &lt;JWT&gt; header → Istio Gateway → Test-UI App\n\nThe browser now includes the token in all future requests\nThe frontend app receives this token\n\n\n🟢 Step 7: App Verifies Token\nTest-UI App → Validates token:\n\nLocally (using public key from Keycloak), OR\nBy asking Keycloak (token introspection endpoint)\n\n\n✅ Step 8: Token is Valid → Access Allowed\nIf valid:\n\nApp renders the authenticated user interface\nUser is now successfully logged in and can use the app\n\n\n🧠 Behind the Scenes\n\nJWT Token is not stored by the server, so it’s stateless\nToken includes user info and expiration\nApp trusts token because it’s signed by Keycloak (verifiable via public key)\n\n\n🔐 Summary of Key Elements:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComponentRoleIstio Ingress GatewayEntry point (TLS termination, routes to app)Test-UI AppValidates token, renders frontendKeycloak ServerAuth provider (SSO / OpenID Connect / OAuth2)JWTProof of authentication sent with each request"},"index":{"slug":"index","filePath":"index.md","title":"Home","links":[],"tags":[],"content":"Welcome\nThis is my homepage for notes."}}